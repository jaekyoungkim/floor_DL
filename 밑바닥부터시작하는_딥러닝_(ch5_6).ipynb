{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "밑바닥부터시작하는 딥러닝 (ch5-6).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMZXdoUVFlVeEyVaMFFg6TS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaekyoungkim/floor_DL/blob/main/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94_%EB%94%A5%EB%9F%AC%EB%8B%9D_(ch5_6).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDmkiOlw3H_t",
        "outputId": "50bf38fe-27d4-4764-b4d1-61b1570a1904"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'deeplearning_from_scratch'...\n",
            "remote: Enumerating objects: 405, done.\u001b[K\n",
            "remote: Total 405 (delta 0), reused 0 (delta 0), pack-reused 405\u001b[K\n",
            "Receiving objects: 100% (405/405), 58.06 MiB | 41.84 MiB/s, done.\n",
            "Resolving deltas: 100% (74/74), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/youbeebee/deeplearning_from_scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ch 5. 오차역전파법 "
      ],
      "metadata": {
        "id": "VPlNiXsY3mUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 앞장에서 신경망의 가중치 매개변수의 기울기는 수치미분을 사용해 계산함\n",
        "# 수치 미분은 단순하고 구현하기도 쉽지만 계산시간이 오래걸린다는게 단점 \n",
        "# 가중치 매개변수의 기울기를 효율적으로 계산하는 오차역전파법\n",
        "# 계산 그래프를 사용하는 이유 : 전체가 아무리 복잡해도 각 노드에서는 단순한 계산에 집주앟여 문제를 단순화 시킬 수 있음 + 중간결과 저장가능 + 역전파 통해 미분을 효율적으로 계산\n",
        "# 연쇄법칙 "
      ],
      "metadata": {
        "id": "6cx21iTS3Mob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 사과쇼핑 예를 파이썬으로 구현하기\n",
        "# 모든 게층은 forward() backward() 라는 공통의 메서드를 갖도록 구현함\n",
        "# forward 순전파, backward 역전파\n",
        "# 곱셈 계층은 mullayer\n",
        "class MuLayer:\n",
        "  def __init__(self): # x,y,를 초기화\n",
        "    self.x = None\n",
        "    self.y = None\n",
        "  \n",
        "  def forward(self, x, y):\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "    out = x * y\n",
        "    return out\n",
        "  \n",
        "  def backward(self, dout):\n",
        "    dx = dout * self.y\n",
        "    dy = dout * self.x\n",
        "    return dx, dy"
      ],
      "metadata": {
        "id": "JY0nJbXL3NwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "apple = 100\n",
        "apple_num =2 \n",
        "tax = 1.1\n",
        "\n",
        "# 계층들\n",
        "mul_apple_layer = MuLayer()\n",
        "mul_tax_layer = MuLayer()\n",
        "\n",
        "# 순전파\n",
        "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
        "price = mul_tax_layer.forward(apple_price, tax)\n",
        "\n",
        "print(price)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBJi-i0v3OT4",
        "outputId": "3da2c335-e0b8-4c8c-f7a9-1c7491d18902"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "220.00000000000003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 역전파\n",
        "dprice = 1\n",
        "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
        "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
        "\n",
        "print(dapple, dapple_num, dtax)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9g9zBHVd3QD-",
        "outputId": "a66ca966-45f6-4de0-8519-f65fff6a7ade"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.2 110.00000000000001 200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 덧셈계층\n",
        "class AddLayer:\n",
        "  def __init__(self):  # 덧셈 계층에서는 초기화가 필요없음 아무일도 안함\n",
        "    pass\n",
        "  \n",
        "  def forward(self, x, y):\n",
        "    out = x+y\n",
        "    return out \n",
        "  \n",
        "  def backward(self, dout): \n",
        "    dx = dout * 1 \n",
        "    dy = dout * 1\n",
        "    return dx, dy"
      ],
      "metadata": {
        "id": "LclqJ3Mx3Uut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 사과 2개, 귤3개구입에 대한 역전파\n",
        "apple = 100\n",
        "apple_num =2 \n",
        "orange = 150\n",
        "orange_num = 3\n",
        "tax = 1.1\n",
        "\n",
        "# 계층들\n",
        "mul_apple_layer = MuLayer()\n",
        "mul_orange_layer = MuLayer()\n",
        "add_apple_orange_layer =AddLayer()\n",
        "mul_tax_layer = MuLayer()\n",
        "\n",
        "# 순전파\n",
        "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
        "orange_price = mul_orange_layer.forward(orange, orange_num)\n",
        "all_price = add_apple_orange_layer.forward(apple_price, orange_price)\n",
        "price = mul_tax_layer.forward(all_price, tax)\n",
        "\n",
        "# 역전파\n",
        "dprice = 1\n",
        "dall_price , dtax = mul_tax_layer.backward(dprice)\n",
        "dapple_price, dorange_price =add_apple_orange_layer.backward(dall_price)\n",
        "dorange, dorange_num = mul_orange_layer.backward(dorange_price)\n",
        "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
        "\n",
        "print(price)\n",
        "print(dapple_num, dapple, dorange, dorange_num, dtax)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LO0v_fY3WG5",
        "outputId": "5ef18b57-152a-4b7d-e0e1-c1936cd75aad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "715.0000000000001\n",
            "110.00000000000001 2.2 3.3000000000000003 165.0 650\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 활성화 함수 계층 구성하기\n",
        "# 1. relu\n",
        "# x > 0 : 역전파는 상류의 값을 그대로 하류로 흘림\n",
        "# x < 0 : 역전파는 하류로 신호를 보내지 않음\n",
        "class Relu:\n",
        "  def __init__(self):\n",
        "    self.mask = None  #  true, false로 구성된 넘파이 배열 \n",
        "  \n",
        "  def forward(self, x):\n",
        "    self.mask = (x<=0) # x 0이하는 true, 0 초과는 FALSE\n",
        "    out = x.copy()\n",
        "    out[self.mask] = 0 \n",
        "    return out\n",
        "  \n",
        "  def backward(self, dout):\n",
        "    dout[self.mask] = 0 \n",
        "    dx = dout\n",
        "    return dx "
      ],
      "metadata": {
        "id": "lEXRHGhe3XEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "x = np.array([[1.0, 0.5], [-2.0, 3.0]])\n",
        "print(x)\n",
        "\n",
        "mask = (x <= 0)\n",
        "print(mask) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFuKQUsC3ej1",
        "outputId": "05da741c-d01e-4f0a-e059-5fa16928e00c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.   0.5]\n",
            " [-2.   3. ]]\n",
            "[[False False]\n",
            " [ True False]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. sigmoid 계층\n",
        "# x-> y  : y^2 * exp(-x) 가 하류 노드로 전파됨  / y(1-y):순전파의 출력(y)만으로 계산할수도있음\n",
        "class Sigmoid:\n",
        "  def __init__(self):\n",
        "    self.out = None\n",
        "  \n",
        "  def forward(self, x):\n",
        "    out = 1/(1+np.exp(-x))\n",
        "    self.out = out \n",
        "    return out\n",
        "  \n",
        "  def backward(self, dout):\n",
        "    dx = dout * (1.0 - self.out) * self.out\n",
        "    return dx\n",
        "\n"
      ],
      "metadata": {
        "id": "JDcoBNeu3fn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Affine / softmax계층 구현하기\n",
        "# 1. affine 계층\n",
        "# 신경망의 순전파때 수행하는 행렬의 곱은 기하학에서는 어파인 변환이라고 함\n",
        "\n"
      ],
      "metadata": {
        "id": "W9RmkHYQ3kdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 배치용 affine 계층\n",
        "# 입력데이터로 x하나만을 고려하지 않고, 데이터 n개를 묶어 순전파하는경우(묶은 데이터를 배치라고 함)\n",
        "x_dot_W = np.array([[0,0,0], [10,10,10]])\n",
        "B = np.array([1,2,3])\n",
        "\n",
        "x_dot_W "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uyih8i6z69bQ",
        "outputId": "48e48875-609b-4d3a-ee54-3a96e5a84978"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0],\n",
              "       [10, 10, 10]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_dot_W + B"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQd5J1su7pzB",
        "outputId": "def661f2-318f-4abf-c528-af61610cfbe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  2,  3],\n",
              "       [11, 12, 13]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dY = np.array([[1,2,3], [4,5,6]])\n",
        "dY"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65tSdrjH7uF-",
        "outputId": "f42cb1a9-47ec-4c5b-d183-fc434f824ca4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 2, 3],\n",
              "       [4, 5, 6]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dB = np.sum(dY, axis=0); dB # 같은 위치에 있는 원소끼리 합쳐줌, 0 번째 축으로 합치기\n",
        "# 편향의 역전파는 n=2라고 가정하면, 그 두 데이터에 대한 미분을 데이터마다 더해수 구함"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVf6IkJ871Y_",
        "outputId": "7623b325-bf05-4a8f-b5d7-e7c19f35a5bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 7, 9])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(dY, axis=1) # 1번째 축으로 합치기"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgBuf5Tj753y",
        "outputId": "29037718-cf94-4ce9-8d45-8d19a83b309e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 6, 15])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(a):\n",
        "    c = np.max(a)\n",
        "    exp_a = np.exp(a - c)  # 오버플로 대책\n",
        "    sum_exp_a = np.sum(exp_a)\n",
        "    y = exp_a / sum_exp_a\n",
        "\n",
        "    return y\n",
        "\n",
        "\n",
        "def cross_entropy_error(y, t):\n",
        "    delta = 1e-7  # 0일때 -무한대가 되지 않기 위해 작은 값을 더함\n",
        "    return -np.sum(t * np.log(y + delta))\n"
      ],
      "metadata": {
        "id": "gz7Z8oQhDfbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Affine:\n",
        "  def __init__(self, W, b):\n",
        "    self.W = W\n",
        "    self.b = b\n",
        "    self.x = None\n",
        "    self.dW = None\n",
        "    self.db = None\n",
        "\n",
        "  def forward(self,x):\n",
        "    self.x = x\n",
        "    out = np.dot(x, self.W) + self.b\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dx = np.dot(dout, self.W.T)  # x에대한 역전파\n",
        "    self.dW = np.dot(self.x.T, dout) #w에대한 역전파\n",
        "    self.db = np.sum(dout, axis= 0) # 배치결과들을 합치기\n",
        "    return dx"
      ],
      "metadata": {
        "id": "LIHbxQgT8AuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. softmax-with-loss 계층\n",
        "# 소프트맥스 함수는 입력값을 정규화하여 출력함\n",
        "class SoftmaxWithLoss:\n",
        "  def __init__(self):\n",
        "    self.loss = None\n",
        "    self.y = None\n",
        "    self.t = None\n",
        "  \n",
        "  def forward(self, x, t):\n",
        "    self.t = t\n",
        "    self.y = softmax(t)\n",
        "    self.loss = cross_entropy_error(self.y , self.t)\n",
        "    return self.loss\n",
        "  \n",
        "  def backward(self, dout=1):\n",
        "    batch_size = self.t.shape[0]\n",
        "    dx = (self.y - self.t) / batch_size # 전파하는 값을 배치의 수로 나눠서 데이터 1개당 오차를 앞 계층으로 전파함\n",
        "    return dx\n"
      ],
      "metadata": {
        "id": "I9C0JS5V9XWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 오차역전파 구현하기\n",
        "# 수치미분은 구현하기는 쉽지만 계산이 오래 걸림 -> 오차역전파법을 이용하면 느린 수치미분과 달리 기울기를 효율적으로 빠르게 구할 수 있음\n",
        "# numerical_gradient : 수치미분방식\n",
        "# gradient : 오차역전파법"
      ],
      "metadata": {
        "id": "KphKMjM9Cu3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys , os\n",
        "#sys.path.append('/content/deeplearning_from_scratch/common')\n",
        "sys.path.append('/content/deeplearning_from_scratch/')\n",
        "import numpy as np \n",
        "from common.layers import *\n",
        "from common.gradient import numerical_gradient\n",
        "from collections import OrderedDict"
      ],
      "metadata": {
        "id": "ek3ON5McJzqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoLayerNet:\n",
        "  def __init__(self, input_size, hidden_size, output_size, weight_init_std= 0.01):\n",
        "    self.params = {}\n",
        "    self.params['W1'] = weight_init_std * np.random.randn(input_size , hidden_size)\n",
        "    self.params['b1'] = np.zeros(hidden_size)\n",
        "    self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "    self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "    #계층생성 \n",
        "    self.layers = OrderedDict() # 순서가 있는 딕셔너리 \n",
        "    # 순전파때는 추가한 순서대로 각 계층에 forward메서드를 호출하기만 하면 됨\n",
        "    # 역전파때는 계층을 반대 순서로 호출하면됨\n",
        "    self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
        "    self.layers['Relu1'] = Relu()\n",
        "    self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
        "    self.lastlayer = SoftmaxWithLoss()\n",
        "\n",
        "  def predict(self,x): #예측\n",
        "    for layer in self.layers.values():\n",
        "      x = layer.forward(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "  # x 입력데이터, t 정답 레이블\n",
        "  def loss(self, x, t):\n",
        "    y= self.predict(x)\n",
        "    return self.lastLayer.forward(y,t)\n",
        "  \n",
        "  def accuracy(self, x, t):\n",
        "    y=self.predict(x)\n",
        "    y= np.argmax(y, axis=1)\n",
        "    if t.ndim !=1 : t= np.argmax(t, axis=1)\n",
        "\n",
        "    accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "  def numerical_gradient(self, x, t):\n",
        "    loss_W = lambda W : self.loss(x,t)\n",
        "    grads = {}\n",
        "    grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "    grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "    grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "    grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "\n",
        "    return grads\n",
        "    \n",
        "\n",
        "\n",
        "  def gradient(self, x, t):\n",
        "    # 순전파\n",
        "    self.loss(x,t)\n",
        "\n",
        "    # 역전파\n",
        "    dout = 1\n",
        "    dout = self.lastLayer.backward(dout)\n",
        "    layers = list(self.layers.values())\n",
        "    layers.reverse()\n",
        "    for layer in layers:\n",
        "      dout = layer.backward(dout)\n",
        "\n",
        "    # 결과저장\n",
        "    grads = {}\n",
        "    grads['W1'] = self.layers['Affine1'].dW\n",
        "    grads['b1'] = self.layers['Affine1'].db\n",
        "    grads['W2'] = self.layers['Affine2'].dW\n",
        "    grads['b2'] = self.layers['Affine2'].db \n",
        "    \n",
        "    return grads   "
      ],
      "metadata": {
        "id": "_sH_XjZFMqGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 오차역전파법으로 구한 기울기 검증하기\n",
        "# 1. 수치미분 \n",
        "# 2. 해석적으로 수식을 풀어 구하는 방법, 매개변수가 많아도 효율적으로 계산할 수 있음\n",
        "# 수치미분은 오차역전파법을 정확히 구현했는지 확인하기 위해 필요함\n",
        "# 수치미분의 장점은 구현하기 쉽다는 것\n",
        "# 수치미분구현에는 버그가 숨어있기 어려운반면, 오차역전파법은 구현하기 복잡해서 종종 실수를 하곤 함\n",
        "# 기울기확인 : 1,2방법의 결과가 같은지 확인"
      ],
      "metadata": {
        "id": "4NCgL1ScaY2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataset.mnist import load_mnist\n",
        "from common.two_layer_net import TwoLayerNet  # ch5 에 들어있는 two_layer_net 파일 common으로 옮기기"
      ],
      "metadata": {
        "id": "ZFtBX8Znex_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize = True, one_hot_label = True)\n",
        "network = TwoLayerNet(input_size = 784, hidden_size = 50, output_size= 10)\n",
        "x_batch = x_train[:3]\n",
        "t_batch = t_train[:3]\n",
        "\n",
        "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
        "grad_backprop = network.gradient(x_batch, t_batch)\n",
        "\n",
        "# 각 가중치의 차이의 절댓값을 구한 후 그 절댓값들의 평균을 구함\n",
        "for key in grad_numerical.keys():\n",
        "  diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]))\n",
        "  print(key + \":\" + str(diff))  # 차이가 매우작음을 알 수 있음"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIu-fbd1hSFo",
        "outputId": "8dd65693-dbf5-4c5f-9a9f-50a706a2b304"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W1:2.2260574784664928e-13\n",
            "b1:6.023729865606287e-13\n",
            "W2:7.829423153370412e-13\n",
            "b2:1.2034817170603062e-10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 오차역전파를 사용한 학습 구현하기\n",
        "network = TwoLayerNet(input_size = 784, hidden_size = 50, output_size=10)\n",
        "iters_num = 10000\n",
        "train_size = x_train.shape[0] ;train_size # 60000\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSDoQByBiYUp",
        "outputId": "3a01a9ef-8982-4599-df59-99a9c84bfd12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "for i in range(iters_num):\n",
        "  batch_mask = np.random.choice(train_size, batch_size) \n",
        "  x_batch = x_train[batch_mask]\n",
        "  t_batch = t_train[batch_mask]\n",
        "\n",
        "  #오차역전파법으로 기울기 구하기\n",
        "  grad = network.gradient(x_batch, t_batch)\n",
        "\n",
        "  # 갱신\n",
        "  for key in ('W1','b1','W2','b2'):\n",
        "    network.params[key] -= learning_rate * grad[key]\n",
        "  \n",
        "  loss = network.loss(x_batch, t_batch)\n",
        "  train_loss_list.append(loss)\n",
        "\n",
        "  if i% iter_per_epoch ==0:\n",
        "    train_acc = network.accuracy(x_train, t_train)\n",
        "    test_acc = network.accuracy(x_test, t_test)\n",
        "    train_acc_list.append(train_acc)\n",
        "    test_acc_list.append(test_acc)\n",
        "    print(train_acc, test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsfA9iQRkNn8",
        "outputId": "7bd08204-9235-4791-99ae-79a4804cf57b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.14916666666666667 0.1508\n",
            "0.9021666666666667 0.9051\n",
            "0.9216333333333333 0.9233\n",
            "0.9330333333333334 0.9354\n",
            "0.94485 0.9425\n",
            "0.9507166666666667 0.9488\n",
            "0.9563833333333334 0.9514\n",
            "0.9596333333333333 0.9545\n",
            "0.9636 0.9579\n",
            "0.9663333333333334 0.9608\n",
            "0.9667166666666667 0.9609\n",
            "0.9706833333333333 0.965\n",
            "0.9728 0.9657\n",
            "0.9729666666666666 0.9665\n",
            "0.9751166666666666 0.9683\n",
            "0.97425 0.9672\n",
            "0.97835 0.9711\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ch 6. 학습 관련 기술들 "
      ],
      "metadata": {
        "id": "54k0tzYul4Zo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 최적화 : 신경망의 최적화는 굉장히 어려운 문제 , 매개변수 공간은 매우넓고 복잡해서 최적의 솔루션은 쉽게 못찾음\n",
        "# 매개변수 갱신\n",
        "# SGD보다 좋은 최적화 방법들을 알아보고자 함\n"
      ],
      "metadata": {
        "id": "vLPs7Syhl6RJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. SGD\n",
        "class SGD:\n",
        "  def __init__(self, lr = 0.01):\n",
        "    self.lr = lr\n",
        "  \n",
        "  def update(self, params, grads):\n",
        "    for key in params.keys():\n",
        "      params[key] -= self.lr * grads[key]"
      ],
      "metadata": {
        "id": "xna-wjVVqTVH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#network = TwoLayerNet(...)\n",
        "#optimizer = SGD() #  이부분을 바꾸면됨\n",
        "#for i in range(10000):\n",
        "# x_batch, t_batch = get_mini_batch(...)\n",
        "# grads = network.gradient(x+batch, t_batch)\n",
        "# params = network.params\n",
        "# optimizer.update(params, grads) \n",
        "\n",
        "# SGD의 단점 : 단순하고 구현도 쉽지만, 비등방성함수에서는 탐색경로가 비효율적 ,무작정 기울어진 방향으로 진행하는 단순한 방식"
      ],
      "metadata": {
        "id": "JpRjOfCb_Jy1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Momentum (운동량)\n",
        "class Momentum():\n",
        "  def __init__(self, lr = 0.01, momentum = 0.9): # v 물체의 속도\n",
        "    self.lr = lr\n",
        "    self.momentum = momentum\n",
        "    self.v = None\n",
        "  \n",
        "  def update(self, params, grads):\n",
        "    if self.v is None:\n",
        "      self.v = {}\n",
        "      for key, val in params.items():\n",
        "        self.v[key] = np.zeros_like(val)  # 초기 array생성해줌\n",
        "      \n",
        "    for key in params.keys():\n",
        "      self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n",
        "      params[key] += self.v[key]  # params = Weight\n"
      ],
      "metadata": {
        "id": "JZJ-2xrYBYh6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. AdaGrad\n",
        "# 신경망 학습에서 학습률값이 중요\n",
        "# 이 값이 너무 작으면 학습시간이 너무 길어지고, 너무크면 발산하여 학습이 제대로 이뤄지지 않음\n",
        "# 학습률을 정하는 효과적기술로 학습률 감소가 있음\n",
        "# 학습을 진행하면서 학습률을 점차 줄여가는 방법\n",
        "# 처음에는 크게 학습하다가 조금씩 작게 학습한다는 것 (실제 신경망 학습에 자주 사용됨)\n",
        "# 학습률을 서서히 낮추는 가장 간단한 방법은 매개변수 전체의 학습률 값을 일괄적으로 낮추는것\n",
        "# 각각의 매개변수에 맞춤형값을 만들어줌\n",
        "# 학습을 진행할수록 갱신강도가 약해짐 실제로 무한히 계속 학습하다보면  어느순간 갱신량이 0이되어 전혀 갱신되지 않음\n",
        "class AdaGrad:\n",
        "  "
      ],
      "metadata": {
        "id": "siZAuWfpG2np"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}