{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "밑바닥부터시작하는 딥러닝 (ch5-6).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNfLB82mV13+jaOfOJJYYdf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaekyoungkim/floor_DL/blob/main/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94_%EB%94%A5%EB%9F%AC%EB%8B%9D_(ch5_6).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDmkiOlw3H_t",
        "outputId": "6b10a709-2756-48a3-c586-2077717258d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'deeplearning_from_scratch'...\n",
            "remote: Enumerating objects: 405, done.\u001b[K\n",
            "remote: Total 405 (delta 0), reused 0 (delta 0), pack-reused 405\u001b[K\n",
            "Receiving objects: 100% (405/405), 58.06 MiB | 40.89 MiB/s, done.\n",
            "Resolving deltas: 100% (74/74), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/youbeebee/deeplearning_from_scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ch 5. 오차역전파법 "
      ],
      "metadata": {
        "id": "VPlNiXsY3mUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 앞장에서 신경망의 가중치 매개변수의 기울기는 수치미분을 사용해 계산함\n",
        "# 수치 미분은 단순하고 구현하기도 쉽지만 계산시간이 오래걸린다는게 단점 \n",
        "# 가중치 매개변수의 기울기를 효율적으로 계산하는 오차역전파법\n",
        "# 계산 그래프를 사용하는 이유 : 전체가 아무리 복잡해도 각 노드에서는 단순한 계산에 집주앟여 문제를 단순화 시킬 수 있음 + 중간결과 저장가능 + 역전파 통해 미분을 효율적으로 계산\n",
        "# 연쇄법칙 "
      ],
      "metadata": {
        "id": "6cx21iTS3Mob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 사과쇼핑 예를 파이썬으로 구현하기\n",
        "# 모든 게층은 forward() backward() 라는 공통의 메서드를 갖도록 구현함\n",
        "# forward 순전파, backward 역전파\n",
        "# 곱셈 계층은 mullayer\n",
        "class MuLayer:\n",
        "  def __init__(self): # x,y,를 초기화\n",
        "    self.x = None\n",
        "    self.y = None\n",
        "  \n",
        "  def forward(self, x, y):\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "    out = x * y\n",
        "    return out\n",
        "  \n",
        "  def backward(self, dout):\n",
        "    dx = dout * self.y\n",
        "    dy = dout * self.x\n",
        "    return dx, dy"
      ],
      "metadata": {
        "id": "JY0nJbXL3NwG"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "apple = 100\n",
        "apple_num =2 \n",
        "tax = 1.1\n",
        "\n",
        "# 계층들\n",
        "mul_apple_layer = MuLayer()\n",
        "mul_tax_layer = MuLayer()\n",
        "\n",
        "# 순전파\n",
        "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
        "price = mul_tax_layer.forward(apple_price, tax)\n",
        "\n",
        "print(price)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBJi-i0v3OT4",
        "outputId": "45d169ed-6360-4658-fb4c-9b78840ed7c0"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "220.00000000000003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 역전파\n",
        "dprice = 1\n",
        "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
        "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
        "\n",
        "print(dapple, dapple_num, dtax)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9g9zBHVd3QD-",
        "outputId": "852a7c41-76e3-407a-da8a-70a5a8e460dd"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.2 110.00000000000001 200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 덧셈계층\n",
        "class AddLayer:\n",
        "  def __init__(self):  # 덧셈 계층에서는 초기화가 필요없음 아무일도 안함\n",
        "    pass\n",
        "  \n",
        "  def forward(self, x, y):\n",
        "    out = x+y\n",
        "    return out \n",
        "  \n",
        "  def backward(self, dout): \n",
        "    dx = dout * 1 \n",
        "    dy = dout * 1\n",
        "    return dx, dy"
      ],
      "metadata": {
        "id": "LclqJ3Mx3Uut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 사과 2개, 귤3개구입에 대한 역전파\n",
        "apple = 100\n",
        "apple_num =2 \n",
        "orange = 150\n",
        "orange_num = 3\n",
        "tax = 1.1\n",
        "\n",
        "# 계층들\n",
        "mul_apple_layer = MuLayer()\n",
        "mul_orange_layer = MuLayer()\n",
        "add_apple_orange_layer =AddLayer()\n",
        "mul_tax_layer = MuLayer()\n",
        "\n",
        "# 순전파\n",
        "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
        "orange_price = mul_orange_layer.forward(orange, orange_num)\n",
        "all_price = add_apple_orange_layer.forward(apple_price, orange_price)\n",
        "price = mul_tax_layer.forward(all_price, tax)\n",
        "\n",
        "# 역전파\n",
        "dprice = 1\n",
        "dall_price , dtax = mul_tax_layer.backward(dprice)\n",
        "dapple_price, dorange_price =add_apple_orange_layer.backward(dall_price)\n",
        "dorange, dorange_num = mul_orange_layer.backward(dorange_price)\n",
        "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
        "\n",
        "print(price)\n",
        "print(dapple_num, dapple, dorange, dorange_num, dtax)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LO0v_fY3WG5",
        "outputId": "5ef18b57-152a-4b7d-e0e1-c1936cd75aad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "715.0000000000001\n",
            "110.00000000000001 2.2 3.3000000000000003 165.0 650\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 활성화 함수 계층 구성하기\n",
        "# 1. relu\n",
        "# x > 0 : 역전파는 상류의 값을 그대로 하류로 흘림\n",
        "# x < 0 : 역전파는 하류로 신호를 보내지 않음\n",
        "class Relu:\n",
        "  def __init__(self):\n",
        "    self.mask = None  #  true, false로 구성된 넘파이 배열 \n",
        "  \n",
        "  def forward(self, x):\n",
        "    self.mask = (x<=0) # x 0이하는 true, 0 초과는 FALSE\n",
        "    out = x.copy()\n",
        "    out[self.mask] = 0 \n",
        "    return out\n",
        "  \n",
        "  def backward(self, dout):\n",
        "    dout[self.mask] = 0 \n",
        "    dx = dout\n",
        "    return dx "
      ],
      "metadata": {
        "id": "lEXRHGhe3XEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "x = np.array([[1.0, 0.5], [-2.0, 3.0]])\n",
        "print(x)\n",
        "\n",
        "mask = (x <= 0)\n",
        "print(mask) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFuKQUsC3ej1",
        "outputId": "05da741c-d01e-4f0a-e059-5fa16928e00c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.   0.5]\n",
            " [-2.   3. ]]\n",
            "[[False False]\n",
            " [ True False]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. sigmoid 계층\n",
        "# x-> y  : y^2 * exp(-x) 가 하류 노드로 전파됨  / y(1-y):순전파의 출력(y)만으로 계산할수도있음\n",
        "class Sigmoid:\n",
        "  def __init__(self):\n",
        "    self.out = None\n",
        "  \n",
        "  def forward(self, x):\n",
        "    out = 1/(1+np.exp(-x))\n",
        "    self.out = out \n",
        "    return out\n",
        "  \n",
        "  def backward(self, dout):\n",
        "    dx = dout * (1.0 - self.out) * self.out\n",
        "    return dx\n",
        "\n"
      ],
      "metadata": {
        "id": "JDcoBNeu3fn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Affine / softmax계층 구현하기\n",
        "# 1. affine 계층\n",
        "# 신경망의 순전파때 수행하는 행렬의 곱은 기하학에서는 어파인 변환이라고 함\n",
        "\n"
      ],
      "metadata": {
        "id": "W9RmkHYQ3kdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 배치용 affine 계층\n",
        "# 입력데이터로 x하나만을 고려하지 않고, 데이터 n개를 묶어 순전파하는경우(묶은 데이터를 배치라고 함)\n",
        "x_dot_W = np.array([[0,0,0], [10,10,10]])\n",
        "B = np.array([1,2,3])\n",
        "\n",
        "x_dot_W "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uyih8i6z69bQ",
        "outputId": "48e48875-609b-4d3a-ee54-3a96e5a84978"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0],\n",
              "       [10, 10, 10]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_dot_W + B"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQd5J1su7pzB",
        "outputId": "def661f2-318f-4abf-c528-af61610cfbe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  2,  3],\n",
              "       [11, 12, 13]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dY = np.array([[1,2,3], [4,5,6]])\n",
        "dY"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65tSdrjH7uF-",
        "outputId": "f42cb1a9-47ec-4c5b-d183-fc434f824ca4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 2, 3],\n",
              "       [4, 5, 6]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dB = np.sum(dY, axis=0); dB # 같은 위치에 있는 원소끼리 합쳐줌, 0 번째 축으로 합치기\n",
        "# 편향의 역전파는 n=2라고 가정하면, 그 두 데이터에 대한 미분을 데이터마다 더해수 구함"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVf6IkJ871Y_",
        "outputId": "7623b325-bf05-4a8f-b5d7-e7c19f35a5bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 7, 9])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(dY, axis=1) # 1번째 축으로 합치기"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgBuf5Tj753y",
        "outputId": "29037718-cf94-4ce9-8d45-8d19a83b309e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 6, 15])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(a):\n",
        "    c = np.max(a)\n",
        "    exp_a = np.exp(a - c)  # 오버플로 대책\n",
        "    sum_exp_a = np.sum(exp_a)\n",
        "    y = exp_a / sum_exp_a\n",
        "\n",
        "    return y\n",
        "\n",
        "\n",
        "def cross_entropy_error(y, t):\n",
        "    delta = 1e-7  # 0일때 -무한대가 되지 않기 위해 작은 값을 더함\n",
        "    return -np.sum(t * np.log(y + delta))\n"
      ],
      "metadata": {
        "id": "gz7Z8oQhDfbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Affine:\n",
        "  def __init__(self, W, b):\n",
        "    self.W = W\n",
        "    self.b = b\n",
        "    self.x = None\n",
        "    self.dW = None\n",
        "    self.db = None\n",
        "\n",
        "  def forward(self,x):\n",
        "    self.x = x\n",
        "    out = np.dot(x, self.W) + self.b\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dx = np.dot(dout, self.W.T)  # x에대한 역전파\n",
        "    self.dW = np.dot(self.x.T, dout) #w에대한 역전파\n",
        "    self.db = np.sum(dout, axis= 0) # 배치결과들을 합치기\n",
        "    return dx"
      ],
      "metadata": {
        "id": "LIHbxQgT8AuE"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. softmax-with-loss 계층\n",
        "# 소프트맥스 함수는 입력값을 정규화하여 출력함\n",
        "class SoftmaxWithLoss:\n",
        "  def __init__(self):\n",
        "    self.loss = None\n",
        "    self.y = None\n",
        "    self.t = None\n",
        "  \n",
        "  def forward(self, x, t):\n",
        "    self.t = t\n",
        "    self.y = softmax(t)\n",
        "    self.loss = cross_entropy_error(self.y , self.t)\n",
        "    return self.loss\n",
        "  \n",
        "  def backward(self, dout=1):\n",
        "    batch_size = self.t.shape[0]\n",
        "    dx = (self.y - self.t) / batch_size # 전파하는 값을 배치의 수로 나눠서 데이터 1개당 오차를 앞 계층으로 전파함\n",
        "    return dx\n"
      ],
      "metadata": {
        "id": "I9C0JS5V9XWb"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 오차역전파 구현하기\n",
        "# 수치미분은 구현하기는 쉽지만 계산이 오래 걸림 -> 오차역전파법을 이용하면 느린 수치미분과 달리 기울기를 효율적으로 빠르게 구할 수 있음\n",
        "# numerical_gradient : 수치미분방식\n",
        "# gradient : 오차역전파법"
      ],
      "metadata": {
        "id": "KphKMjM9Cu3t"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys , os\n",
        "#sys.path.append('/content/deeplearning_from_scratch/common')\n",
        "sys.path.append('/content/deeplearning_from_scratch/')\n",
        "import numpy as np \n",
        "from common.layers import *\n",
        "from common.gradient import numerical_gradient\n",
        "from collections import OrderedDict\n",
        "from common.multi_layer_net import *"
      ],
      "metadata": {
        "id": "ek3ON5McJzqB"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoLayerNet:\n",
        "  def __init__(self, input_size, hidden_size, output_size, weight_init_std= 0.01):\n",
        "    self.params = {}\n",
        "    self.params['W1'] = weight_init_std * np.random.randn(input_size , hidden_size)\n",
        "    self.params['b1'] = np.zeros(hidden_size)\n",
        "    self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "    self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "    #계층생성 \n",
        "    self.layers = OrderedDict() # 순서가 있는 딕셔너리 \n",
        "    # 순전파때는 추가한 순서대로 각 계층에 forward메서드를 호출하기만 하면 됨\n",
        "    # 역전파때는 계층을 반대 순서로 호출하면됨\n",
        "    self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
        "    self.layers['Relu1'] = Relu()\n",
        "    self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
        "    self.lastlayer = SoftmaxWithLoss()\n",
        "\n",
        "  def predict(self,x): #예측\n",
        "    for layer in self.layers.values():\n",
        "      x = layer.forward(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "  # x 입력데이터, t 정답 레이블\n",
        "  def loss(self, x, t):\n",
        "    y= self.predict(x)\n",
        "    return self.lastLayer.forward(y,t)\n",
        "  \n",
        "  def accuracy(self, x, t):\n",
        "    y=self.predict(x)\n",
        "    y= np.argmax(y, axis=1)\n",
        "    if t.ndim !=1 : t= np.argmax(t, axis=1)\n",
        "\n",
        "    accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "  def numerical_gradient(self, x, t):\n",
        "    loss_W = lambda W : self.loss(x,t)\n",
        "    grads = {}\n",
        "    grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "    grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "    grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "    grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "\n",
        "    return grads\n",
        "    \n",
        "\n",
        "\n",
        "  def gradient(self, x, t):\n",
        "    # 순전파\n",
        "    self.loss(x,t)\n",
        "\n",
        "    # 역전파\n",
        "    dout = 1\n",
        "    dout = self.lastLayer.backward(dout)\n",
        "    layers = list(self.layers.values())\n",
        "    layers.reverse()\n",
        "    for layer in layers:\n",
        "      dout = layer.backward(dout)\n",
        "\n",
        "    # 결과저장\n",
        "    grads = {}\n",
        "    grads['W1'] = self.layers['Affine1'].dW\n",
        "    grads['b1'] = self.layers['Affine1'].db\n",
        "    grads['W2'] = self.layers['Affine2'].dW\n",
        "    grads['b2'] = self.layers['Affine2'].db \n",
        "    \n",
        "    return grads   "
      ],
      "metadata": {
        "id": "_sH_XjZFMqGV"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 오차역전파법으로 구한 기울기 검증하기\n",
        "# 1. 수치미분 \n",
        "# 2. 해석적으로 수식을 풀어 구하는 방법, 매개변수가 많아도 효율적으로 계산할 수 있음\n",
        "# 수치미분은 오차역전파법을 정확히 구현했는지 확인하기 위해 필요함\n",
        "# 수치미분의 장점은 구현하기 쉽다는 것\n",
        "# 수치미분구현에는 버그가 숨어있기 어려운반면, 오차역전파법은 구현하기 복잡해서 종종 실수를 하곤 함\n",
        "# 기울기확인 : 1,2방법의 결과가 같은지 확인"
      ],
      "metadata": {
        "id": "4NCgL1ScaY2X"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataset.mnist import load_mnist\n",
        "from common.two_layer_net import TwoLayerNet  # ch5 에 들어있는 two_layer_net 파일 common으로 옮기기"
      ],
      "metadata": {
        "id": "ZFtBX8Znex_X"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize = True, one_hot_label = True)\n",
        "network = TwoLayerNet(input_size = 784, hidden_size = 50, output_size= 10)\n",
        "x_batch = x_train[:3]\n",
        "t_batch = t_train[:3]\n",
        "\n",
        "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
        "grad_backprop = network.gradient(x_batch, t_batch)\n",
        "\n",
        "# 각 가중치의 차이의 절댓값을 구한 후 그 절댓값들의 평균을 구함\n",
        "for key in grad_numerical.keys():\n",
        "  diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]))\n",
        "  print(key + \":\" + str(diff))  # 차이가 매우작음을 알 수 있음"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIu-fbd1hSFo",
        "outputId": "8dd65693-dbf5-4c5f-9a9f-50a706a2b304"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W1:2.2260574784664928e-13\n",
            "b1:6.023729865606287e-13\n",
            "W2:7.829423153370412e-13\n",
            "b2:1.2034817170603062e-10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 오차역전파를 사용한 학습 구현하기\n",
        "network = TwoLayerNet(input_size = 784, hidden_size = 50, output_size=10)\n",
        "iters_num = 10000\n",
        "train_size = x_train.shape[0] ;train_size # 60000\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSDoQByBiYUp",
        "outputId": "3a01a9ef-8982-4599-df59-99a9c84bfd12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "for i in range(iters_num):\n",
        "  batch_mask = np.random.choice(train_size, batch_size) \n",
        "  x_batch = x_train[batch_mask]\n",
        "  t_batch = t_train[batch_mask]\n",
        "\n",
        "  #오차역전파법으로 기울기 구하기\n",
        "  grad = network.gradient(x_batch, t_batch)\n",
        "\n",
        "  # 갱신\n",
        "  for key in ('W1','b1','W2','b2'):\n",
        "    network.params[key] -= learning_rate * grad[key]\n",
        "  \n",
        "  loss = network.loss(x_batch, t_batch)\n",
        "  train_loss_list.append(loss)\n",
        "\n",
        "  if i% iter_per_epoch ==0:\n",
        "    train_acc = network.accuracy(x_train, t_train)\n",
        "    test_acc = network.accuracy(x_test, t_test)\n",
        "    train_acc_list.append(train_acc)\n",
        "    test_acc_list.append(test_acc)\n",
        "    print(train_acc, test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsfA9iQRkNn8",
        "outputId": "7bd08204-9235-4791-99ae-79a4804cf57b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.14916666666666667 0.1508\n",
            "0.9021666666666667 0.9051\n",
            "0.9216333333333333 0.9233\n",
            "0.9330333333333334 0.9354\n",
            "0.94485 0.9425\n",
            "0.9507166666666667 0.9488\n",
            "0.9563833333333334 0.9514\n",
            "0.9596333333333333 0.9545\n",
            "0.9636 0.9579\n",
            "0.9663333333333334 0.9608\n",
            "0.9667166666666667 0.9609\n",
            "0.9706833333333333 0.965\n",
            "0.9728 0.9657\n",
            "0.9729666666666666 0.9665\n",
            "0.9751166666666666 0.9683\n",
            "0.97425 0.9672\n",
            "0.97835 0.9711\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ch 6. 학습 관련 기술들 "
      ],
      "metadata": {
        "id": "54k0tzYul4Zo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 최적화 : 신경망의 최적화는 굉장히 어려운 문제 , 매개변수 공간은 매우넓고 복잡해서 최적의 솔루션은 쉽게 못찾음\n",
        "# 매개변수 갱신\n",
        "# SGD보다 좋은 최적화 방법들을 알아보고자 함\n"
      ],
      "metadata": {
        "id": "vLPs7Syhl6RJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. SGD\n",
        "class SGD:\n",
        "  def __init__(self, lr = 0.01):\n",
        "    self.lr = lr\n",
        "  \n",
        "  def update(self, params, grads):\n",
        "    for key in params.keys():\n",
        "      params[key] -= self.lr * grads[key]"
      ],
      "metadata": {
        "id": "xna-wjVVqTVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#network = TwoLayerNet(...)\n",
        "#optimizer = SGD() #  이부분을 바꾸면됨\n",
        "#for i in range(10000):\n",
        "# x_batch, t_batch = get_mini_batch(...)\n",
        "# grads = network.gradient(x+batch, t_batch)\n",
        "# params = network.params\n",
        "# optimizer.update(params, grads) \n",
        "\n",
        "# SGD의 단점 : 단순하고 구현도 쉽지만, 비등방성함수에서는 탐색경로가 비효율적 ,무작정 기울어진 방향으로 진행하는 단순한 방식"
      ],
      "metadata": {
        "id": "JpRjOfCb_Jy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Momentum (운동량)\n",
        "class Momentum():\n",
        "  def __init__(self, lr = 0.01, momentum = 0.9): # v 물체의 속도\n",
        "    self.lr = lr\n",
        "    self.momentum = momentum\n",
        "    self.v = None\n",
        "  \n",
        "  def update(self, params, grads):\n",
        "    if self.v is None:\n",
        "      self.v = {}\n",
        "      for key, val in params.items():\n",
        "        self.v[key] = np.zeros_like(val)  # 초기 array생성해줌\n",
        "      \n",
        "    for key in params.keys():\n",
        "      self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n",
        "      params[key] += self.v[key]  # params = Weight\n"
      ],
      "metadata": {
        "id": "JZJ-2xrYBYh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. AdaGrad\n",
        "# 신경망 학습에서 학습률값이 중요\n",
        "# 이 값이 너무 작으면 학습시간이 너무 길어지고, 너무크면 발산하여 학습이 제대로 이뤄지지 않음\n",
        "# 학습률을 정하는 효과적기술로 학습률 감소가 있음\n",
        "# 학습을 진행하면서 학습률을 점차 줄여가는 방법\n",
        "# 처음에는 크게 학습하다가 조금씩 작게 학습한다는 것 (실제 신경망 학습에 자주 사용됨)\n",
        "# 학습률을 서서히 낮추는 가장 간단한 방법은 매개변수 전체의 학습률 값을 일괄적으로 낮추는것\n",
        "# 각각의 매개변수에 맞춤형값을 만들어줌\n",
        "# 학습을 진행할수록 갱신강도가 약해짐 실제로 무한히 계속 학습하다보면  어느순간 갱신량이 0이되어 전혀 갱신되지 않음\n",
        "class AdaGrad:\n",
        "  def __init__(self, lr = 0.01):\n",
        "    self.lr = lr\n",
        "    self.h = None\n",
        "  def update(self , params, grads):\n",
        "    if self.h is None:\n",
        "      self.h = {}\n",
        "      for key , val in params.items():\n",
        "        self.h[key] = np.zeros_like(val)\n",
        "    for key in params.keys():\n",
        "      self.h[key] += grads[key] * grads[key]\n",
        "      params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)  # 아주 작은값을 추가로더해줌\n",
        "\n",
        "# 최솟값을 향해 효율적으로 움직임, 처음에는 크게 움직이지만 그 큰 움직임에 비례해 갱신 정도도 큰폭으로 작아지도록 조정\n",
        "# y축 방향으로 갱신속도가 빠르게 약해지고, 지그재그 움직임이 감소"
      ],
      "metadata": {
        "id": "siZAuWfpG2np"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Adam\n",
        "# 모멘텀은 공이 그릇 바닥을 구르는듯함 움직임을 보였음. adagrad는 매개변수의 원소마다 적응적으로 갱신정도를 조정함\n",
        "# 2015년에 제안된 방법 , 이론은 다소 복잡하지만 직관적으로는 모멘텀과 adagrad를 융합한 방법\n",
        "# 편향보정이 진행됨\n",
        "# 모멘텀과 비슷한 패턴인데, 공의 좌우 흔들림이 적음\n",
        "# 학습의 갱신속도를 적응적으로 조정해서 얻는 혜택이 있음\n",
        "# 파라미터가 3개임 : 지금까지의 학습률, 일차모멘텀용 계수, 이차 모멘텀용 계수 (기본값은 0.9, 0.999)\n"
      ],
      "metadata": {
        "id": "1CJFTSkIv3e2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 주로, SGD, ADAM을 사용"
      ],
      "metadata": {
        "id": "a3F819U0w9Cy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 가중치감소 (Weight decay) : 가중치 매개변수의 값이 작아지도록 학습하는 방법, 가중치값을 작게하여 오버피팅이 일어나지 않게 하는것\n",
        "# 가중치 초깃값을 0으로 하면 학습이 올바로 이뤄지지않음\n",
        "# 이유 : 오차역전파법에서 모든 가중치의 값이 똑같이 갱신됨\n",
        "# 가중치가 고르게 되어버리는 상황을 막으려면 (가중치의 대칭적인 구조를 무너뜨리려면) 초깃값을 무작위로 설정해야함"
      ],
      "metadata": {
        "id": "qdYlNDOBx1rq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 은닉층의 활성화값 분포\n",
        "# 가중치의 초깃값에 따라 은닉층 활성화값들이 어떻게 변하는지 실험\n",
        "# weight_init_activation_histogramp.py 참고하기\n",
        "# coding: utf-8\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def ReLU(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n"
      ],
      "metadata": {
        "id": "4vLMyFlVzgFm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_layer_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Abof2jSr1hlT",
        "outputId": "c25d8850-48d4-46f5-82a6-13fb5dc8d54f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "######################################################\n",
        "input_data = np.random.randn(1000, 100)  # 1000개의 데이터\n",
        "node_num = 100  # 각 은닉층의 노드(뉴런) 수\n",
        "hidden_layer_size = 5  # 은닉층이 5개\n",
        "activations = {}  # 이곳에 활성화 결과를 저장\n",
        "\n",
        "x = input_data\n",
        "\n",
        "for i in range(hidden_layer_size):\n",
        "    if i != 0: # 맨첨에는 여길 안지남\n",
        "        x = activations[i-1]\n",
        "\n",
        "    # 초깃값을 다양하게 바꿔가며 실험해보자！\n",
        "    # w = np.random.randn(node_num, node_num) * 1\n",
        "    # w = np.random.randn(node_num, node_num) * 0.01\n",
        "    # w = np.random.randn(node_num, node_num) * np.sqrt(1.0 / node_num)\n",
        "    w = np.random.randn(node_num, node_num) * np.sqrt(2.0 / node_num)\n",
        "\n",
        "    a = np.dot(x, w)\n",
        "\n",
        "    # 활성화 함수도 바꿔가며 실험해보자！\n",
        "    z = sigmoid(a)\n",
        "    # z = ReLU(a)\n",
        "    # z = tanh(a)\n",
        "\n",
        "    activations[i] = z # activation의 값을 하나씩 채워나감\n",
        "\n",
        "# 히스토그램 그리기\n",
        "for i, a in activations.items(): \n",
        "    plt.subplot(1, len(activations), i+1)\n",
        "    plt.title(str(i+1) + \"-layer\")\n",
        "    if i != 0:\n",
        "        plt.yticks([], [])\n",
        "    # plt.xlim(0.1, 1)\n",
        "    # plt.ylim(0, 7000)\n",
        "    plt.hist(a.flatten(), 30, range=(0, 1))\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "mCoskMWe0ihe",
        "outputId": "2aba395b-ab28-446b-9ed1-4fc73130a481"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEICAYAAABYoZ8gAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAZ0ElEQVR4nO3df5BdZX3H8ffH8MsWakDWNCbBjbJKQ1sjXQMdWougJEDb4Iw6oQqpg7NWk1b7k9DpFIqmxRkV6xTTRhMJ/oKMWNlCWpoi1KEjkI1gJCCyhdAkDWQ1BEKR2NBv/zjPwnX3bvbevXfvPfc+n9fMnT33Oc859znf3f2e55zz3HMUEZiZWT5e1u4GmJlZaznxm5llxonfzCwzTvxmZplx4jczy4wTv5lZZrJK/JJ2SHpbu9tRNo5LdY7LeJJC0sntbkeZdGJMOj7xS1opaUjSQUnXtbs9ZSDpaEnrJD0u6YCk+yWd1+52lYGkL0naI+kZST+Q9P52t6ksJPVJel7Sl9rdlnaTdGeKxbPp9XC729RMHZ/4gf8GPgasb3dDqpF0RBs+9ghgJ/AbwCuAvwA2SuptQ1uqalNcAP4G6I2InwN+G/iYpF9pU1vGaWNcAK4FtrTx86uSNKNNH70yIo5Nrze0qQ1VNRqTjk/8EfH1iPgG8KN6lpO0SNK3Je1PPcC/k3RUmnetpE+OqT8o6Q/T9Ksl3SRpRNJjkv6got6Vkr6WepbPAL/b8EbWKSL+JyKujIgdEfF/EXEL8BgwaYLr5rgARMT2iDg4+ja9XjfZct0eF0nLgP3A7XUsc4Gk+9LR005JV1bMu1XS74+pv03SO9L0KZI2S9on6WFJ766od52kNZI2Sfof4K2Nbl+rdExMIqIrXhS9/usmqbMDeFua/hXgDIrecS/wEPCRNG8RxZHEy9L7E4HngFkUO8utwF8CRwGvBR4FFqe6VwL/C1yY6r68BLGZBTwPnOK4BMBnU7sD+A5wbM5xAX4O+AEwN7XnS4epG8DJafos4JdSu38ZeBK4MM17N3BPxXJvpOicHQX8LMUR6ftSPN8E/BBYkOpeBzwNnJnWfUwbYnInMJLa9R/AWd0Uk47v8U9VRGyNiLsj4lBE7AD+geLUCBFxL0WQz0nVlwF3RsSTwJuBnoi4KiJ+EhGPAp9LdUZ9OyK+EUVv+8et2qZqJB0JfBnYEBHfn6x+DnGJiA8BxwG/DnwdOHj4Jbo+Lh8F1kXErnoWiog7I+J7qd3bgK+SYgIMAq+X1JfeXwzcGBE/AX4T2BERX0jxvA+4CXhXxepvjoj/SOt+vpGNm6LLKHbSc4C1wD9JmvTIsFNi0rWJX9I/V1yYeU+V+a+XdIukJ9Ih9l9T9NRGbQDem6bfC3wxTb8GeHU65N8vaT/w5xS9u1E7m75BUyDpZRTt/gmwMpVlHxeAiHghIu6i6OV+MNe4SFoIvA24psq87RUx+fUq80+XdEc6hfU08HukmKTEdCPw3vR3eBE/HZPTx8TkPcDPV6y+rX8rEXFPRByIiIMRsYGi139+t8SknReSplVETDaKZQ1wH3BRRByQ9BHgnRXzvwQ8IOmNwC8A30jlO4HHIqKPibX9lqeSBKyjSDDnR8T/guNSxRHA6zKOy1kUp67+q/iT4VhghqQFEXHqJMt+Bfg74LyIeF7Spxm/M/wicBfwXER8O5XvBP49It5+mHWX7W8lAHVLTDq+xy/pCEnHADMo/mCPUW0jI44DngGelXQK8MHKmemwdwvFL+mmikPwe4EDki6T9HJJMyT9oqQ3N22jmmMNRQL6rTpPH3RtXCS9StIyScem9i2m6HXVckGzW+OyluLi9sL0+nvgVmBxDcseB+xLCW4R8DuVM1NS+z/gk7zUswW4heKUx8WSjkyvN0v6hcY3p3GSZkpaPJpL0hHgW4B/qWHxjohJxyd+iqGKPwZWURxi/ziVTeZPKH4pByjOud5Ypc4Gigs1L/6CIuIFivNxCylGyvwQ+DzFsMlSkPQa4AMUbXzicKcwqujauFD0mD4I7AKeAj5BcYF2sIZluzIuEfFcRDwx+gKeBZ6PiJEaFv8QcJWkAxQXrzdWqXM9RUxe/G5ARBwAzqW4zvHfwBPAx4GjG9qY5jmSYrDI6MXd36e4QPuDGpbtiJgoomxHVOUh6S0Uv5zXhAP1IselOsdlPEmXAAMR8WvtbktZlCEm3dDjnxYqRsN8GPi8/4lf4rhU57iMJ+lnKHrAa9vdlrIoS0yc+KtI59X2A7OBT7e5OaXhuFTnuIyXrp+MUIxj/0qbm1MKZYqJT/WYmWXGPX4zs8yUehz/iSeeGL29ve1uxrTbunXrDyOip9b6OcTFMamunrg4JtXlEJfJYlLqxN/b28vQ0FC7mzHtJD1eT/0c4uKYVFdPXByT6nKIy2Qx8akeM7PMOPGbmWWm5sSfvmp+n6Rb0vv5ku6RNCzpRr10b/Kj0/vhNL+3Yh2Xp/KH09AmMzNrsXp6/B+muAf5qI8D10TEyRRff780lV8KPJXKr0n1kLSA4uvIpwJLgM+qfU/WMTPLVk2JX9Jc4AKKe4yM3vnxbOBrqcoGigdJACxN70nzz0n1lwI3pNucPgYMUzzAwszMWqjWHv+ngT+juKscwCuB/RFxKL3fRfHAAtLPnQBp/tOp/ovlVZZ5kaQBFQ9PHxoZqeU+UWZmVo9JE7+k3wT2RsTWFrSHiFgbEf0R0d/TU/PQXDMzq1Et4/jPBH5b0vnAMRTP5/xbYKakI1Kvfi6wO9XfDcwDdqX74r+C4rmSo+WjKpcxM7MWmbTHHxGXR8TciOiluDj7zYh4D3AHLz2BaDlwc5oeTO9J87+Z7lY4CCxLo37mA30UD6kwM7MWamQc/2XAH0kapjiHvy6VrwNemcr/iOIBKUTEdoqHEjxI8SSbFekhFaXRu+rWdjehNHpX3ep4NMDxKzgG5fxbqOuWDRFxJ3Bnmn6UKqNy0gOF3zW2PM1bDayut5HtNPoL23H1BW1uiZlZc/ibu2Zmmck68R/uEGzsvLIdqrVKGQ9Tzawxpb47Zzs4yZlZt3Pit5/iHZ9Z98v6VI+ZWY66vsdfbVTO2F5trb3csfU80sfMOpF7/GbWEh4oUB5d3+M3ayUnNusE2ST+6fiHrFynT/uYWafwqR6zxL11y0XXJn6fT2wux9Kse3Rt4jczs+qc+JvERxhm1imc+M3MMtOVid89bzOziXVl4rep8Q6z+RxTKyMnfjOzzEya+CUdI+leSd+VtF3SX6Xy6yQ9Jun+9FqYyiXpM5KGJW2TdFrFupZLeiS9lk/0mWbWPTzwoXxq+ebuQeDsiHhW0pHAXZL+Oc3704j42pj651E8SL0POB1YA5wu6QTgCqAfCGCrpMGIeKoZGwI+rLbW8mM5rVNN2uOPwrPp7ZHpFYdZZClwfVrubmCmpNnAYmBzROxLyX4zsKSx5pePezdmVnY1neOXNEPS/cBeiuR9T5q1Op3OuUbS0alsDrCzYvFdqWyi8rGfNSBpSNLQyMhInZtjZmaTqSnxR8QLEbEQmAsskvSLwOXAKcCbgROAy5rRoIhYGxH9EdHf09PTjFWambVdmc4G1DWqJyL2A3cASyJiTzqdcxD4ArAoVdsNzKtYbG4qm6jczMxaqJZRPT2SZqbplwNvB76fztsjScCFwANpkUHgkjS65wzg6YjYA9wGnCvpeEnHA+emMusQZeqxmNnU1TKqZzawQdIMih3Fxoi4RdI3JfUAAu4Hfi/V3wScDwwDzwHvA4iIfZI+CmxJ9a6KiH3N25Ry8YiP7lXt0Z3+PVsnmTTxR8Q24E1Vys+eoH4AKyaYtx5YX2cbzdrGO3DrRv7mrplZZrJ59KKZtY6vBZWbe/xmZplxj9+y596p5aYrEr//cRvj+FkjPKqp83RF4jdrBe8grVv4HL/VzV/kMpu6Mvz/OPGbmWWmo0/1tHuvaWbWidzjNzPLjBO/WQ18dGndxInfzCwzTvxmZplx4p9mPkVgZmXjxG9m1mRlGKt/OE78ZmaZceK3KSt7r8bMqnPiNzPLTC0PWz9G0r2Svitpu6S/SuXzJd0jaVjSjZKOSuVHp/fDaX5vxbouT+UPS1o8XRtlZs3lI7vuUkuP/yBwdkS8EVgILJF0BvBx4JqIOBl4Crg01b8UeCqVX5PqIWkBsAw4FVgCfDY9wN3MrCuVdYc5aeKPwrPp7ZHpFcDZwNdS+QbgwjS9NL0nzT9HklL5DRFxMCIeA4aBRU3ZCrMS87UQK5uabtKWeuZbgZOBa4H/BPZHxKFUZRcwJ03PAXYCRMQhSU8Dr0zld1estnKZys8aAAYATjrppDo3x6w9nNitk9R0cTciXoiIhcBcil76KdPVoIhYGxH9EdHf09MzXR9jTeSkZ9ZZ6hrVExH7gTuAXwVmSho9YpgL7E7Tu4F5AGn+K4AfVZZXWcbMzFqkllE9PZJmpumXA28HHqLYAbwzVVsO3JymB9N70vxvRkSk8mVp1M98oA+4t1kbYlZ2PtdvZVHLOf7ZwIZ0nv9lwMaIuEXSg8ANkj4G3AesS/XXAV+UNAzsoxjJQ0Rsl7QReBA4BKyIiBeauzlmzeEEbd1s0sQfEduAN1Upf5Qqo3Ii4nngXROsazWwuv5mmplZs/ibuy3gQ3wzK5OOfeauE6lNB/9dWQ7c4zczy4wTv5lZZpz4zcwy48RvTeEL2Gado2Mv7lrjnKjN8uQev5lZZpz4zcwy48RvZpYZJ34zs8w48ZuZZcaJ38wsM078ZmaZceI3M8uME7+ZWWac+M3M2qRdtzqp5Zm78yTdIelBSdslfTiVXylpt6T70+v8imUulzQs6WFJiyvKl6SyYUmrpmeTzMzscGq5V88h4I8j4juSjgO2Stqc5l0TEZ+orCxpAcVzdk8FXg38m6TXp9nXUjysfRewRdJgRDzYjA2xchjtvey4+oI2t8TMJlLLM3f3AHvS9AFJDwFzDrPIUuCGiDgIPJYeuj76bN7h9KxeJN2Q6jrxm3UQ79w7X11355TUS/Hg9XuAM4GVki4BhiiOCp6i2CncXbHYLl7aUewcU356lc8YAAYATjrppHqaZ2Yt5Lu7dq6aL+5KOha4CfhIRDwDrAFeByykOCL4ZDMaFBFrI6I/Ivp7enqasUozm2Z+HkNnqanHL+lIiqT/5Yj4OkBEPFkx/3PALentbmBexeJzUxmHKTczK7VuOsVVy6geAeuAhyLiUxXlsyuqvQN4IE0PAsskHS1pPtAH3AtsAfokzZd0FMUF4MHmbIaZmdWqlh7/mcDFwPck3Z/K/hy4SNJCIIAdwAcAImK7pI0UF20PASsi4gUASSuB24AZwPqI2N7EbTEzsxrUMqrnLkBVZm06zDKrgdVVyjcdbjkzM5t+/uaumVlmnPjNzDLjxG9mlhknfjOzzDjxm5llxonfzCwzdd2rx7qDv1pvljf3+M3MMuMev5nVxEeK3cM9fjOzzDjxm1nT5Hh75k7cXid+M7OSqNxxTudO1InfzCwzHXdxtxMPq8zMysQ9fjOzzDjxm7VYjhdArVyc+M3MMlPLM3fnSbpD0oOStkv6cCo/QdJmSY+kn8enckn6jKRhSdsknVaxruWp/iOSlk/fZpWTe3pmVga19PgPAX8cEQuAM4AVkhYAq4DbI6IPuD29BziP4gHrfcAAsAaKHQVwBXA6sAi4YnRnYWZmrTNp4o+IPRHxnTR9AHgImAMsBTakahuAC9P0UuD6KNwNzJQ0G1gMbI6IfRHxFLAZWNLUrTEzs0nVdY5fUi/wJuAeYFZE7EmzngBmpek5wM6KxXalsonKx37GgKQhSUMjIyP1NM/MzGpQc+KXdCxwE/CRiHimcl5EBBDNaFBErI2I/ojo7+npacYqzcysQk2JX9KRFEn/yxHx9VT8ZDqFQ/q5N5XvBuZVLD43lU1Ubl3IF7LNyquWUT0C1gEPRcSnKmYNAqMjc5YDN1eUX5JG95wBPJ1OCd0GnCvp+HRR99xUZmZmLVRLj/9M4GLgbEn3p9f5wNXA2yU9ArwtvQfYBDwKDAOfAz4EEBH7gI8CW9LrqlRmlqUyHxH5iK27TXqvnoi4C9AEs8+pUj+AFROsaz2wvp4Gmpl1u1bvZP3NXTNrKR9NtF/H3Z3TzKbP2ITsBN2d3OM3M8uME7+ZWWac+M3MMuPEb2aWGSd+M7PMOPGbmWXGid/M2sJDRdvH4/jNzKagk3dc7vGbmU2ik5N8NU78ZmaZceLPTLf1XMysfk78ZmaZceI3M8uME7+ZWWac+M3MMuPEb2bWBu0caFHLw9bXS9or6YGKsisl7R7zDN7ReZdLGpb0sKTFFeVLUtmwpFXN3xQzM6tFLT3+64AlVcqviYiF6bUJQNICYBlwalrms5JmSJoBXAucBywALkp1rct5+KjloNMeJ1nLw9a/Jam3xvUtBW6IiIPAY5KGgUVp3nBEPAog6YZU98G6W2xmZg1p5Bz/Sknb0qmg41PZHGBnRZ1dqWyi8nEkDUgakjQ0MjLSQPPMzKyaqSb+NcDrgIXAHuCTzWpQRKyNiP6I6O/p6WnWas3MLJnS3Tkj4snRaUmfA25Jb3cD8yqqzk1lHKbczMxaaEo9fkmzK96+Axgd8TMILJN0tKT5QB9wL7AF6JM0X9JRFBeAB6febDMzm6pJe/ySvgqcBZwoaRdwBXCWpIVAADuADwBExHZJGyku2h4CVkTEC2k9K4HbgBnA+ojY3vStMTOzSdUyqueiKsXrDlN/NbC6SvkmYFNdrTMzs6bzN3fNzDLjxG9mlhknfjOzzDjxm5llxonfzCwzU/oCl5lZDjrpxmv1cI/fzKxkpnuH4x5/Jrq152Jm9XOP38wsM078ZmaZceI3M8uME7+ZWWac+M3MMuNRPWZmTdIpo+fc4zeztutddWvHJM1u4MRvZpYZJ36zNnJP19ph0sQvab2kvZIeqCg7QdJmSY+kn8enckn6jKRhSdsknVaxzPJU/xFJy6dnc6yMnNzMyqWWHv91wJIxZauA2yOiD7g9vQc4j+IB633AALAGih0FxbN6TwcWAVeM7izMzKy1Jk38EfEtYN+Y4qXAhjS9Abiwovz6KNwNzJQ0G1gMbI6IfRHxFLCZ8TsTMzNrgame458VEXvS9BPArDQ9B9hZUW9XKpuofBxJA5KGJA2NjIxMsXlmZjaRhi/uRkQA0YS2jK5vbUT0R0R/T09Ps1ZrZmbJVBP/k+kUDunn3lS+G5hXUW9uKpuo3MzMWmyqiX8QGB2Zsxy4uaL8kjS65wzg6XRK6DbgXEnHp4u656YyMzNrsUlv2SDpq8BZwImSdlGMzrka2CjpUuBx4N2p+ibgfGAYeA54H0BE7JP0UWBLqndVRIy9YGxmZi0waeKPiIsmmHVOlboBrJhgPeuB9XW1zsy6mr/f0R4d881dfwnILB/+X59eHZP4zcysOZz4zcwy48RvZlZi03Ga2w9i6XI+V2pmY7nH3wa+UG1m7eQev2XLO1/LlXv8ZmaZceI3M8uME7+ZWWZ8jt+y43P7ljv3+M3MMuPEb2aWGSd+axl/f8GsHJz4zcwy48RvZpYZJ34zs8w0lPgl7ZD0PUn3SxpKZSdI2izpkfTz+FQuSZ+RNCxpm6TTmrEBZmZWn2b0+N8aEQsjoj+9XwXcHhF9wO3pPcB5QF96DQBrmvDZZmZWp+n4AtdSioezA2wA7gQuS+XXp+fy3i1ppqTZEbFnGtpgVpVHFVmjuuFvqNEefwD/KmmrpIFUNqsimT8BzErTc4CdFcvuSmU/RdKApCFJQyMjIw02z8zMxmq0x/9rEbFb0quAzZK+XzkzIkJS1LPCiFgLrAXo7++va1l7STf0SsxsejTU44+I3ennXuAfgUXAk5JmA6Sfe1P13cC8isXnpjIza6NO+WJdp7SzE0w58Uv6WUnHjU4D5wIPAIPA8lRtOXBzmh4ELkmje84Anvb5fTOz2jRzx9fIqZ5ZwD9KGl3PVyLiXyRtATZKuhR4HHh3qr8JOB8YBp4D3tfAZ1sHG/3j3XH1BW1uiVmeppz4I+JR4I1Vyn8EnFOlPIAVU/08M7NW6fZTSv7mrplZZpz4zcwy4ydwmWWqjKczytimbuQev5lZZpz4zcwy48RvVgL+cpK1khO/mVlmnPjNzDLjxG9mlhkn/i7kc8VmdjhO/NY23kGZtYcTv5kB3hHnxInfzCwzTvxmZplx4jczy4wTv5lZZpz4zcwy48RvZpaZlt+PX9IS4G+BGcDnI+LqVrehW3XicDw/f9es9Vra45c0A7gWOA9YAFwkaUEr22BmlrtW9/gXAcPpQe1IugFYCjzY4nZYZjrxaMhsuigiWvdh0juBJRHx/vT+YuD0iFhZUWcAGEhv3wA8nKZPBH7Yssa2xug2vSYiempdSNII8DiOyYsqYlK5jm5RuT01x8Uxqc7/PyV85m5ErAXWji2XNBQR/W1o0rSZ6jaN/kIdk5dU/pF3W1wck/Ea2R7//7R+VM9uYF7F+7mpzMzMWqTViX8L0CdpvqSjgGXAYIvbYGaWtZae6omIQ5JWArdRDOdcHxHba1x83OmfLtDoNjkm07eOMnFMxnNMqqtpm1p6cdfMzNrP39w1M8uME7+ZWWY6IvFLWiLpYUnDkla1uz2NkLRe0l5JDzS4nq6JCTQnLo5J1XU4JtXXk3dcIqLUL4qLwP8JvBY4CvgusKDd7Wpge94CnAY84Jg0Ly6OiWPiuNQel07o8b94m4eI+AkwepuHjhQR3wL2NbiarooJNCUujsl4jkl12celExL/HGBnxftdqSxnjsl4jsl4jkl12celExK/mZk1USckft/mYTzHZDzHZDzHpLrs49IJid+3eRjPMRnPMRnPMaku+7iUPvFHxCFg9DYPDwEbo/bbPJSOpK8C3wbeIGmXpEvrXUe3xQQaj4tjMp5jUp3j4ls2mJllp/Q9fjMzay4nfjOzzDjxm5llxonfzCwzTvxmZplx4jczy4wTv5lZZv4fRpMW6yjsPfsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 5 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "######################################################\n",
        "input_data = np.random.randn(1000, 100)  # 1000개의 데이터\n",
        "node_num = 100  # 각 은닉층의 노드(뉴런) 수\n",
        "hidden_layer_size = 5  # 은닉층이 5개\n",
        "activations = {}  # 이곳에 활성화 결과를 저장\n",
        "\n",
        "x = input_data\n",
        "\n",
        "for i in range(hidden_layer_size):\n",
        "    if i != 0:\n",
        "        x = activations[i-1]\n",
        "\n",
        "    # 초깃값을 다양하게 바꿔가며 실험해보자！\n",
        "    # w = np.random.randn(node_num, node_num) * 1\n",
        "    # w = np.random.randn(node_num, node_num) * 0.01\n",
        "    w = np.random.randn(node_num, node_num) * np.sqrt(1.0 / node_num)\n",
        "    # w = np.random.randn(node_num, node_num) * np.sqrt(2.0 / node_num)\n",
        "\n",
        "    a = np.dot(x, w)\n",
        "\n",
        "    # 활성화 함수도 바꿔가며 실험해보자！\n",
        "    # z = sigmoid(a)\n",
        "    # z = ReLU(a)\n",
        "    z = tanh(a)\n",
        "\n",
        "    activations[i] = z\n",
        "\n",
        "# 히스토그램 그리기\n",
        "for i, a in activations.items():\n",
        "    plt.subplot(1, len(activations), i+1)\n",
        "    plt.title(str(i+1) + \"-layer\")\n",
        "    if i != 0:\n",
        "        plt.yticks([], [])\n",
        "    # plt.xlim(0.1, 1)\n",
        "    # plt.ylim(0, 7000)\n",
        "    plt.hist(a.flatten(), 30, range=(0, 1))\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "Tv0ko4_U1HrW",
        "outputId": "8fb0bfda-0339-430d-88ed-674801b75a3d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 5 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "######################################################\n",
        "input_data = np.random.randn(1000, 100)  # 1000개의 데이터\n",
        "node_num = 100  # 각 은닉층의 노드(뉴런) 수\n",
        "hidden_layer_size = 5  # 은닉층이 5개\n",
        "activations = {}  # 이곳에 활성화 결과를 저장\n",
        "\n",
        "x = input_data\n",
        "\n",
        "for i in range(hidden_layer_size):\n",
        "    if i != 0:\n",
        "        x = activations[i-1]\n",
        "\n",
        "    # 초깃값을 다양하게 바꿔가며 실험해보자！\n",
        "    w = np.random.randn(node_num, node_num) * 1\n",
        "    # w = np.random.randn(node_num, node_num) * 0.01\n",
        "    # w = np.random.randn(node_num, node_num) * np.sqrt(1.0 / node_num)\n",
        "    # w = np.random.randn(node_num, node_num) * np.sqrt(2.0 / node_num)\n",
        "\n",
        "    a = np.dot(x, w)\n",
        "\n",
        "    # 활성화 함수도 바꿔가며 실험해보자！\n",
        "    z = sigmoid(a)\n",
        "    # z = ReLU(a)\n",
        "    # z = tanh(a)\n",
        "\n",
        "    activations[i] = z\n",
        "\n",
        "# 히스토그램 그리기\n",
        "for i, a in activations.items():\n",
        "    plt.subplot(1, len(activations), i+1)\n",
        "    plt.title(str(i+1) + \"-layer\")\n",
        "    if i != 0:\n",
        "        plt.yticks([], [])\n",
        "    # plt.xlim(0.1, 1)\n",
        "    # plt.ylim(0, 7000)\n",
        "    plt.hist(a.flatten(), 30, range=(0, 1))\n",
        "\n",
        "plt.show()\n",
        "# 각 층의 활성화값들이 0과 1에 치우쳐져 있음\n",
        "# 데이터가 0/1에 치우쳐분포하게되면 역전파의 기울기 값이 점점 작아지다가 사라짐(기울기 소실)\n",
        "# 층을 깊게하는 딥러닝에서는 기울기 소실은 더 심각한 문제가됨\n",
        "# "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "43VI0U842yRr",
        "outputId": "a72cfb3f-8ee5-4234-b875-34d287892417"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 5 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "######################################################\n",
        "# 표준편차를 0.01로 바꿈\n",
        "input_data = np.random.randn(1000, 100)  # 1000개의 데이터\n",
        "node_num = 100  # 각 은닉층의 노드(뉴런) 수\n",
        "hidden_layer_size = 5  # 은닉층이 5개\n",
        "activations = {}  # 이곳에 활성화 결과를 저장\n",
        "\n",
        "x = input_data\n",
        "\n",
        "for i in range(hidden_layer_size):\n",
        "    if i != 0:\n",
        "        x = activations[i-1]\n",
        "\n",
        "    # 초깃값을 다양하게 바꿔가며 실험해보자！\n",
        "    # w = np.random.randn(node_num, node_num) * 1\n",
        "    w = np.random.randn(node_num, node_num) * 0.01\n",
        "    # w = np.random.randn(node_num, node_num) * np.sqrt(1.0 / node_num)\n",
        "    # w = np.random.randn(node_num, node_num) * np.sqrt(2.0 / node_num)\n",
        "\n",
        "    a = np.dot(x, w)\n",
        "\n",
        "    # 활성화 함수도 바꿔가며 실험해보자！\n",
        "    z = sigmoid(a)\n",
        "    # z = ReLU(a)\n",
        "    # z = tanh(a)\n",
        "\n",
        "    activations[i] = z\n",
        "\n",
        "# 히스토그램 그리기\n",
        "for i, a in activations.items():\n",
        "    plt.subplot(1, len(activations), i+1)\n",
        "    plt.title(str(i+1) + \"-layer\")\n",
        "    if i != 0:\n",
        "        plt.yticks([], [])\n",
        "    # plt.xlim(0.1, 1)\n",
        "    # plt.ylim(0, 7000)\n",
        "    plt.hist(a.flatten(), 30, range=(0, 1))\n",
        "\n",
        "plt.show()\n",
        "# 0.5 부근에 집중되어있음\n",
        "# 0과 1에 치우치진않지만 기울기 소실문제는 없지만, 활성화값들이 치우쳤다는것은 표현력 관점에서는 쿤문제가 있는것\n",
        "# 다수의 뉴런이 거의 같은값을 출력하고있으니 뉴런을 여러개 둔 의미가 없어짐\n",
        "# 각 층의 활성화값은 적당히 고루 분포되어야함\n",
        "# 층과 층 사이에 적당하게 다양한 데이터가 흐르게 해야 신경망학습이 효율적으로 이뤄짐\n",
        "# 반대로 치우친 데이터가 흐르면 기울기 소실이나 표현력 제한 문제에 빠져서 학습이 잘 이뤄지지않는경우가 생김\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "BxBjxjQLxGOm",
        "outputId": "2b498366-cd80-454f-9dbd-09ececa8bd27"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 5 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 벤지오, 글로로트 논문에서는 권장하는 가중치 초깃값인 : xavier 초깃값을 사용\n",
        "# xavier 초깃값은 일반적인 딥러닝 프레임워크들이 표준적으로 이용하고있음\n",
        "# 카페 프레임워크는 가중치 초깃값을 설정할때 인수로 xavier로 지정할 수 있음\n",
        "# 앞 계층의 노드가 n개라면 표준편차가 1/root(n)인 분포를 사용하면 된다는 결론\n"
      ],
      "metadata": {
        "id": "NliMgV466Zoa"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# xavier 초깃값을 사용하면 앞 층에 노드가 많을수록 대상 노드의 초깃값으로 설정하는 가중치가 좁게 퍼져나감\n",
        "\n",
        "######################################################\n",
        "# 표준편차를 0.01로 바꿈\n",
        "input_data = np.random.randn(1000, 100)  # 1000개의 데이터\n",
        "node_num = 100  # 각 은닉층의 노드(뉴런) 수\n",
        "hidden_layer_size = 5  # 은닉층이 5개\n",
        "activations = {}  # 이곳에 활성화 결과를 저장\n",
        "\n",
        "x = input_data\n",
        "\n",
        "for i in range(hidden_layer_size):\n",
        "    if i != 0:\n",
        "        x = activations[i-1]\n",
        "\n",
        "    # 초깃값을 다양하게 바꿔가며 실험해보자！\n",
        "    # w = np.random.randn(node_num, node_num) * 1\n",
        "    # w = np.random.randn(node_num, node_num) * 0.01\n",
        "    w = np.random.randn(node_num, node_num) / np.sqrt(node_num)  # 사비에르 초깃값\n",
        "    # w = np.random.randn(node_num, node_num) * np.sqrt(2.0 / node_num)\n",
        "\n",
        "    a = np.dot(x, w)\n",
        "\n",
        "    # 활성화 함수도 바꿔가며 실험해보자！\n",
        "    z = sigmoid(a)\n",
        "    # z = ReLU(a)\n",
        "    # z = tanh(a)\n",
        "\n",
        "    activations[i] = z\n",
        "\n",
        "# 히스토그램 그리기\n",
        "for i, a in activations.items():\n",
        "    plt.subplot(1, len(activations), i+1)\n",
        "    plt.title(str(i+1) + \"-layer\")\n",
        "    if i != 0:\n",
        "        plt.yticks([], [])\n",
        "    # plt.xlim(0.1, 1)\n",
        "    # plt.ylim(0, 7000)\n",
        "    plt.hist(a.flatten(), 30, range=(0, 1))\n",
        "\n",
        "plt.show()\n",
        "# 층이 깊어지면서 갈수록 약간씩 일그러지고있음\n",
        "# 앞에서 본 방식보다는 확실히 넓게 분포함\n",
        "# 각층에 흐르는 데이터는 적당히 퍼져있으므로, 시그모이드 함수의 표현력도 제한받지 않음\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "iTQFik-w7iIU",
        "outputId": "868b1df6-133a-4487-ccbe-409c1130d573"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 5 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# HE초깃값\n",
        "# Sigmoid, tanh함수는 좌우대칭이라 중앙부근이 선형인 함수로 볼수있음\n",
        "# RELU를 사용할때는 이에 특화된 초깃값을 이용하라고 권장함 > he초깃값\n",
        "# 모든층에서 균일하게 분포함을 확인, 층이 깊어져도 분포가 균일하게 유지되기에 역전파 할때도 적절한 값이 나올것으로 기대됨\n"
      ],
      "metadata": {
        "id": "GlNzmONRtthl"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RELU => HE 초기값\n",
        "# Sigmoid, tanh => xavier 초기값"
      ],
      "metadata": {
        "id": "DyKaKt5BBJCF"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 배치정규화\n",
        "# 각층이 활성화를 적당히 퍼뜨리도록 강제하기\n",
        "# 2015년에 제안된 방법\n",
        "# 나온지 얼마 안된 기법임에도 많은 연구자와 기술자가 즐겨 사용하고있음\n",
        "# 학습을 빨리 진행할수있다(학습속도개선) + 초깃값에 크게 의존하지않음 + 오버피팅을 억제(드롭아웃등의 필요성 감소)\n",
        "# affine - BN - relu - affine - BN - relu ...\n",
        "# 학습시 미니배치를 단위로 정규화, 데이터 분포가 평균1, 분산1이 되도록 정규화 [x1....xm] m개의 입력데이터에 대한 평균과 분산\n",
        "# 해당작업을 활성화함수의 앞혹은 뒤에 삽입함으로써 데이터 분포가 덜 치우치게 할 수 있음\n",
        "# 배치 정규화 계층마다 이 정규화된 데이터에 고유한 확대와 이동변환을 수행함\n",
        "# y = gamma * x hat + beta\n",
        "# 효과 : 학습 속도를 높임"
      ],
      "metadata": {
        "id": "ymN6gww7BPRY"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#오버피팅 : 훈련데이터가 적거나 매개변수가 많고 표현력이 높은 모델에서 발생함\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
        "x_train = x_train[:300]\n",
        "t_train = t_train[:300]\n"
      ],
      "metadata": {
        "id": "kncBvJBXD0Y5"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from common.optimizer import SGD\n",
        "network = MultiLayerNet(input_size= 784, hidden_size_list= [100,100,100,100,100,100], output_size= 10)\n",
        "optimizer = SGD(lr = 0.01)"
      ],
      "metadata": {
        "id": "jiNpll1YEwQP"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_epochs = 201\n",
        "train_size = x_train.shape[0] ; \n",
        "batch_size = 100\n",
        "train_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUcU-JhhFbKf",
        "outputId": "9475af15-d40a-4906-d7bf-92f8041e3608"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "epoch_cnt = 0\n",
        "\n",
        "for i in range(100000):\n",
        "  batch_mask = np.random.choice(train_size, batch_size)\n",
        "  x_batch = x_train[batch_mask]\n",
        "  t_batch = t_train[batch_mask]\n",
        "\n",
        "  grads = network.gradient(x_batch, t_batch)\n",
        "  optimizer.update(network.params, grads)\n",
        "\n",
        "  if i % iter_per_epoch == 0 :\n",
        "    train_acc = network.accuracy(x_train, t_train)\n",
        "    test_acc = network.accuracy(x_test, t_test)\n",
        "    train_acc_list.append(train_acc)\n",
        "    test_acc_list.append(test_acc)\n",
        "\n",
        "    epoch_cnt += 1\n",
        "    if epoch_cnt >= max_epochs:\n",
        "      break"
      ],
      "metadata": {
        "id": "VmeId7dJGIkQ"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_acc_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSUr2wvmHP60",
        "outputId": "ba656e73-fcd4-4ec8-ed25-ff5a3cce5d69"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.0875, 0.0937, 0.1005, 0.1066, 0.1138, 0.124, 0.1372, 0.1482, 0.1621, 0.1774, 0.1989, 0.2213, 0.2482, 0.2638, 0.2822, 0.3013, 0.3093, 0.3388, 0.3672, 0.3751, 0.4025, 0.4028, 0.4257, 0.4258, 0.4477, 0.4789, 0.4868, 0.487, 0.5037, 0.5207, 0.539, 0.5459, 0.5589, 0.5659, 0.5815, 0.5946, 0.5978, 0.6046, 0.615, 0.6234, 0.6263, 0.627, 0.6263, 0.6379, 0.641, 0.6353, 0.6439, 0.6471, 0.6524, 0.6536, 0.6557, 0.6671, 0.6717, 0.6684, 0.6654, 0.6746, 0.6748, 0.6817, 0.6796, 0.6767, 0.6808, 0.6838, 0.6822, 0.6847, 0.6899, 0.6784, 0.6928, 0.6945, 0.6975, 0.6906, 0.7, 0.7019, 0.6976, 0.699, 0.6992, 0.6956, 0.6961, 0.6986, 0.6871, 0.6994, 0.7036, 0.6948, 0.7064, 0.7052, 0.7158, 0.7131, 0.7118, 0.7138, 0.7141, 0.7186, 0.7155, 0.7117, 0.7149, 0.7136, 0.721, 0.7149, 0.7202, 0.7141, 0.7193, 0.7192, 0.7208, 0.714, 0.722, 0.7216, 0.7198, 0.7244, 0.7236, 0.7276, 0.7252, 0.7257, 0.7284, 0.726, 0.727, 0.7267, 0.7265, 0.7278, 0.7279, 0.725, 0.7283, 0.726, 0.7252, 0.7234, 0.7303, 0.7279, 0.7319, 0.7286, 0.7292, 0.7294, 0.7277, 0.7308, 0.7315, 0.7293, 0.7236, 0.733, 0.7296, 0.7338, 0.7337, 0.7327, 0.7335, 0.7344, 0.7329, 0.7322, 0.7351, 0.7343, 0.7322, 0.7358, 0.7363, 0.7356, 0.7364, 0.736, 0.7378, 0.737, 0.738, 0.7366, 0.7353, 0.7369, 0.736, 0.7358, 0.7335, 0.7364, 0.7363, 0.7371, 0.7384, 0.7375, 0.7398, 0.7394, 0.7371, 0.7388, 0.741, 0.7404, 0.7417, 0.7413, 0.7372, 0.7406, 0.7396, 0.7391, 0.7395, 0.7406, 0.7404, 0.7387, 0.7386, 0.7376, 0.7384, 0.7375, 0.7382, 0.7392, 0.7397, 0.7412, 0.7418, 0.7425, 0.7415, 0.7403, 0.741, 0.7409, 0.7425, 0.7394, 0.7419, 0.7405, 0.7412, 0.7405, 0.7407]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_acc_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqnWNWW6Hg9z",
        "outputId": "ff250974-f875-4111-adbb-86c37f08a50d"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.06666666666666667, 0.08666666666666667, 0.1, 0.10666666666666667, 0.12, 0.14666666666666667, 0.18, 0.18666666666666668, 0.20333333333333334, 0.22333333333333333, 0.2633333333333333, 0.30666666666666664, 0.37, 0.4033333333333333, 0.4533333333333333, 0.4666666666666667, 0.49666666666666665, 0.5333333333333333, 0.5466666666666666, 0.55, 0.5766666666666667, 0.58, 0.6033333333333334, 0.5933333333333334, 0.6133333333333333, 0.64, 0.6566666666666666, 0.6566666666666666, 0.6666666666666666, 0.6933333333333334, 0.7033333333333334, 0.72, 0.7233333333333334, 0.7333333333333333, 0.7533333333333333, 0.7866666666666666, 0.7766666666666666, 0.7866666666666666, 0.8066666666666666, 0.8066666666666666, 0.8133333333333334, 0.82, 0.82, 0.8166666666666667, 0.8533333333333334, 0.84, 0.8533333333333334, 0.8733333333333333, 0.88, 0.88, 0.8833333333333333, 0.88, 0.8866666666666667, 0.8866666666666667, 0.8966666666666666, 0.9033333333333333, 0.9033333333333333, 0.9166666666666666, 0.9166666666666666, 0.9233333333333333, 0.9233333333333333, 0.9333333333333333, 0.9366666666666666, 0.93, 0.94, 0.9466666666666667, 0.9433333333333334, 0.9533333333333334, 0.95, 0.9533333333333334, 0.95, 0.9466666666666667, 0.9533333333333334, 0.95, 0.9566666666666667, 0.9533333333333334, 0.9533333333333334, 0.9566666666666667, 0.9666666666666667, 0.96, 0.96, 0.9666666666666667, 0.9633333333333334, 0.9633333333333334, 0.97, 0.9666666666666667, 0.97, 0.97, 0.97, 0.97, 0.9733333333333334, 0.9766666666666667, 0.9766666666666667, 0.9766666666666667, 0.98, 0.9766666666666667, 0.9833333333333333, 0.9833333333333333, 0.9866666666666667, 0.9833333333333333, 0.9866666666666667, 0.9766666666666667, 0.9833333333333333, 0.9833333333333333, 0.9866666666666667, 0.9866666666666667, 0.9833333333333333, 0.9833333333333333, 0.9866666666666667, 0.9866666666666667, 0.9833333333333333, 0.9866666666666667, 0.9866666666666667, 0.9866666666666667, 0.9866666666666667, 0.9866666666666667, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.9933333333333333, 0.9933333333333333, 0.99, 0.9933333333333333, 0.99, 0.9966666666666667, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 오버피팅 방지용: 가중치감소 (decaying weight)\n",
        "# 큰 가중치에 대해서는 그에 상응하는 큰 패널티를 부과하여 오버피팅을 억제하는 방법\n",
        "# 원래 오버피팅은 가중치 매개변수의 값이 커서 발생하는경우가 대다수\n",
        "# 손실함수에 가중치의 L2노름을 더한 가중치 감소방법\n",
        "\n",
        "\n",
        "# weight decay（가중치 감쇠） 설정 =======================\n",
        "#weight_decay_lambda = 0 # weight decay를 사용하지 않을 경우\n",
        "weight_decay_lambda = 0.1\n",
        "# ====================================================\n",
        "\n",
        "network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10,\n",
        "                        weight_decay_lambda=weight_decay_lambda)\n",
        "optimizer = SGD(lr=0.01) # 학습률이 0.01인 SGD로 매개변수 갱신\n",
        "\n",
        "max_epochs = 201\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "epoch_cnt = 0\n",
        "\n",
        "for i in range(1000000000):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "\n",
        "    grads = network.gradient(x_batch, t_batch)\n",
        "    optimizer.update(network.params, grads)\n",
        "\n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "\n",
        "        print(\"epoch:\" + str(epoch_cnt) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc))\n",
        "\n",
        "        epoch_cnt += 1\n",
        "        if epoch_cnt >= max_epochs:\n",
        "            break\n",
        "\n",
        "\n",
        "# 그래프 그리기==========\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
        "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "# 두 acc간의 간격이 줄어들었음"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Z9rVyruUHoRj",
        "outputId": "086ce135-bd91-4bd0-ed21-49c0ff974d0e"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:0, train acc:0.09, test acc:0.1061\n",
            "epoch:1, train acc:0.09666666666666666, test acc:0.1095\n",
            "epoch:2, train acc:0.11, test acc:0.1197\n",
            "epoch:3, train acc:0.13, test acc:0.1358\n",
            "epoch:4, train acc:0.15333333333333332, test acc:0.1599\n",
            "epoch:5, train acc:0.18333333333333332, test acc:0.1803\n",
            "epoch:6, train acc:0.24, test acc:0.2045\n",
            "epoch:7, train acc:0.2633333333333333, test acc:0.229\n",
            "epoch:8, train acc:0.2966666666666667, test acc:0.2439\n",
            "epoch:9, train acc:0.33, test acc:0.2594\n",
            "epoch:10, train acc:0.3433333333333333, test acc:0.2668\n",
            "epoch:11, train acc:0.35333333333333333, test acc:0.2786\n",
            "epoch:12, train acc:0.3933333333333333, test acc:0.2921\n",
            "epoch:13, train acc:0.4166666666666667, test acc:0.3048\n",
            "epoch:14, train acc:0.44333333333333336, test acc:0.3163\n",
            "epoch:15, train acc:0.4533333333333333, test acc:0.329\n",
            "epoch:16, train acc:0.4633333333333333, test acc:0.3444\n",
            "epoch:17, train acc:0.4866666666666667, test acc:0.3577\n",
            "epoch:18, train acc:0.5166666666666667, test acc:0.3679\n",
            "epoch:19, train acc:0.5333333333333333, test acc:0.3781\n",
            "epoch:20, train acc:0.52, test acc:0.3758\n",
            "epoch:21, train acc:0.53, test acc:0.3919\n",
            "epoch:22, train acc:0.56, test acc:0.4068\n",
            "epoch:23, train acc:0.5566666666666666, test acc:0.4131\n",
            "epoch:24, train acc:0.5733333333333334, test acc:0.4262\n",
            "epoch:25, train acc:0.5966666666666667, test acc:0.4325\n",
            "epoch:26, train acc:0.5833333333333334, test acc:0.4279\n",
            "epoch:27, train acc:0.58, test acc:0.4315\n",
            "epoch:28, train acc:0.5633333333333334, test acc:0.4214\n",
            "epoch:29, train acc:0.57, test acc:0.4353\n",
            "epoch:30, train acc:0.6, test acc:0.4538\n",
            "epoch:31, train acc:0.6433333333333333, test acc:0.4638\n",
            "epoch:32, train acc:0.6266666666666667, test acc:0.4765\n",
            "epoch:33, train acc:0.6433333333333333, test acc:0.489\n",
            "epoch:34, train acc:0.6566666666666666, test acc:0.4993\n",
            "epoch:35, train acc:0.67, test acc:0.5135\n",
            "epoch:36, train acc:0.6666666666666666, test acc:0.514\n",
            "epoch:37, train acc:0.6666666666666666, test acc:0.5268\n",
            "epoch:38, train acc:0.6733333333333333, test acc:0.5286\n",
            "epoch:39, train acc:0.68, test acc:0.5461\n",
            "epoch:40, train acc:0.6833333333333333, test acc:0.5497\n",
            "epoch:41, train acc:0.6933333333333334, test acc:0.5588\n",
            "epoch:42, train acc:0.7, test acc:0.5557\n",
            "epoch:43, train acc:0.7133333333333334, test acc:0.5526\n",
            "epoch:44, train acc:0.7166666666666667, test acc:0.5629\n",
            "epoch:45, train acc:0.7033333333333334, test acc:0.5752\n",
            "epoch:46, train acc:0.7166666666666667, test acc:0.5918\n",
            "epoch:47, train acc:0.7166666666666667, test acc:0.5877\n",
            "epoch:48, train acc:0.7133333333333334, test acc:0.5866\n",
            "epoch:49, train acc:0.7166666666666667, test acc:0.593\n",
            "epoch:50, train acc:0.7266666666666667, test acc:0.5992\n",
            "epoch:51, train acc:0.7166666666666667, test acc:0.5911\n",
            "epoch:52, train acc:0.72, test acc:0.594\n",
            "epoch:53, train acc:0.7233333333333334, test acc:0.5942\n",
            "epoch:54, train acc:0.7533333333333333, test acc:0.6052\n",
            "epoch:55, train acc:0.7333333333333333, test acc:0.6103\n",
            "epoch:56, train acc:0.75, test acc:0.6084\n",
            "epoch:57, train acc:0.74, test acc:0.6138\n",
            "epoch:58, train acc:0.7766666666666666, test acc:0.6217\n",
            "epoch:59, train acc:0.7833333333333333, test acc:0.6215\n",
            "epoch:60, train acc:0.77, test acc:0.618\n",
            "epoch:61, train acc:0.7466666666666667, test acc:0.6132\n",
            "epoch:62, train acc:0.75, test acc:0.6195\n",
            "epoch:63, train acc:0.7733333333333333, test acc:0.6322\n",
            "epoch:64, train acc:0.7733333333333333, test acc:0.6298\n",
            "epoch:65, train acc:0.79, test acc:0.6302\n",
            "epoch:66, train acc:0.7833333333333333, test acc:0.6243\n",
            "epoch:67, train acc:0.7866666666666666, test acc:0.6326\n",
            "epoch:68, train acc:0.8066666666666666, test acc:0.6495\n",
            "epoch:69, train acc:0.8033333333333333, test acc:0.6454\n",
            "epoch:70, train acc:0.8033333333333333, test acc:0.6482\n",
            "epoch:71, train acc:0.8133333333333334, test acc:0.6545\n",
            "epoch:72, train acc:0.82, test acc:0.6533\n",
            "epoch:73, train acc:0.83, test acc:0.6729\n",
            "epoch:74, train acc:0.83, test acc:0.6751\n",
            "epoch:75, train acc:0.84, test acc:0.6813\n",
            "epoch:76, train acc:0.8433333333333334, test acc:0.6857\n",
            "epoch:77, train acc:0.8433333333333334, test acc:0.6803\n",
            "epoch:78, train acc:0.8366666666666667, test acc:0.6778\n",
            "epoch:79, train acc:0.8466666666666667, test acc:0.6834\n",
            "epoch:80, train acc:0.8466666666666667, test acc:0.6802\n",
            "epoch:81, train acc:0.8466666666666667, test acc:0.6703\n",
            "epoch:82, train acc:0.8433333333333334, test acc:0.6788\n",
            "epoch:83, train acc:0.8266666666666667, test acc:0.6701\n",
            "epoch:84, train acc:0.85, test acc:0.6795\n",
            "epoch:85, train acc:0.8466666666666667, test acc:0.6802\n",
            "epoch:86, train acc:0.8466666666666667, test acc:0.6895\n",
            "epoch:87, train acc:0.8533333333333334, test acc:0.6939\n",
            "epoch:88, train acc:0.8366666666666667, test acc:0.69\n",
            "epoch:89, train acc:0.84, test acc:0.6914\n",
            "epoch:90, train acc:0.85, test acc:0.6903\n",
            "epoch:91, train acc:0.86, test acc:0.7017\n",
            "epoch:92, train acc:0.8566666666666667, test acc:0.7009\n",
            "epoch:93, train acc:0.8533333333333334, test acc:0.6977\n",
            "epoch:94, train acc:0.87, test acc:0.7062\n",
            "epoch:95, train acc:0.8666666666666667, test acc:0.701\n",
            "epoch:96, train acc:0.8633333333333333, test acc:0.6946\n",
            "epoch:97, train acc:0.8566666666666667, test acc:0.7054\n",
            "epoch:98, train acc:0.84, test acc:0.6859\n",
            "epoch:99, train acc:0.8733333333333333, test acc:0.7062\n",
            "epoch:100, train acc:0.88, test acc:0.7098\n",
            "epoch:101, train acc:0.87, test acc:0.698\n",
            "epoch:102, train acc:0.88, test acc:0.7025\n",
            "epoch:103, train acc:0.8733333333333333, test acc:0.7086\n",
            "epoch:104, train acc:0.8766666666666667, test acc:0.7055\n",
            "epoch:105, train acc:0.87, test acc:0.7103\n",
            "epoch:106, train acc:0.8766666666666667, test acc:0.7157\n",
            "epoch:107, train acc:0.8766666666666667, test acc:0.7096\n",
            "epoch:108, train acc:0.8733333333333333, test acc:0.7076\n",
            "epoch:109, train acc:0.8833333333333333, test acc:0.7156\n",
            "epoch:110, train acc:0.8766666666666667, test acc:0.7173\n",
            "epoch:111, train acc:0.8733333333333333, test acc:0.7097\n",
            "epoch:112, train acc:0.89, test acc:0.7179\n",
            "epoch:113, train acc:0.8933333333333333, test acc:0.7207\n",
            "epoch:114, train acc:0.8966666666666666, test acc:0.7148\n",
            "epoch:115, train acc:0.8833333333333333, test acc:0.7189\n",
            "epoch:116, train acc:0.88, test acc:0.7154\n",
            "epoch:117, train acc:0.8933333333333333, test acc:0.7247\n",
            "epoch:118, train acc:0.8933333333333333, test acc:0.7247\n",
            "epoch:119, train acc:0.9, test acc:0.7254\n",
            "epoch:120, train acc:0.9033333333333333, test acc:0.7286\n",
            "epoch:121, train acc:0.9066666666666666, test acc:0.7264\n",
            "epoch:122, train acc:0.8933333333333333, test acc:0.7241\n",
            "epoch:123, train acc:0.8966666666666666, test acc:0.7268\n",
            "epoch:124, train acc:0.9033333333333333, test acc:0.7309\n",
            "epoch:125, train acc:0.8933333333333333, test acc:0.7225\n",
            "epoch:126, train acc:0.9, test acc:0.7273\n",
            "epoch:127, train acc:0.91, test acc:0.7308\n",
            "epoch:128, train acc:0.9, test acc:0.7232\n",
            "epoch:129, train acc:0.9066666666666666, test acc:0.7265\n",
            "epoch:130, train acc:0.9066666666666666, test acc:0.7224\n",
            "epoch:131, train acc:0.9, test acc:0.7246\n",
            "epoch:132, train acc:0.9066666666666666, test acc:0.722\n",
            "epoch:133, train acc:0.9166666666666666, test acc:0.7231\n",
            "epoch:134, train acc:0.9033333333333333, test acc:0.7237\n",
            "epoch:135, train acc:0.8966666666666666, test acc:0.7252\n",
            "epoch:136, train acc:0.9033333333333333, test acc:0.7298\n",
            "epoch:137, train acc:0.9033333333333333, test acc:0.7282\n",
            "epoch:138, train acc:0.91, test acc:0.7237\n",
            "epoch:139, train acc:0.9066666666666666, test acc:0.7272\n",
            "epoch:140, train acc:0.9, test acc:0.7214\n",
            "epoch:141, train acc:0.91, test acc:0.7267\n",
            "epoch:142, train acc:0.9033333333333333, test acc:0.7225\n",
            "epoch:143, train acc:0.8966666666666666, test acc:0.718\n",
            "epoch:144, train acc:0.9033333333333333, test acc:0.7244\n",
            "epoch:145, train acc:0.9033333333333333, test acc:0.7208\n",
            "epoch:146, train acc:0.9066666666666666, test acc:0.7227\n",
            "epoch:147, train acc:0.9066666666666666, test acc:0.7318\n",
            "epoch:148, train acc:0.9033333333333333, test acc:0.7289\n",
            "epoch:149, train acc:0.91, test acc:0.7253\n",
            "epoch:150, train acc:0.9033333333333333, test acc:0.7186\n",
            "epoch:151, train acc:0.9133333333333333, test acc:0.7253\n",
            "epoch:152, train acc:0.91, test acc:0.7302\n",
            "epoch:153, train acc:0.9033333333333333, test acc:0.73\n",
            "epoch:154, train acc:0.9066666666666666, test acc:0.733\n",
            "epoch:155, train acc:0.91, test acc:0.7321\n",
            "epoch:156, train acc:0.9066666666666666, test acc:0.7244\n",
            "epoch:157, train acc:0.9033333333333333, test acc:0.7271\n",
            "epoch:158, train acc:0.9133333333333333, test acc:0.7218\n",
            "epoch:159, train acc:0.91, test acc:0.7256\n",
            "epoch:160, train acc:0.92, test acc:0.7277\n",
            "epoch:161, train acc:0.92, test acc:0.7305\n",
            "epoch:162, train acc:0.92, test acc:0.7314\n",
            "epoch:163, train acc:0.9133333333333333, test acc:0.7376\n",
            "epoch:164, train acc:0.91, test acc:0.7309\n",
            "epoch:165, train acc:0.9133333333333333, test acc:0.7268\n",
            "epoch:166, train acc:0.9133333333333333, test acc:0.7321\n",
            "epoch:167, train acc:0.9266666666666666, test acc:0.7288\n",
            "epoch:168, train acc:0.9166666666666666, test acc:0.7267\n",
            "epoch:169, train acc:0.9133333333333333, test acc:0.7296\n",
            "epoch:170, train acc:0.9233333333333333, test acc:0.7315\n",
            "epoch:171, train acc:0.9033333333333333, test acc:0.724\n",
            "epoch:172, train acc:0.9166666666666666, test acc:0.7282\n",
            "epoch:173, train acc:0.91, test acc:0.7277\n",
            "epoch:174, train acc:0.9266666666666666, test acc:0.7331\n",
            "epoch:175, train acc:0.9166666666666666, test acc:0.7332\n",
            "epoch:176, train acc:0.9133333333333333, test acc:0.7279\n",
            "epoch:177, train acc:0.9233333333333333, test acc:0.7232\n",
            "epoch:178, train acc:0.92, test acc:0.7204\n",
            "epoch:179, train acc:0.9133333333333333, test acc:0.7268\n",
            "epoch:180, train acc:0.9033333333333333, test acc:0.7232\n",
            "epoch:181, train acc:0.9066666666666666, test acc:0.7311\n",
            "epoch:182, train acc:0.9166666666666666, test acc:0.7326\n",
            "epoch:183, train acc:0.9133333333333333, test acc:0.7254\n",
            "epoch:184, train acc:0.9233333333333333, test acc:0.7315\n",
            "epoch:185, train acc:0.9233333333333333, test acc:0.734\n",
            "epoch:186, train acc:0.92, test acc:0.7283\n",
            "epoch:187, train acc:0.9133333333333333, test acc:0.7253\n",
            "epoch:188, train acc:0.92, test acc:0.7277\n",
            "epoch:189, train acc:0.9266666666666666, test acc:0.7315\n",
            "epoch:190, train acc:0.9166666666666666, test acc:0.723\n",
            "epoch:191, train acc:0.92, test acc:0.7309\n",
            "epoch:192, train acc:0.92, test acc:0.7272\n",
            "epoch:193, train acc:0.9233333333333333, test acc:0.7252\n",
            "epoch:194, train acc:0.9266666666666666, test acc:0.7331\n",
            "epoch:195, train acc:0.9133333333333333, test acc:0.7368\n",
            "epoch:196, train acc:0.9266666666666666, test acc:0.7345\n",
            "epoch:197, train acc:0.93, test acc:0.7327\n",
            "epoch:198, train acc:0.92, test acc:0.7294\n",
            "epoch:199, train acc:0.92, test acc:0.7282\n",
            "epoch:200, train acc:0.9133333333333333, test acc:0.7203\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 드랍아웃(drop out)\n",
        "# 뉴런을 임의로 삭제하면서 학습하는 방법\n",
        "# 훈련때 은닉층의 뉴런을 무작위로 골라 삭제함\n",
        "# 삭제된 뉴런은 신호를 전달하지 않게됨\n",
        "# 훈련때는 데이터를 흘릴때마다 삭제할 뉴런을 무작위로 선택하고, 시험할때는 모든 뉴런에 신호를 전달함\n",
        "# 시험때는 각 뉴런의 출력에 훈련때 삭제안한 비율을 곱하여 출력함\n",
        "class Dropout:\n",
        "  def __init__(self, dropout_ratio =0.5):\n",
        "    self.dropout_ratio = dropout_ratio \n",
        "    self.mask = None\n",
        "\n",
        "  def forward(self, x, train_flg = True): \n",
        "    if train_flg: #  train_flg = True 훈련시에는 순전파 때마다 삭제할 뉴런을 false로 표시\n",
        "      self.mask = np.random.rand(*x.shape) > self.dropout_ratio \n",
        "      return x*self.mask\n",
        "    \n",
        "    else : \n",
        "      return x*(1.0 - self.dropout_ratio)\n",
        "    \n",
        "  def backward(self, dout): # relu와 같음, 순전파때 신호를 통과시키는 뉴런은 역전파때도 신호를 그대로 통과시키고, 순전파때 통과시키지 않은 뉴런은 역전파때도 신호차단\n",
        "    return dout * self.mask \n"
      ],
      "metadata": {
        "id": "5hCKYi5ZK6wi"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### coding: utf-8\n",
        "import os\n",
        "import sys\n",
        "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
        "from common.trainer import Trainer\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
        "\n",
        "# 오버피팅을 재현하기 위해 학습 데이터 수를 줄임\n",
        "x_train = x_train[:300]\n",
        "t_train = t_train[:300]\n",
        "\n",
        "# 드롭아웃 사용 유무와 비울 설정 ========================\n",
        "use_dropout = True  # 드롭아웃을 쓰지 않을 때는 False\n",
        "dropout_ratio = 0.2\n",
        "# ====================================================\n",
        "\n",
        "network = MultiLayerNetExtend(input_size=784,\n",
        "                              hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
        "                              output_size=10, use_dropout=use_dropout,\n",
        "                              dropout_ration=dropout_ratio)\n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                  epochs=301, mini_batch_size=100,\n",
        "                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)\n",
        "trainer.train()\n",
        "\n",
        "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list\n",
        "\n",
        "# 그래프 그리기==========\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(len(train_acc_list))\n",
        "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
        "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "# epoch:301, train acc:0.73, test acc:0.6315"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HJD2a_L9NFbW",
        "outputId": "4af42106-42b8-4a2c-ab90-df398718b53e"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss:2.306904087919828\n",
            "=== epoch:1, train acc:0.11666666666666667, test acc:0.1137 ===\n",
            "train loss:2.3074513583364555\n",
            "train loss:2.3129723251611995\n",
            "train loss:2.3143567388681765\n",
            "=== epoch:2, train acc:0.12333333333333334, test acc:0.1151 ===\n",
            "train loss:2.2991808369175075\n",
            "train loss:2.3094032542619023\n",
            "train loss:2.306011326498018\n",
            "=== epoch:3, train acc:0.12333333333333334, test acc:0.1167 ===\n",
            "train loss:2.3033329908535944\n",
            "train loss:2.3000323486673095\n",
            "train loss:2.3056937763365584\n",
            "=== epoch:4, train acc:0.12666666666666668, test acc:0.1196 ===\n",
            "train loss:2.3073060611465404\n",
            "train loss:2.300166372610188\n",
            "train loss:2.302622768446473\n",
            "=== epoch:5, train acc:0.14, test acc:0.1224 ===\n",
            "train loss:2.3069459964240684\n",
            "train loss:2.2988684935546897\n",
            "train loss:2.3071026909632435\n",
            "=== epoch:6, train acc:0.15333333333333332, test acc:0.1259 ===\n",
            "train loss:2.306186961423251\n",
            "train loss:2.301121020339581\n",
            "train loss:2.3058770513184252\n",
            "=== epoch:7, train acc:0.15333333333333332, test acc:0.1309 ===\n",
            "train loss:2.3017391395756532\n",
            "train loss:2.3063763369628703\n",
            "train loss:2.298363408656353\n",
            "=== epoch:8, train acc:0.15666666666666668, test acc:0.133 ===\n",
            "train loss:2.2980521297340224\n",
            "train loss:2.3043129109729903\n",
            "train loss:2.295575157583544\n",
            "=== epoch:9, train acc:0.15666666666666668, test acc:0.1337 ===\n",
            "train loss:2.298150008078791\n",
            "train loss:2.2863290387842357\n",
            "train loss:2.302420920937618\n",
            "=== epoch:10, train acc:0.16, test acc:0.1393 ===\n",
            "train loss:2.297439258834989\n",
            "train loss:2.2979098761985357\n",
            "train loss:2.2871319141226483\n",
            "=== epoch:11, train acc:0.15666666666666668, test acc:0.1411 ===\n",
            "train loss:2.289596419697709\n",
            "train loss:2.2977255937484755\n",
            "train loss:2.2931612269786097\n",
            "=== epoch:12, train acc:0.15666666666666668, test acc:0.1442 ===\n",
            "train loss:2.3028951731187557\n",
            "train loss:2.301858618153913\n",
            "train loss:2.2993670028928768\n",
            "=== epoch:13, train acc:0.17333333333333334, test acc:0.1473 ===\n",
            "train loss:2.2932947782496553\n",
            "train loss:2.2819888811690454\n",
            "train loss:2.2896880265098627\n",
            "=== epoch:14, train acc:0.18, test acc:0.1491 ===\n",
            "train loss:2.2899695058882625\n",
            "train loss:2.2952871514645685\n",
            "train loss:2.297546573677085\n",
            "=== epoch:15, train acc:0.18666666666666668, test acc:0.1516 ===\n",
            "train loss:2.2962004784227243\n",
            "train loss:2.274002207959352\n",
            "train loss:2.295200328968659\n",
            "=== epoch:16, train acc:0.19, test acc:0.1547 ===\n",
            "train loss:2.2974644677916727\n",
            "train loss:2.2771563550711753\n",
            "train loss:2.289231366427881\n",
            "=== epoch:17, train acc:0.18, test acc:0.1573 ===\n",
            "train loss:2.284790172465138\n",
            "train loss:2.28651820861477\n",
            "train loss:2.275978159089175\n",
            "=== epoch:18, train acc:0.18666666666666668, test acc:0.1594 ===\n",
            "train loss:2.2943924274667764\n",
            "train loss:2.28495576230465\n",
            "train loss:2.283372579555562\n",
            "=== epoch:19, train acc:0.19, test acc:0.1628 ===\n",
            "train loss:2.2983373452427425\n",
            "train loss:2.2836442931964562\n",
            "train loss:2.279148977960182\n",
            "=== epoch:20, train acc:0.18333333333333332, test acc:0.1619 ===\n",
            "train loss:2.280942653969217\n",
            "train loss:2.2888413064327393\n",
            "train loss:2.283024127216895\n",
            "=== epoch:21, train acc:0.18666666666666668, test acc:0.1638 ===\n",
            "train loss:2.292930514232347\n",
            "train loss:2.2846936461095133\n",
            "train loss:2.2754809489285894\n",
            "=== epoch:22, train acc:0.19333333333333333, test acc:0.163 ===\n",
            "train loss:2.280464997267439\n",
            "train loss:2.280519021952914\n",
            "train loss:2.291698353659103\n",
            "=== epoch:23, train acc:0.18666666666666668, test acc:0.1647 ===\n",
            "train loss:2.287852624128848\n",
            "train loss:2.278862880546058\n",
            "train loss:2.2711607796082136\n",
            "=== epoch:24, train acc:0.19333333333333333, test acc:0.1667 ===\n",
            "train loss:2.2849536995688973\n",
            "train loss:2.2874703439321316\n",
            "train loss:2.2845288870446403\n",
            "=== epoch:25, train acc:0.20666666666666667, test acc:0.1646 ===\n",
            "train loss:2.276022558231393\n",
            "train loss:2.2878363189672966\n",
            "train loss:2.2933172278823064\n",
            "=== epoch:26, train acc:0.21, test acc:0.1681 ===\n",
            "train loss:2.285955159581875\n",
            "train loss:2.268052710959206\n",
            "train loss:2.284570416926056\n",
            "=== epoch:27, train acc:0.20666666666666667, test acc:0.1677 ===\n",
            "train loss:2.286473010813011\n",
            "train loss:2.273548795151767\n",
            "train loss:2.2835445424671663\n",
            "=== epoch:28, train acc:0.21333333333333335, test acc:0.1727 ===\n",
            "train loss:2.284754432202193\n",
            "train loss:2.2791458490203573\n",
            "train loss:2.2666113862605837\n",
            "=== epoch:29, train acc:0.20666666666666667, test acc:0.1716 ===\n",
            "train loss:2.2847314409953534\n",
            "train loss:2.277121692394297\n",
            "train loss:2.290007587012658\n",
            "=== epoch:30, train acc:0.21666666666666667, test acc:0.1755 ===\n",
            "train loss:2.271575861255738\n",
            "train loss:2.283838615963481\n",
            "train loss:2.284200394955252\n",
            "=== epoch:31, train acc:0.22666666666666666, test acc:0.177 ===\n",
            "train loss:2.2772031207259964\n",
            "train loss:2.276506803173833\n",
            "train loss:2.2647158265949714\n",
            "=== epoch:32, train acc:0.23666666666666666, test acc:0.1798 ===\n",
            "train loss:2.2794761794181437\n",
            "train loss:2.2799451043162136\n",
            "train loss:2.2719131002032107\n",
            "=== epoch:33, train acc:0.23333333333333334, test acc:0.1835 ===\n",
            "train loss:2.2752863429233163\n",
            "train loss:2.2689836343187206\n",
            "train loss:2.2744446078704352\n",
            "=== epoch:34, train acc:0.23666666666666666, test acc:0.1843 ===\n",
            "train loss:2.2835248970487445\n",
            "train loss:2.2629090511947045\n",
            "train loss:2.269366492804532\n",
            "=== epoch:35, train acc:0.23, test acc:0.1835 ===\n",
            "train loss:2.2683422016170662\n",
            "train loss:2.2691847663931206\n",
            "train loss:2.2679014031880937\n",
            "=== epoch:36, train acc:0.22666666666666666, test acc:0.1785 ===\n",
            "train loss:2.26986816191327\n",
            "train loss:2.2657540978950665\n",
            "train loss:2.279730294501559\n",
            "=== epoch:37, train acc:0.23333333333333334, test acc:0.1794 ===\n",
            "train loss:2.267593593371646\n",
            "train loss:2.2786135387127455\n",
            "train loss:2.27293980482691\n",
            "=== epoch:38, train acc:0.24, test acc:0.185 ===\n",
            "train loss:2.2735827032034184\n",
            "train loss:2.274242985437427\n",
            "train loss:2.266236489614\n",
            "=== epoch:39, train acc:0.25, test acc:0.1878 ===\n",
            "train loss:2.2701531203119845\n",
            "train loss:2.2860022747015334\n",
            "train loss:2.2698400365244895\n",
            "=== epoch:40, train acc:0.24, test acc:0.188 ===\n",
            "train loss:2.277896705165171\n",
            "train loss:2.2893125832378263\n",
            "train loss:2.2553602604272287\n",
            "=== epoch:41, train acc:0.24, test acc:0.1921 ===\n",
            "train loss:2.269861861516383\n",
            "train loss:2.264307862109794\n",
            "train loss:2.276239830168542\n",
            "=== epoch:42, train acc:0.24666666666666667, test acc:0.1962 ===\n",
            "train loss:2.265864411952216\n",
            "train loss:2.2576227046438526\n",
            "train loss:2.2731578269663286\n",
            "=== epoch:43, train acc:0.24, test acc:0.1938 ===\n",
            "train loss:2.2713940277998175\n",
            "train loss:2.2560119722501644\n",
            "train loss:2.2650409709165205\n",
            "=== epoch:44, train acc:0.25, test acc:0.192 ===\n",
            "train loss:2.257862099677848\n",
            "train loss:2.2577516699527935\n",
            "train loss:2.2763377645822604\n",
            "=== epoch:45, train acc:0.24666666666666667, test acc:0.1902 ===\n",
            "train loss:2.263320441665378\n",
            "train loss:2.2797715596660355\n",
            "train loss:2.2638312712716764\n",
            "=== epoch:46, train acc:0.24666666666666667, test acc:0.1892 ===\n",
            "train loss:2.2584354349248774\n",
            "train loss:2.2566301488380613\n",
            "train loss:2.2698532783723224\n",
            "=== epoch:47, train acc:0.25333333333333335, test acc:0.1891 ===\n",
            "train loss:2.2620824433057622\n",
            "train loss:2.262658989231016\n",
            "train loss:2.261643479420915\n",
            "=== epoch:48, train acc:0.25666666666666665, test acc:0.1924 ===\n",
            "train loss:2.2651135754162506\n",
            "train loss:2.262884735723517\n",
            "train loss:2.2716744968198985\n",
            "=== epoch:49, train acc:0.26, test acc:0.1952 ===\n",
            "train loss:2.266020658045521\n",
            "train loss:2.2606216320038217\n",
            "train loss:2.272434736107405\n",
            "=== epoch:50, train acc:0.2733333333333333, test acc:0.1937 ===\n",
            "train loss:2.2724038083123497\n",
            "train loss:2.263082036601819\n",
            "train loss:2.255633130462511\n",
            "=== epoch:51, train acc:0.27666666666666667, test acc:0.1932 ===\n",
            "train loss:2.2577786402941773\n",
            "train loss:2.2648105430322674\n",
            "train loss:2.2582796650186348\n",
            "=== epoch:52, train acc:0.2833333333333333, test acc:0.1969 ===\n",
            "train loss:2.25448675081547\n",
            "train loss:2.263751465019883\n",
            "train loss:2.2547108062388457\n",
            "=== epoch:53, train acc:0.27666666666666667, test acc:0.204 ===\n",
            "train loss:2.2540873616521013\n",
            "train loss:2.271896998581572\n",
            "train loss:2.255475990420511\n",
            "=== epoch:54, train acc:0.2833333333333333, test acc:0.1995 ===\n",
            "train loss:2.2519171366845314\n",
            "train loss:2.2738900934966666\n",
            "train loss:2.263909809807972\n",
            "=== epoch:55, train acc:0.27666666666666667, test acc:0.2086 ===\n",
            "train loss:2.2562453608264823\n",
            "train loss:2.2503062131116733\n",
            "train loss:2.2661260846169538\n",
            "=== epoch:56, train acc:0.2833333333333333, test acc:0.2084 ===\n",
            "train loss:2.254398428218565\n",
            "train loss:2.2558701049112546\n",
            "train loss:2.260044370580546\n",
            "=== epoch:57, train acc:0.28, test acc:0.2069 ===\n",
            "train loss:2.2525876956050155\n",
            "train loss:2.257209529333491\n",
            "train loss:2.245854988995294\n",
            "=== epoch:58, train acc:0.2833333333333333, test acc:0.2123 ===\n",
            "train loss:2.2531927624448738\n",
            "train loss:2.2524891509642826\n",
            "train loss:2.2560917564934213\n",
            "=== epoch:59, train acc:0.29, test acc:0.2139 ===\n",
            "train loss:2.267842029684307\n",
            "train loss:2.2513622078267654\n",
            "train loss:2.262763392231571\n",
            "=== epoch:60, train acc:0.2966666666666667, test acc:0.218 ===\n",
            "train loss:2.263826956298868\n",
            "train loss:2.2564613308596\n",
            "train loss:2.243291768266525\n",
            "=== epoch:61, train acc:0.3, test acc:0.2165 ===\n",
            "train loss:2.250645684156484\n",
            "train loss:2.2567429803856607\n",
            "train loss:2.253421481006773\n",
            "=== epoch:62, train acc:0.3, test acc:0.2173 ===\n",
            "train loss:2.2617978123537354\n",
            "train loss:2.2547615120172297\n",
            "train loss:2.2507311300792927\n",
            "=== epoch:63, train acc:0.3, test acc:0.2207 ===\n",
            "train loss:2.248072369504361\n",
            "train loss:2.2488319804352885\n",
            "train loss:2.2444333493499244\n",
            "=== epoch:64, train acc:0.30333333333333334, test acc:0.2229 ===\n",
            "train loss:2.2371803826922987\n",
            "train loss:2.252638566542211\n",
            "train loss:2.2410730803173884\n",
            "=== epoch:65, train acc:0.30333333333333334, test acc:0.2245 ===\n",
            "train loss:2.249149378002703\n",
            "train loss:2.2463010523627926\n",
            "train loss:2.237508607996275\n",
            "=== epoch:66, train acc:0.3, test acc:0.2261 ===\n",
            "train loss:2.2487962163252666\n",
            "train loss:2.2490399483687136\n",
            "train loss:2.2460014333908704\n",
            "=== epoch:67, train acc:0.31, test acc:0.2306 ===\n",
            "train loss:2.2350550901851216\n",
            "train loss:2.2461115884917775\n",
            "train loss:2.258767758045745\n",
            "=== epoch:68, train acc:0.31, test acc:0.2298 ===\n",
            "train loss:2.2346730815012186\n",
            "train loss:2.234019771851251\n",
            "train loss:2.2535080072345974\n",
            "=== epoch:69, train acc:0.31333333333333335, test acc:0.2276 ===\n",
            "train loss:2.252952943097485\n",
            "train loss:2.2346406733203286\n",
            "train loss:2.2528494487811885\n",
            "=== epoch:70, train acc:0.31333333333333335, test acc:0.2282 ===\n",
            "train loss:2.256702995617291\n",
            "train loss:2.2465248421670974\n",
            "train loss:2.239369713529368\n",
            "=== epoch:71, train acc:0.31, test acc:0.228 ===\n",
            "train loss:2.2496673557608586\n",
            "train loss:2.2295803207988643\n",
            "train loss:2.2336851766276005\n",
            "=== epoch:72, train acc:0.31, test acc:0.2261 ===\n",
            "train loss:2.2466693473358403\n",
            "train loss:2.2417594768328364\n",
            "train loss:2.243014999964867\n",
            "=== epoch:73, train acc:0.32, test acc:0.2346 ===\n",
            "train loss:2.243649370504224\n",
            "train loss:2.2274172218064217\n",
            "train loss:2.234036109234536\n",
            "=== epoch:74, train acc:0.31666666666666665, test acc:0.2327 ===\n",
            "train loss:2.2282395536635935\n",
            "train loss:2.229645275946224\n",
            "train loss:2.2367313398285504\n",
            "=== epoch:75, train acc:0.31666666666666665, test acc:0.2335 ===\n",
            "train loss:2.2191893952607384\n",
            "train loss:2.2313857845579927\n",
            "train loss:2.24248680607073\n",
            "=== epoch:76, train acc:0.31, test acc:0.2326 ===\n",
            "train loss:2.2371601096610543\n",
            "train loss:2.237760795416365\n",
            "train loss:2.23594602814928\n",
            "=== epoch:77, train acc:0.30333333333333334, test acc:0.2279 ===\n",
            "train loss:2.232837319740609\n",
            "train loss:2.2259797133745356\n",
            "train loss:2.231979821364078\n",
            "=== epoch:78, train acc:0.30333333333333334, test acc:0.2329 ===\n",
            "train loss:2.2287225540389715\n",
            "train loss:2.2408311435019517\n",
            "train loss:2.2236993716148863\n",
            "=== epoch:79, train acc:0.30666666666666664, test acc:0.2341 ===\n",
            "train loss:2.218816599311642\n",
            "train loss:2.2495602406206543\n",
            "train loss:2.2304985706825957\n",
            "=== epoch:80, train acc:0.30666666666666664, test acc:0.232 ===\n",
            "train loss:2.2257422613498345\n",
            "train loss:2.2186632861145745\n",
            "train loss:2.224906578876113\n",
            "=== epoch:81, train acc:0.31, test acc:0.2382 ===\n",
            "train loss:2.2300109796338377\n",
            "train loss:2.217693242824593\n",
            "train loss:2.2312140393576074\n",
            "=== epoch:82, train acc:0.32, test acc:0.2362 ===\n",
            "train loss:2.2362481898915543\n",
            "train loss:2.2203263380192446\n",
            "train loss:2.2072777139509476\n",
            "=== epoch:83, train acc:0.3233333333333333, test acc:0.2344 ===\n",
            "train loss:2.2327408994704365\n",
            "train loss:2.2282577676339006\n",
            "train loss:2.221506296619152\n",
            "=== epoch:84, train acc:0.31666666666666665, test acc:0.234 ===\n",
            "train loss:2.2288806552947453\n",
            "train loss:2.228865276069878\n",
            "train loss:2.2241070651263293\n",
            "=== epoch:85, train acc:0.32, test acc:0.2405 ===\n",
            "train loss:2.236411139223356\n",
            "train loss:2.211731451251734\n",
            "train loss:2.2398498513298337\n",
            "=== epoch:86, train acc:0.32, test acc:0.2397 ===\n",
            "train loss:2.214443995202289\n",
            "train loss:2.2086527527171507\n",
            "train loss:2.24238601548541\n",
            "=== epoch:87, train acc:0.33, test acc:0.2423 ===\n",
            "train loss:2.1919018424680057\n",
            "train loss:2.2179506743032893\n",
            "train loss:2.2104688948878346\n",
            "=== epoch:88, train acc:0.3333333333333333, test acc:0.2419 ===\n",
            "train loss:2.2053790515983125\n",
            "train loss:2.209415457309059\n",
            "train loss:2.2344132445179614\n",
            "=== epoch:89, train acc:0.34, test acc:0.2449 ===\n",
            "train loss:2.214356581376831\n",
            "train loss:2.210786653261873\n",
            "train loss:2.216265235567885\n",
            "=== epoch:90, train acc:0.35333333333333333, test acc:0.254 ===\n",
            "train loss:2.222241436312402\n",
            "train loss:2.20661774998101\n",
            "train loss:2.2064715930669694\n",
            "=== epoch:91, train acc:0.35333333333333333, test acc:0.2515 ===\n",
            "train loss:2.196731735951996\n",
            "train loss:2.1988693560122248\n",
            "train loss:2.216262395819584\n",
            "=== epoch:92, train acc:0.35, test acc:0.247 ===\n",
            "train loss:2.215912873985688\n",
            "train loss:2.217403579907071\n",
            "train loss:2.21350770053421\n",
            "=== epoch:93, train acc:0.3566666666666667, test acc:0.2449 ===\n",
            "train loss:2.1932801149145273\n",
            "train loss:2.2047224368264584\n",
            "train loss:2.188137467962079\n",
            "=== epoch:94, train acc:0.3566666666666667, test acc:0.2446 ===\n",
            "train loss:2.1907303595861167\n",
            "train loss:2.1971366335704623\n",
            "train loss:2.195691907271262\n",
            "=== epoch:95, train acc:0.3466666666666667, test acc:0.2481 ===\n",
            "train loss:2.200880423104999\n",
            "train loss:2.2086647130384205\n",
            "train loss:2.2033781218104758\n",
            "=== epoch:96, train acc:0.35, test acc:0.2506 ===\n",
            "train loss:2.172091127249015\n",
            "train loss:2.2166013870129335\n",
            "train loss:2.2067686539459213\n",
            "=== epoch:97, train acc:0.35, test acc:0.2557 ===\n",
            "train loss:2.214611177965881\n",
            "train loss:2.1930631920363455\n",
            "train loss:2.174617262267091\n",
            "=== epoch:98, train acc:0.35333333333333333, test acc:0.2568 ===\n",
            "train loss:2.1965607323593925\n",
            "train loss:2.1949637080133844\n",
            "train loss:2.2222305587462206\n",
            "=== epoch:99, train acc:0.35333333333333333, test acc:0.2626 ===\n",
            "train loss:2.1970761385277204\n",
            "train loss:2.199315267705818\n",
            "train loss:2.1874684430296223\n",
            "=== epoch:100, train acc:0.3566666666666667, test acc:0.2598 ===\n",
            "train loss:2.184646875261067\n",
            "train loss:2.199966687583829\n",
            "train loss:2.1718064496833915\n",
            "=== epoch:101, train acc:0.35333333333333333, test acc:0.2616 ===\n",
            "train loss:2.1953334781166034\n",
            "train loss:2.19873491935748\n",
            "train loss:2.201784378637995\n",
            "=== epoch:102, train acc:0.36, test acc:0.2663 ===\n",
            "train loss:2.1896151669021298\n",
            "train loss:2.200635438729367\n",
            "train loss:2.1653743648048267\n",
            "=== epoch:103, train acc:0.37, test acc:0.2727 ===\n",
            "train loss:2.1870701141919\n",
            "train loss:2.192928704959216\n",
            "train loss:2.171058697760332\n",
            "=== epoch:104, train acc:0.37333333333333335, test acc:0.2746 ===\n",
            "train loss:2.2065363229578465\n",
            "train loss:2.1611730538971314\n",
            "train loss:2.200218710139999\n",
            "=== epoch:105, train acc:0.37333333333333335, test acc:0.2779 ===\n",
            "train loss:2.1922524674086343\n",
            "train loss:2.2024738410665843\n",
            "train loss:2.1901060330971203\n",
            "=== epoch:106, train acc:0.38, test acc:0.2788 ===\n",
            "train loss:2.174174840640894\n",
            "train loss:2.1831336574153766\n",
            "train loss:2.171493398632644\n",
            "=== epoch:107, train acc:0.38, test acc:0.2742 ===\n",
            "train loss:2.145091217876494\n",
            "train loss:2.1807471391304145\n",
            "train loss:2.1946845869199962\n",
            "=== epoch:108, train acc:0.38, test acc:0.274 ===\n",
            "train loss:2.1897502058344576\n",
            "train loss:2.1576822922085457\n",
            "train loss:2.202046521291197\n",
            "=== epoch:109, train acc:0.37, test acc:0.2741 ===\n",
            "train loss:2.2018157744169446\n",
            "train loss:2.1846707987828218\n",
            "train loss:2.1880818611596586\n",
            "=== epoch:110, train acc:0.37, test acc:0.2764 ===\n",
            "train loss:2.177123646253585\n",
            "train loss:2.1786300116166326\n",
            "train loss:2.1762921167055795\n",
            "=== epoch:111, train acc:0.38333333333333336, test acc:0.2813 ===\n",
            "train loss:2.168639322197503\n",
            "train loss:2.1533357161448325\n",
            "train loss:2.140850951994929\n",
            "=== epoch:112, train acc:0.3933333333333333, test acc:0.284 ===\n",
            "train loss:2.1833025476790042\n",
            "train loss:2.1705993001751187\n",
            "train loss:2.147683036438231\n",
            "=== epoch:113, train acc:0.38666666666666666, test acc:0.2774 ===\n",
            "train loss:2.161579479778893\n",
            "train loss:2.176971738207968\n",
            "train loss:2.1648153269254293\n",
            "=== epoch:114, train acc:0.38333333333333336, test acc:0.2785 ===\n",
            "train loss:2.1593132083859703\n",
            "train loss:2.1356256040254733\n",
            "train loss:2.17541785270775\n",
            "=== epoch:115, train acc:0.38, test acc:0.2753 ===\n",
            "train loss:2.197293277797256\n",
            "train loss:2.1819378575943147\n",
            "train loss:2.165121244496331\n",
            "=== epoch:116, train acc:0.38333333333333336, test acc:0.2758 ===\n",
            "train loss:2.163572676481898\n",
            "train loss:2.1547771498014057\n",
            "train loss:2.1556808566268852\n",
            "=== epoch:117, train acc:0.38333333333333336, test acc:0.2793 ===\n",
            "train loss:2.152113804849623\n",
            "train loss:2.149772968273968\n",
            "train loss:2.171847189747685\n",
            "=== epoch:118, train acc:0.39, test acc:0.2846 ===\n",
            "train loss:2.159897445252925\n",
            "train loss:2.1597515628837094\n",
            "train loss:2.161159769746639\n",
            "=== epoch:119, train acc:0.3933333333333333, test acc:0.2886 ===\n",
            "train loss:2.1561056435340675\n",
            "train loss:2.1508605395370695\n",
            "train loss:2.138895381708822\n",
            "=== epoch:120, train acc:0.39, test acc:0.2887 ===\n",
            "train loss:2.157021913884679\n",
            "train loss:2.137333548604773\n",
            "train loss:2.1544961615862657\n",
            "=== epoch:121, train acc:0.3933333333333333, test acc:0.2955 ===\n",
            "train loss:2.128443757710015\n",
            "train loss:2.119988206805135\n",
            "train loss:2.124089599320305\n",
            "=== epoch:122, train acc:0.3933333333333333, test acc:0.2935 ===\n",
            "train loss:2.137016948184352\n",
            "train loss:2.153000528585882\n",
            "train loss:2.1279298184296063\n",
            "=== epoch:123, train acc:0.4033333333333333, test acc:0.2918 ===\n",
            "train loss:2.1202208732738472\n",
            "train loss:2.135110111746646\n",
            "train loss:2.128025943044952\n",
            "=== epoch:124, train acc:0.4, test acc:0.2939 ===\n",
            "train loss:2.1375299312019598\n",
            "train loss:2.13764320861176\n",
            "train loss:2.150854530878291\n",
            "=== epoch:125, train acc:0.4066666666666667, test acc:0.2979 ===\n",
            "train loss:2.1281127819393357\n",
            "train loss:2.121603008113133\n",
            "train loss:2.1590203353787736\n",
            "=== epoch:126, train acc:0.41333333333333333, test acc:0.2998 ===\n",
            "train loss:2.1282058926221783\n",
            "train loss:2.090388890988351\n",
            "train loss:2.1524105697576066\n",
            "=== epoch:127, train acc:0.41333333333333333, test acc:0.3006 ===\n",
            "train loss:2.1318912432327037\n",
            "train loss:2.1224543405697944\n",
            "train loss:2.0961765798473575\n",
            "=== epoch:128, train acc:0.41, test acc:0.2987 ===\n",
            "train loss:2.113616725199556\n",
            "train loss:2.0926792297139736\n",
            "train loss:2.0664251996412815\n",
            "=== epoch:129, train acc:0.4066666666666667, test acc:0.2973 ===\n",
            "train loss:2.1394512478629015\n",
            "train loss:2.1076524730431583\n",
            "train loss:2.1427746684139417\n",
            "=== epoch:130, train acc:0.4066666666666667, test acc:0.3014 ===\n",
            "train loss:2.090541650180596\n",
            "train loss:2.1132005567937373\n",
            "train loss:2.0823853798364196\n",
            "=== epoch:131, train acc:0.4033333333333333, test acc:0.2988 ===\n",
            "train loss:2.126784899091324\n",
            "train loss:2.1191510052941793\n",
            "train loss:2.0795784125506156\n",
            "=== epoch:132, train acc:0.41, test acc:0.3009 ===\n",
            "train loss:2.114879701630065\n",
            "train loss:2.069277895908441\n",
            "train loss:2.135472915356744\n",
            "=== epoch:133, train acc:0.4033333333333333, test acc:0.2955 ===\n",
            "train loss:2.107188327345035\n",
            "train loss:2.1139886970010546\n",
            "train loss:2.1031906740339683\n",
            "=== epoch:134, train acc:0.4066666666666667, test acc:0.2939 ===\n",
            "train loss:2.103351409762226\n",
            "train loss:2.0758307967587935\n",
            "train loss:2.070350582877938\n",
            "=== epoch:135, train acc:0.41, test acc:0.301 ===\n",
            "train loss:2.1017191265442037\n",
            "train loss:2.064215984584889\n",
            "train loss:2.083465303901048\n",
            "=== epoch:136, train acc:0.4066666666666667, test acc:0.2995 ===\n",
            "train loss:2.0958708515201807\n",
            "train loss:2.087322501752456\n",
            "train loss:2.0723415107113254\n",
            "=== epoch:137, train acc:0.41, test acc:0.2993 ===\n",
            "train loss:2.0376519845337633\n",
            "train loss:2.111014468698444\n",
            "train loss:2.0960323195524753\n",
            "=== epoch:138, train acc:0.41, test acc:0.2998 ===\n",
            "train loss:2.115635623987662\n",
            "train loss:2.103028219848524\n",
            "train loss:2.142331230797625\n",
            "=== epoch:139, train acc:0.41333333333333333, test acc:0.304 ===\n",
            "train loss:2.0598928551510274\n",
            "train loss:2.099136992433174\n",
            "train loss:2.096004774780826\n",
            "=== epoch:140, train acc:0.41333333333333333, test acc:0.3051 ===\n",
            "train loss:2.0748025039591753\n",
            "train loss:2.121245793186135\n",
            "train loss:2.121889978771261\n",
            "=== epoch:141, train acc:0.4166666666666667, test acc:0.3078 ===\n",
            "train loss:2.042513144959203\n",
            "train loss:2.1325656441350413\n",
            "train loss:2.092676612766772\n",
            "=== epoch:142, train acc:0.41333333333333333, test acc:0.3105 ===\n",
            "train loss:2.106718268531035\n",
            "train loss:2.0786809369447803\n",
            "train loss:2.064935541716503\n",
            "=== epoch:143, train acc:0.4166666666666667, test acc:0.3118 ===\n",
            "train loss:2.027887815702735\n",
            "train loss:2.0985140658152712\n",
            "train loss:2.072495181181212\n",
            "=== epoch:144, train acc:0.42333333333333334, test acc:0.3154 ===\n",
            "train loss:2.094496270166395\n",
            "train loss:2.0498172718930268\n",
            "train loss:2.067847965683714\n",
            "=== epoch:145, train acc:0.4266666666666667, test acc:0.3153 ===\n",
            "train loss:2.065578906011668\n",
            "train loss:2.037046540262217\n",
            "train loss:2.108726533334373\n",
            "=== epoch:146, train acc:0.4266666666666667, test acc:0.3174 ===\n",
            "train loss:2.009023329215539\n",
            "train loss:2.015723674375165\n",
            "train loss:2.06798704364063\n",
            "=== epoch:147, train acc:0.42333333333333334, test acc:0.3119 ===\n",
            "train loss:2.0089193077199776\n",
            "train loss:2.04980304046375\n",
            "train loss:2.002214403820159\n",
            "=== epoch:148, train acc:0.4166666666666667, test acc:0.3071 ===\n",
            "train loss:1.9890537297847095\n",
            "train loss:2.0549450920226455\n",
            "train loss:2.0751705965770055\n",
            "=== epoch:149, train acc:0.4166666666666667, test acc:0.3088 ===\n",
            "train loss:2.039655670824267\n",
            "train loss:2.0830286020361104\n",
            "train loss:2.038348043903034\n",
            "=== epoch:150, train acc:0.4166666666666667, test acc:0.3139 ===\n",
            "train loss:2.0474204390916935\n",
            "train loss:1.9902030841681448\n",
            "train loss:2.015788025968123\n",
            "=== epoch:151, train acc:0.4266666666666667, test acc:0.3187 ===\n",
            "train loss:2.042402848740666\n",
            "train loss:1.9984823079556302\n",
            "train loss:2.052982727402805\n",
            "=== epoch:152, train acc:0.42333333333333334, test acc:0.3159 ===\n",
            "train loss:2.002759786073735\n",
            "train loss:2.0096590640427583\n",
            "train loss:2.0167427446780333\n",
            "=== epoch:153, train acc:0.42333333333333334, test acc:0.3151 ===\n",
            "train loss:2.031728377850249\n",
            "train loss:1.9917309453632048\n",
            "train loss:1.9610559422635458\n",
            "=== epoch:154, train acc:0.41, test acc:0.3171 ===\n",
            "train loss:2.082253799591617\n",
            "train loss:2.011504498425164\n",
            "train loss:2.024378639470476\n",
            "=== epoch:155, train acc:0.42, test acc:0.3235 ===\n",
            "train loss:1.999760482608285\n",
            "train loss:2.0518015677149504\n",
            "train loss:2.015309052295494\n",
            "=== epoch:156, train acc:0.42, test acc:0.3272 ===\n",
            "train loss:2.014733853066075\n",
            "train loss:2.0719670509581976\n",
            "train loss:1.9909895135560713\n",
            "=== epoch:157, train acc:0.4266666666666667, test acc:0.3335 ===\n",
            "train loss:2.073040927961538\n",
            "train loss:2.058829238012334\n",
            "train loss:1.9942919887534936\n",
            "=== epoch:158, train acc:0.44666666666666666, test acc:0.342 ===\n",
            "train loss:2.0146309674065708\n",
            "train loss:2.0257005178213205\n",
            "train loss:1.990991927518015\n",
            "=== epoch:159, train acc:0.44666666666666666, test acc:0.3427 ===\n",
            "train loss:1.9834743927296725\n",
            "train loss:1.9984705493629087\n",
            "train loss:1.957738046521213\n",
            "=== epoch:160, train acc:0.44333333333333336, test acc:0.344 ===\n",
            "train loss:2.0177234038951783\n",
            "train loss:1.974677270070205\n",
            "train loss:1.9749106708592807\n",
            "=== epoch:161, train acc:0.4533333333333333, test acc:0.3496 ===\n",
            "train loss:2.0079112725959773\n",
            "train loss:1.9755453550829314\n",
            "train loss:1.976138631567642\n",
            "=== epoch:162, train acc:0.46, test acc:0.3527 ===\n",
            "train loss:1.9238715599776253\n",
            "train loss:2.022158320267387\n",
            "train loss:1.9581760069037335\n",
            "=== epoch:163, train acc:0.4533333333333333, test acc:0.359 ===\n",
            "train loss:1.9304848527892389\n",
            "train loss:1.913845776600473\n",
            "train loss:1.9962366347429727\n",
            "=== epoch:164, train acc:0.44, test acc:0.3555 ===\n",
            "train loss:1.9539545269689906\n",
            "train loss:2.0087683145184885\n",
            "train loss:1.9138212281836544\n",
            "=== epoch:165, train acc:0.44333333333333336, test acc:0.3554 ===\n",
            "train loss:1.9154900447993433\n",
            "train loss:1.939818389494221\n",
            "train loss:1.9947464992653337\n",
            "=== epoch:166, train acc:0.44, test acc:0.356 ===\n",
            "train loss:1.980100200163731\n",
            "train loss:1.9589067858632934\n",
            "train loss:2.0041154718074115\n",
            "=== epoch:167, train acc:0.44333333333333336, test acc:0.3601 ===\n",
            "train loss:1.8790106624793628\n",
            "train loss:1.9238226475903073\n",
            "train loss:1.8895454832698784\n",
            "=== epoch:168, train acc:0.44333333333333336, test acc:0.3594 ===\n",
            "train loss:1.8970252150549913\n",
            "train loss:1.9493897929602542\n",
            "train loss:1.887425405366636\n",
            "=== epoch:169, train acc:0.45, test acc:0.3616 ===\n",
            "train loss:1.8961389544099247\n",
            "train loss:1.919229206866126\n",
            "train loss:1.9565200263379048\n",
            "=== epoch:170, train acc:0.45, test acc:0.3657 ===\n",
            "train loss:1.8891574390801624\n",
            "train loss:1.9540000179708015\n",
            "train loss:1.9003250666523621\n",
            "=== epoch:171, train acc:0.45666666666666667, test acc:0.3641 ===\n",
            "train loss:1.909427083196908\n",
            "train loss:1.9657647541832142\n",
            "train loss:1.9544961287476168\n",
            "=== epoch:172, train acc:0.4666666666666667, test acc:0.3674 ===\n",
            "train loss:1.9344697441725835\n",
            "train loss:1.979386118523817\n",
            "train loss:1.9198895913231635\n",
            "=== epoch:173, train acc:0.4766666666666667, test acc:0.3749 ===\n",
            "train loss:1.9263715427817898\n",
            "train loss:1.9510704721157413\n",
            "train loss:1.86484671000788\n",
            "=== epoch:174, train acc:0.48333333333333334, test acc:0.3787 ===\n",
            "train loss:1.9919386833088044\n",
            "train loss:1.9217603602204603\n",
            "train loss:1.9762640794077553\n",
            "=== epoch:175, train acc:0.4866666666666667, test acc:0.3818 ===\n",
            "train loss:1.9134808350715042\n",
            "train loss:1.9155894272281215\n",
            "train loss:1.9497118612469118\n",
            "=== epoch:176, train acc:0.49, test acc:0.3854 ===\n",
            "train loss:1.9220745086154218\n",
            "train loss:1.9400657283150695\n",
            "train loss:1.9796075030601792\n",
            "=== epoch:177, train acc:0.49333333333333335, test acc:0.3888 ===\n",
            "train loss:1.9364724854301911\n",
            "train loss:1.944749540583868\n",
            "train loss:1.9426767995680236\n",
            "=== epoch:178, train acc:0.5, test acc:0.3939 ===\n",
            "train loss:1.9696820540087685\n",
            "train loss:1.8476880071970958\n",
            "train loss:1.7967869551312534\n",
            "=== epoch:179, train acc:0.5, test acc:0.3911 ===\n",
            "train loss:1.9106069071338485\n",
            "train loss:1.9803589146725145\n",
            "train loss:1.9227981487946781\n",
            "=== epoch:180, train acc:0.5, test acc:0.3973 ===\n",
            "train loss:1.8849798677372283\n",
            "train loss:1.9564274292643773\n",
            "train loss:1.8727708226425808\n",
            "=== epoch:181, train acc:0.5, test acc:0.3962 ===\n",
            "train loss:1.8722203265422157\n",
            "train loss:1.8290028924946464\n",
            "train loss:1.860924009315808\n",
            "=== epoch:182, train acc:0.5, test acc:0.3964 ===\n",
            "train loss:1.8286914347582826\n",
            "train loss:1.8783464200668203\n",
            "train loss:1.8199440989326725\n",
            "=== epoch:183, train acc:0.5, test acc:0.3954 ===\n",
            "train loss:1.981530394618344\n",
            "train loss:1.8136046930926495\n",
            "train loss:1.8536965955956988\n",
            "=== epoch:184, train acc:0.5066666666666667, test acc:0.3999 ===\n",
            "train loss:1.7523251455937885\n",
            "train loss:1.7899756419197559\n",
            "train loss:1.8237461788988236\n",
            "=== epoch:185, train acc:0.5066666666666667, test acc:0.3998 ===\n",
            "train loss:1.8143545639090402\n",
            "train loss:1.857595651701943\n",
            "train loss:1.7674198610712428\n",
            "=== epoch:186, train acc:0.5133333333333333, test acc:0.4066 ===\n",
            "train loss:1.8474664851315543\n",
            "train loss:1.8553070958676572\n",
            "train loss:1.7942515896640823\n",
            "=== epoch:187, train acc:0.5166666666666667, test acc:0.4041 ===\n",
            "train loss:1.8550637187035963\n",
            "train loss:1.8691457387907833\n",
            "train loss:1.784963885724986\n",
            "=== epoch:188, train acc:0.5166666666666667, test acc:0.4056 ===\n",
            "train loss:1.8244068902152069\n",
            "train loss:1.792837435211437\n",
            "train loss:1.8786942077533988\n",
            "=== epoch:189, train acc:0.5166666666666667, test acc:0.4061 ===\n",
            "train loss:1.814597475644628\n",
            "train loss:1.7893317932109445\n",
            "train loss:1.899948856354053\n",
            "=== epoch:190, train acc:0.49666666666666665, test acc:0.4032 ===\n",
            "train loss:1.8170979697871832\n",
            "train loss:1.843943521551898\n",
            "train loss:1.8655518400242486\n",
            "=== epoch:191, train acc:0.5133333333333333, test acc:0.4055 ===\n",
            "train loss:1.8350603918242212\n",
            "train loss:1.7997464975697801\n",
            "train loss:1.7999480555374938\n",
            "=== epoch:192, train acc:0.5, test acc:0.4094 ===\n",
            "train loss:1.7866252422001005\n",
            "train loss:1.8216441904734015\n",
            "train loss:1.7546686543373722\n",
            "=== epoch:193, train acc:0.5, test acc:0.409 ===\n",
            "train loss:1.822419105757431\n",
            "train loss:1.8569476643048386\n",
            "train loss:1.7099816666588616\n",
            "=== epoch:194, train acc:0.5133333333333333, test acc:0.4156 ===\n",
            "train loss:1.7776054687583338\n",
            "train loss:1.753232317270841\n",
            "train loss:1.7730418271374027\n",
            "=== epoch:195, train acc:0.51, test acc:0.4126 ===\n",
            "train loss:1.7652197380233121\n",
            "train loss:1.7783706758631244\n",
            "train loss:1.8506630750291981\n",
            "=== epoch:196, train acc:0.5166666666666667, test acc:0.4168 ===\n",
            "train loss:1.836957345431416\n",
            "train loss:1.786471487908322\n",
            "train loss:1.7640553351643478\n",
            "=== epoch:197, train acc:0.52, test acc:0.421 ===\n",
            "train loss:1.8028804142710744\n",
            "train loss:1.7890737409853588\n",
            "train loss:1.6778803774385713\n",
            "=== epoch:198, train acc:0.52, test acc:0.422 ===\n",
            "train loss:1.8070731232247093\n",
            "train loss:1.7733837804557846\n",
            "train loss:1.82309880474267\n",
            "=== epoch:199, train acc:0.5233333333333333, test acc:0.4251 ===\n",
            "train loss:1.798666451203217\n",
            "train loss:1.8114739623784286\n",
            "train loss:1.78496340058169\n",
            "=== epoch:200, train acc:0.5233333333333333, test acc:0.4302 ===\n",
            "train loss:1.780708322308581\n",
            "train loss:1.686585169448594\n",
            "train loss:1.7923741665111377\n",
            "=== epoch:201, train acc:0.52, test acc:0.4266 ===\n",
            "train loss:1.7441171372938284\n",
            "train loss:1.815818550578403\n",
            "train loss:1.7283637834553773\n",
            "=== epoch:202, train acc:0.52, test acc:0.4304 ===\n",
            "train loss:1.777808922141554\n",
            "train loss:1.7013422539943277\n",
            "train loss:1.7187872400620778\n",
            "=== epoch:203, train acc:0.52, test acc:0.4337 ===\n",
            "train loss:1.686043686905092\n",
            "train loss:1.6840869336863875\n",
            "train loss:1.714734794157857\n",
            "=== epoch:204, train acc:0.52, test acc:0.4331 ===\n",
            "train loss:1.769003002699847\n",
            "train loss:1.6049433489513283\n",
            "train loss:1.7340357861878704\n",
            "=== epoch:205, train acc:0.52, test acc:0.432 ===\n",
            "train loss:1.807231079908328\n",
            "train loss:1.797280066646879\n",
            "train loss:1.6829582508573844\n",
            "=== epoch:206, train acc:0.52, test acc:0.4372 ===\n",
            "train loss:1.6990162276184853\n",
            "train loss:1.6913741562233695\n",
            "train loss:1.677617418938214\n",
            "=== epoch:207, train acc:0.5233333333333333, test acc:0.4351 ===\n",
            "train loss:1.5832555956523242\n",
            "train loss:1.8050082312354219\n",
            "train loss:1.6844398871342008\n",
            "=== epoch:208, train acc:0.5233333333333333, test acc:0.4365 ===\n",
            "train loss:1.7306891272259417\n",
            "train loss:1.7225140190785382\n",
            "train loss:1.7127643769741527\n",
            "=== epoch:209, train acc:0.53, test acc:0.4397 ===\n",
            "train loss:1.6365040966642055\n",
            "train loss:1.6677548468029568\n",
            "train loss:1.640196701989991\n",
            "=== epoch:210, train acc:0.53, test acc:0.4402 ===\n",
            "train loss:1.5568437137772395\n",
            "train loss:1.725677910147706\n",
            "train loss:1.7470322299573682\n",
            "=== epoch:211, train acc:0.53, test acc:0.44 ===\n",
            "train loss:1.6795900508737975\n",
            "train loss:1.6051479319067186\n",
            "train loss:1.5146958221048854\n",
            "=== epoch:212, train acc:0.5333333333333333, test acc:0.4412 ===\n",
            "train loss:1.7611766224918428\n",
            "train loss:1.6308281360378805\n",
            "train loss:1.6218197328008024\n",
            "=== epoch:213, train acc:0.5433333333333333, test acc:0.4437 ===\n",
            "train loss:1.6435534301097812\n",
            "train loss:1.7002795466167329\n",
            "train loss:1.7692957412840342\n",
            "=== epoch:214, train acc:0.5466666666666666, test acc:0.4464 ===\n",
            "train loss:1.721195769870902\n",
            "train loss:1.6117142439391636\n",
            "train loss:1.586051095666336\n",
            "=== epoch:215, train acc:0.5433333333333333, test acc:0.4464 ===\n",
            "train loss:1.5363237070235705\n",
            "train loss:1.6210282162955978\n",
            "train loss:1.5777106277598185\n",
            "=== epoch:216, train acc:0.55, test acc:0.4492 ===\n",
            "train loss:1.680661741982356\n",
            "train loss:1.5219509343111017\n",
            "train loss:1.6263848682584068\n",
            "=== epoch:217, train acc:0.55, test acc:0.4508 ===\n",
            "train loss:1.5855931407910355\n",
            "train loss:1.5616710541618233\n",
            "train loss:1.629880409244595\n",
            "=== epoch:218, train acc:0.55, test acc:0.4497 ===\n",
            "train loss:1.540219571922173\n",
            "train loss:1.621515559089772\n",
            "train loss:1.5763563720449907\n",
            "=== epoch:219, train acc:0.55, test acc:0.451 ===\n",
            "train loss:1.4563717977325588\n",
            "train loss:1.6537977044358056\n",
            "train loss:1.584427380719208\n",
            "=== epoch:220, train acc:0.5466666666666666, test acc:0.4522 ===\n",
            "train loss:1.6321383897229402\n",
            "train loss:1.5428732401832985\n",
            "train loss:1.471072220518363\n",
            "=== epoch:221, train acc:0.5433333333333333, test acc:0.4539 ===\n",
            "train loss:1.5659399779733354\n",
            "train loss:1.6236123869316643\n",
            "train loss:1.4331255159280927\n",
            "=== epoch:222, train acc:0.5466666666666666, test acc:0.4548 ===\n",
            "train loss:1.524908853861751\n",
            "train loss:1.640715716837026\n",
            "train loss:1.6347279429505361\n",
            "=== epoch:223, train acc:0.5466666666666666, test acc:0.4554 ===\n",
            "train loss:1.5702674238032412\n",
            "train loss:1.6451054173769217\n",
            "train loss:1.6464741477772582\n",
            "=== epoch:224, train acc:0.55, test acc:0.4552 ===\n",
            "train loss:1.6452903966968009\n",
            "train loss:1.589705443803713\n",
            "train loss:1.6219653954457172\n",
            "=== epoch:225, train acc:0.55, test acc:0.4607 ===\n",
            "train loss:1.7003328660105959\n",
            "train loss:1.5430303602358304\n",
            "train loss:1.4886757860603026\n",
            "=== epoch:226, train acc:0.55, test acc:0.4643 ===\n",
            "train loss:1.6513519500223068\n",
            "train loss:1.6497044197524053\n",
            "train loss:1.4925101469649993\n",
            "=== epoch:227, train acc:0.5533333333333333, test acc:0.4708 ===\n",
            "train loss:1.4648799580496688\n",
            "train loss:1.5779748143495913\n",
            "train loss:1.6034167261377013\n",
            "=== epoch:228, train acc:0.5533333333333333, test acc:0.4726 ===\n",
            "train loss:1.5887855303765366\n",
            "train loss:1.5876902465530762\n",
            "train loss:1.5423750899385262\n",
            "=== epoch:229, train acc:0.5533333333333333, test acc:0.4779 ===\n",
            "train loss:1.4412337889931637\n",
            "train loss:1.5352273428923693\n",
            "train loss:1.51196758726398\n",
            "=== epoch:230, train acc:0.5533333333333333, test acc:0.4769 ===\n",
            "train loss:1.5967604803307958\n",
            "train loss:1.6179424194511076\n",
            "train loss:1.4452724150635903\n",
            "=== epoch:231, train acc:0.56, test acc:0.4796 ===\n",
            "train loss:1.5311030878613472\n",
            "train loss:1.6084987962722184\n",
            "train loss:1.4191951214630762\n",
            "=== epoch:232, train acc:0.5633333333333334, test acc:0.479 ===\n",
            "train loss:1.5694484615761504\n",
            "train loss:1.3678821973506845\n",
            "train loss:1.5622809169853307\n",
            "=== epoch:233, train acc:0.56, test acc:0.4793 ===\n",
            "train loss:1.486009991531702\n",
            "train loss:1.4481146329058754\n",
            "train loss:1.4760159425680723\n",
            "=== epoch:234, train acc:0.5633333333333334, test acc:0.4814 ===\n",
            "train loss:1.5188738488553335\n",
            "train loss:1.5140915870832006\n",
            "train loss:1.4876458023613102\n",
            "=== epoch:235, train acc:0.57, test acc:0.4855 ===\n",
            "train loss:1.4196392041690595\n",
            "train loss:1.5155902934301708\n",
            "train loss:1.581386679334679\n",
            "=== epoch:236, train acc:0.57, test acc:0.4881 ===\n",
            "train loss:1.4263808950508865\n",
            "train loss:1.5773393776933469\n",
            "train loss:1.4044558896520665\n",
            "=== epoch:237, train acc:0.5666666666666667, test acc:0.4877 ===\n",
            "train loss:1.4972737695443028\n",
            "train loss:1.4345354696356887\n",
            "train loss:1.554851887934928\n",
            "=== epoch:238, train acc:0.57, test acc:0.4939 ===\n",
            "train loss:1.4661589480055535\n",
            "train loss:1.4305240161341999\n",
            "train loss:1.4438458341162599\n",
            "=== epoch:239, train acc:0.57, test acc:0.4929 ===\n",
            "train loss:1.4677823804245147\n",
            "train loss:1.5602398758515117\n",
            "train loss:1.580013430797892\n",
            "=== epoch:240, train acc:0.57, test acc:0.4943 ===\n",
            "train loss:1.4573685197502728\n",
            "train loss:1.4689500826509527\n",
            "train loss:1.422553800991725\n",
            "=== epoch:241, train acc:0.57, test acc:0.4954 ===\n",
            "train loss:1.5679510672946215\n",
            "train loss:1.397984964280054\n",
            "train loss:1.4911718669146259\n",
            "=== epoch:242, train acc:0.58, test acc:0.5005 ===\n",
            "train loss:1.5819093729030917\n",
            "train loss:1.3963372193632728\n",
            "train loss:1.5260159293528526\n",
            "=== epoch:243, train acc:0.5833333333333334, test acc:0.5054 ===\n",
            "train loss:1.476549397243996\n",
            "train loss:1.3255436570353307\n",
            "train loss:1.3239310631872996\n",
            "=== epoch:244, train acc:0.59, test acc:0.5114 ===\n",
            "train loss:1.4455462030466362\n",
            "train loss:1.4570818799809673\n",
            "train loss:1.3722222764571168\n",
            "=== epoch:245, train acc:0.5966666666666667, test acc:0.5148 ===\n",
            "train loss:1.418390951541033\n",
            "train loss:1.5353405008288976\n",
            "train loss:1.4843750322550517\n",
            "=== epoch:246, train acc:0.5866666666666667, test acc:0.5128 ===\n",
            "train loss:1.3967331237433445\n",
            "train loss:1.363517431167545\n",
            "train loss:1.2181219988171013\n",
            "=== epoch:247, train acc:0.58, test acc:0.5098 ===\n",
            "train loss:1.3995614936183285\n",
            "train loss:1.498821518147804\n",
            "train loss:1.3688966214887832\n",
            "=== epoch:248, train acc:0.6033333333333334, test acc:0.5168 ===\n",
            "train loss:1.3856414279726657\n",
            "train loss:1.3136838267815785\n",
            "train loss:1.4187439639746313\n",
            "=== epoch:249, train acc:0.6033333333333334, test acc:0.519 ===\n",
            "train loss:1.4669429267007033\n",
            "train loss:1.318227477658573\n",
            "train loss:1.2645397603617035\n",
            "=== epoch:250, train acc:0.6, test acc:0.5151 ===\n",
            "train loss:1.491627529230637\n",
            "train loss:1.3165808015659217\n",
            "train loss:1.4942808276664459\n",
            "=== epoch:251, train acc:0.6066666666666667, test acc:0.5193 ===\n",
            "train loss:1.3410156427557118\n",
            "train loss:1.4216767644775528\n",
            "train loss:1.4194774430072337\n",
            "=== epoch:252, train acc:0.5966666666666667, test acc:0.5154 ===\n",
            "train loss:1.3014758320594837\n",
            "train loss:1.4169998300455395\n",
            "train loss:1.3863270246165151\n",
            "=== epoch:253, train acc:0.6033333333333334, test acc:0.5168 ===\n",
            "train loss:1.2882451367104093\n",
            "train loss:1.4601675082793815\n",
            "train loss:1.4541081948508974\n",
            "=== epoch:254, train acc:0.6133333333333333, test acc:0.5238 ===\n",
            "train loss:1.4800463824512284\n",
            "train loss:1.3031659778425992\n",
            "train loss:1.3743443351771685\n",
            "=== epoch:255, train acc:0.6133333333333333, test acc:0.5262 ===\n",
            "train loss:1.4553139837270106\n",
            "train loss:1.2609097986364346\n",
            "train loss:1.4444257348357403\n",
            "=== epoch:256, train acc:0.6066666666666667, test acc:0.5247 ===\n",
            "train loss:1.3405864202221205\n",
            "train loss:1.331109269004417\n",
            "train loss:1.3602234143606808\n",
            "=== epoch:257, train acc:0.61, test acc:0.5288 ===\n",
            "train loss:1.3600268950834309\n",
            "train loss:1.2699940731795514\n",
            "train loss:1.3552049544517057\n",
            "=== epoch:258, train acc:0.6166666666666667, test acc:0.5317 ===\n",
            "train loss:1.242611006956366\n",
            "train loss:1.2306050352419247\n",
            "train loss:1.3375351239094175\n",
            "=== epoch:259, train acc:0.6166666666666667, test acc:0.5319 ===\n",
            "train loss:1.3876017460793446\n",
            "train loss:1.3165298548601065\n",
            "train loss:1.3443935014297785\n",
            "=== epoch:260, train acc:0.63, test acc:0.5337 ===\n",
            "train loss:1.3269473183189577\n",
            "train loss:1.3275884873878263\n",
            "train loss:1.3867477058520743\n",
            "=== epoch:261, train acc:0.6266666666666667, test acc:0.5318 ===\n",
            "train loss:1.3036479631681916\n",
            "train loss:1.2776902811971589\n",
            "train loss:1.163618673886392\n",
            "=== epoch:262, train acc:0.6266666666666667, test acc:0.5357 ===\n",
            "train loss:1.439263649927762\n",
            "train loss:1.2651242290847424\n",
            "train loss:1.284564434501879\n",
            "=== epoch:263, train acc:0.63, test acc:0.5392 ===\n",
            "train loss:1.2399295106788886\n",
            "train loss:1.243918409032319\n",
            "train loss:1.231979986953438\n",
            "=== epoch:264, train acc:0.63, test acc:0.5395 ===\n",
            "train loss:1.3968869281030822\n",
            "train loss:1.1687632871602984\n",
            "train loss:1.3301565691425956\n",
            "=== epoch:265, train acc:0.6266666666666667, test acc:0.5388 ===\n",
            "train loss:1.2000801545248583\n",
            "train loss:1.209887607856296\n",
            "train loss:1.282363501799249\n",
            "=== epoch:266, train acc:0.6333333333333333, test acc:0.5397 ===\n",
            "train loss:1.1927773187285111\n",
            "train loss:1.2007912873487525\n",
            "train loss:1.1633150681197464\n",
            "=== epoch:267, train acc:0.6333333333333333, test acc:0.5399 ===\n",
            "train loss:1.2412967946668945\n",
            "train loss:1.167994998426466\n",
            "train loss:1.2823825177465724\n",
            "=== epoch:268, train acc:0.6233333333333333, test acc:0.5376 ===\n",
            "train loss:1.1753512483854345\n",
            "train loss:1.2970391091724813\n",
            "train loss:1.1404249539750542\n",
            "=== epoch:269, train acc:0.63, test acc:0.5393 ===\n",
            "train loss:1.2789982149535992\n",
            "train loss:1.1850017921740035\n",
            "train loss:1.097843016434915\n",
            "=== epoch:270, train acc:0.6333333333333333, test acc:0.5416 ===\n",
            "train loss:1.2321091006046823\n",
            "train loss:1.221397380713094\n",
            "train loss:1.2178794396418602\n",
            "=== epoch:271, train acc:0.6366666666666667, test acc:0.545 ===\n",
            "train loss:1.1237903384825116\n",
            "train loss:1.2733797878104136\n",
            "train loss:1.311194922253537\n",
            "=== epoch:272, train acc:0.6466666666666666, test acc:0.5481 ===\n",
            "train loss:1.235491741428977\n",
            "train loss:1.3586254412135907\n",
            "train loss:1.3094752969818109\n",
            "=== epoch:273, train acc:0.64, test acc:0.5518 ===\n",
            "train loss:1.2788980417521554\n",
            "train loss:1.0978886399584091\n",
            "train loss:1.1327544496565098\n",
            "=== epoch:274, train acc:0.65, test acc:0.5539 ===\n",
            "train loss:1.2048425084150498\n",
            "train loss:1.2563475824867725\n",
            "train loss:1.2348550306303638\n",
            "=== epoch:275, train acc:0.65, test acc:0.5541 ===\n",
            "train loss:1.1444087799925118\n",
            "train loss:1.1107123844947964\n",
            "train loss:1.153380275428801\n",
            "=== epoch:276, train acc:0.66, test acc:0.5552 ===\n",
            "train loss:1.2442768189737463\n",
            "train loss:0.9978690633374744\n",
            "train loss:1.2738432657739451\n",
            "=== epoch:277, train acc:0.6666666666666666, test acc:0.5599 ===\n",
            "train loss:1.063872603648515\n",
            "train loss:1.2015371084618809\n",
            "train loss:1.1036290030451976\n",
            "=== epoch:278, train acc:0.6566666666666666, test acc:0.5602 ===\n",
            "train loss:1.0351602140563905\n",
            "train loss:1.211607492727603\n",
            "train loss:1.2141650156247374\n",
            "=== epoch:279, train acc:0.6633333333333333, test acc:0.5603 ===\n",
            "train loss:1.177952428570669\n",
            "train loss:1.271313354692298\n",
            "train loss:1.2426278857454802\n",
            "=== epoch:280, train acc:0.67, test acc:0.5612 ===\n",
            "train loss:1.2205865302662007\n",
            "train loss:1.173375354035632\n",
            "train loss:1.0574764456281975\n",
            "=== epoch:281, train acc:0.6633333333333333, test acc:0.5608 ===\n",
            "train loss:1.237580302889534\n",
            "train loss:1.2415182866559848\n",
            "train loss:1.061993229141367\n",
            "=== epoch:282, train acc:0.6733333333333333, test acc:0.5616 ===\n",
            "train loss:1.139288112362883\n",
            "train loss:1.2446898261185186\n",
            "train loss:1.1585154597764313\n",
            "=== epoch:283, train acc:0.68, test acc:0.5654 ===\n",
            "train loss:1.2635288276043282\n",
            "train loss:1.1890322474027757\n",
            "train loss:1.1343115148833276\n",
            "=== epoch:284, train acc:0.6833333333333333, test acc:0.5688 ===\n",
            "train loss:1.2010682099399397\n",
            "train loss:1.0464361308111239\n",
            "train loss:1.1824457518743978\n",
            "=== epoch:285, train acc:0.6833333333333333, test acc:0.5684 ===\n",
            "train loss:1.2221897939711608\n",
            "train loss:1.0777684370958718\n",
            "train loss:1.2847217948903193\n",
            "=== epoch:286, train acc:0.68, test acc:0.5707 ===\n",
            "train loss:1.1940931810918394\n",
            "train loss:1.050651477425324\n",
            "train loss:0.9615453481437328\n",
            "=== epoch:287, train acc:0.69, test acc:0.5677 ===\n",
            "train loss:0.9876614040534545\n",
            "train loss:1.1440350864240865\n",
            "train loss:1.0502400623711061\n",
            "=== epoch:288, train acc:0.6833333333333333, test acc:0.5674 ===\n",
            "train loss:1.0778765658285037\n",
            "train loss:1.154002884623478\n",
            "train loss:1.0924760739996813\n",
            "=== epoch:289, train acc:0.69, test acc:0.5687 ===\n",
            "train loss:1.0331168591685966\n",
            "train loss:1.096712330075818\n",
            "train loss:0.9660947329859976\n",
            "=== epoch:290, train acc:0.6833333333333333, test acc:0.5657 ===\n",
            "train loss:1.1111082227953548\n",
            "train loss:1.1613551627621899\n",
            "train loss:1.1292662682434311\n",
            "=== epoch:291, train acc:0.69, test acc:0.5667 ===\n",
            "train loss:1.1272365982344874\n",
            "train loss:1.1755803342966105\n",
            "train loss:0.9704006873992553\n",
            "=== epoch:292, train acc:0.69, test acc:0.5674 ===\n",
            "train loss:1.0907219834169986\n",
            "train loss:1.161151134676936\n",
            "train loss:1.0054674318809\n",
            "=== epoch:293, train acc:0.7, test acc:0.5718 ===\n",
            "train loss:1.1529808408839142\n",
            "train loss:0.9032862032265371\n",
            "train loss:1.0010414416893596\n",
            "=== epoch:294, train acc:0.7033333333333334, test acc:0.5727 ===\n",
            "train loss:0.9502535840523396\n",
            "train loss:1.004862276970374\n",
            "train loss:0.9711930430012614\n",
            "=== epoch:295, train acc:0.6933333333333334, test acc:0.5725 ===\n",
            "train loss:0.9480266913229367\n",
            "train loss:1.046988721210338\n",
            "train loss:1.0967642816784553\n",
            "=== epoch:296, train acc:0.7066666666666667, test acc:0.5739 ===\n",
            "train loss:0.9239716038101693\n",
            "train loss:1.0206760498689287\n",
            "train loss:1.1798540036871998\n",
            "=== epoch:297, train acc:0.7066666666666667, test acc:0.5782 ===\n",
            "train loss:1.1006209331511347\n",
            "train loss:0.9956393394129887\n",
            "train loss:1.049032458756227\n",
            "=== epoch:298, train acc:0.7, test acc:0.5798 ===\n",
            "train loss:0.9356764044575331\n",
            "train loss:1.1404728762806582\n",
            "train loss:1.0226910385103882\n",
            "=== epoch:299, train acc:0.71, test acc:0.5823 ===\n",
            "train loss:1.1152108378759955\n",
            "train loss:1.0093020373918993\n",
            "train loss:1.0432415542350888\n",
            "=== epoch:300, train acc:0.7066666666666667, test acc:0.5808 ===\n",
            "train loss:1.0915189344201885\n",
            "train loss:0.9415904408609266\n",
            "train loss:0.9789972567949324\n",
            "=== epoch:301, train acc:0.7133333333333334, test acc:0.5803 ===\n",
            "train loss:0.9903744216756167\n",
            "train loss:0.9269480751418692\n",
            "=============== Final Test Accuracy ===============\n",
            "test acc:0.5796\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dropout을 적용시  훈련데이터와 시험데이터에 대한 정확도 차이가 감소\n"
      ],
      "metadata": {
        "id": "0nWIXrswTC91"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 적절한 하이퍼파라미터 찾기\n",
        "# 뉴런수, 배치크기, 매개변수갱신시 학습률과 가중치 감소률등\n",
        "# 하이퍼파라미터의 성능을 평가할때는 시험데이터를 사용해서 안됨(중요!)\n",
        "# 하이퍼파라미터 전용 확인데이터가 필요(validation data)\n",
        "# 훈련데이터 : 매개변수학습\n",
        "# 검증데이터 : 하이퍼파라미터성능평가\n",
        "# 시험데이터 : 신경망의 범용성능 평가\n"
      ],
      "metadata": {
        "id": "2PepakPIWw5N"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from common.util import *\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist()\n",
        "x_train, t_train = shuffle_dataset(x_train, t_train)\n",
        "validation_rate = 0.2\n",
        "validation_num = int(x_train.shape[0] * validation_rate)\n",
        "x_val = x_train[:validation_num]\n",
        "t_val = t_train[:validation_num]\n",
        "x_train = x_train[validation_num:]\n",
        "t_train = x_train[validation_num:]"
      ],
      "metadata": {
        "id": "HHQaiFigYALA"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼파라미터를 최적화할때의 핵심은 하이퍼파라미터의 최적값이 존재하는 범위를 조금씩 줄여가는것\n",
        "# 범위를 조금씩 줄이려면 우선 대략적인 범위를 설정하고 그 범위에서 무작위로 하이퍼파라미터값을 골라낸후, 그 값으로 정확도를 평가\n",
        "# 하이퍼파라미터의 범위는 대략적으로 지정하는것이 효과적\n",
        "\n"
      ],
      "metadata": {
        "id": "UE32d_NQY2P9"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼파라미터 최적화 구현하기\n",
        "# coding: utf-8\n",
        "import sys\n",
        "import os\n",
        "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "from common.multi_layer_net import MultiLayerNet\n",
        "from common.util import shuffle_dataset\n",
        "from common.trainer import Trainer\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
        "\n",
        "# 결과를 빠르게 얻기 위해 훈련 데이터를 줄임\n",
        "x_train = x_train[:500]\n",
        "t_train = t_train[:500]\n",
        "\n",
        "# 20%를 검증 데이터로 분할\n",
        "validation_rate = 0.20\n",
        "validation_num = int(x_train.shape[0] * validation_rate)\n",
        "x_train, t_train = shuffle_dataset(x_train, t_train)\n",
        "x_val = x_train[:validation_num]\n",
        "t_val = t_train[:validation_num]\n",
        "x_train = x_train[validation_num:]\n",
        "t_train = t_train[validation_num:]\n",
        "\n",
        "\n",
        "def __train(lr, weight_decay, epocs=50):\n",
        "    network = MultiLayerNet(input_size=784,\n",
        "                            hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
        "                            output_size=10, weight_decay_lambda=weight_decay)\n",
        "    trainer = Trainer(network, x_train, t_train, x_val, t_val,\n",
        "                      epochs=epocs, mini_batch_size=100,\n",
        "                      optimizer='sgd',\n",
        "                      optimizer_param={'lr': lr}, verbose=False)\n",
        "    trainer.train()\n",
        "\n",
        "    return trainer.test_acc_list, trainer.train_acc_list\n",
        "\n",
        "\n",
        "# 하이퍼파라미터 무작위 탐색======================================\n",
        "optimization_trial = 100\n",
        "results_val = {}\n",
        "results_train = {}\n",
        "for _ in range(optimization_trial):\n",
        "    # 탐색한 하이퍼파라미터의 범위 지정===============\n",
        "    weight_decay = 10 ** np.random.uniform(-8, -4)\n",
        "    lr = 10 ** np.random.uniform(-6, -2)\n",
        "    # ================================================\n",
        "\n",
        "    val_acc_list, train_acc_list = __train(lr, weight_decay)\n",
        "    print(\"val acc:\" + str(val_acc_list[-1]) + \" | lr:\" + str(lr) + \", weight decay:\" + str(weight_decay))\n",
        "    key = \"lr:\" + str(lr) + \", weight decay:\" + str(weight_decay)\n",
        "    results_val[key] = val_acc_list\n",
        "    results_train[key] = train_acc_list\n",
        "\n",
        "# 그래프 그리기========================================================\n",
        "print(\"=========== Hyper-Parameter Optimization Result ===========\")\n",
        "graph_draw_num = 20\n",
        "col_num = 5\n",
        "row_num = int(np.ceil(graph_draw_num / col_num))\n",
        "i = 0\n",
        "\n",
        "for key, val_acc_list in sorted(results_val.items(), key=lambda x: x[1][-1], reverse=True):\n",
        "    print(\"Best-\" + str(i+1) + \"(val acc:\" + str(val_acc_list[-1]) + \") | \" + key)\n",
        "\n",
        "    plt.subplot(row_num, col_num, i+1)\n",
        "    plt.title(\"Best-\" + str(i+1))\n",
        "    plt.ylim(0.0, 1.0)\n",
        "    if i % 5:\n",
        "        plt.yticks([])\n",
        "    plt.xticks([])\n",
        "    x = np.arange(len(val_acc_list))\n",
        "    plt.plot(x, val_acc_list)\n",
        "    plt.plot(x, results_train[key], \"--\")\n",
        "    i += 1\n",
        "\n",
        "    if i >= graph_draw_num:\n",
        "        break\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hcQv_PZmaUUo",
        "outputId": "4c9850db-4f51-4cc4-8697-65c7c77f4f3d"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val acc:0.1 | lr:2.5657805635443897e-06, weight decay:3.261037971980958e-07\n",
            "val acc:0.13 | lr:3.207455299003434e-05, weight decay:4.293734433887859e-06\n",
            "val acc:0.09 | lr:0.00046132441915134, weight decay:2.216468190785819e-06\n",
            "val acc:0.05 | lr:1.7222268565839515e-06, weight decay:4.1731573554603054e-05\n",
            "val acc:0.15 | lr:0.0009501321035748135, weight decay:4.18996014271824e-08\n",
            "val acc:0.06 | lr:1.3045386386218909e-06, weight decay:1.4729867638614423e-07\n",
            "val acc:0.14 | lr:3.224378361801174e-06, weight decay:2.7810987496104533e-08\n",
            "val acc:0.14 | lr:0.0001236163802445288, weight decay:2.9399533046767748e-08\n",
            "val acc:0.08 | lr:2.4470590861091277e-05, weight decay:1.0023165037135063e-07\n",
            "val acc:0.1 | lr:3.209639994518135e-06, weight decay:1.7930242419195104e-08\n",
            "val acc:0.14 | lr:6.018870241412246e-05, weight decay:6.182114230057707e-06\n",
            "val acc:0.12 | lr:1.0221681531759824e-05, weight decay:3.2512793838191316e-07\n",
            "val acc:0.08 | lr:0.00031769176580046947, weight decay:9.106657140353981e-08\n",
            "val acc:0.13 | lr:2.7666947195267797e-06, weight decay:5.560709801043304e-08\n",
            "val acc:0.08 | lr:2.2892559269314294e-05, weight decay:5.579773948810707e-07\n",
            "val acc:0.13 | lr:4.530069776287245e-06, weight decay:5.349405907972718e-05\n",
            "val acc:0.1 | lr:1.0496528208596427e-05, weight decay:9.390597832818843e-05\n",
            "val acc:0.17 | lr:0.0006961285274745385, weight decay:2.2673880552688193e-07\n",
            "val acc:0.47 | lr:0.0032460560963214536, weight decay:3.018348608690289e-08\n",
            "val acc:0.28 | lr:0.0017215927972548811, weight decay:8.069154793600567e-06\n",
            "val acc:0.14 | lr:2.2448958882537086e-06, weight decay:5.853191083521355e-05\n",
            "val acc:0.09 | lr:7.40796812978104e-05, weight decay:2.0962072924512733e-07\n",
            "val acc:0.1 | lr:5.9869950451708824e-05, weight decay:3.906187345495247e-06\n",
            "val acc:0.26 | lr:0.001788214738471175, weight decay:4.002095833961625e-08\n",
            "val acc:0.19 | lr:0.00019787506309133164, weight decay:1.8598863128609663e-06\n",
            "val acc:0.06 | lr:2.7899473903923083e-06, weight decay:3.8063240089215195e-06\n",
            "val acc:0.09 | lr:0.00013086714178917017, weight decay:8.506704306958454e-07\n",
            "val acc:0.06 | lr:0.00013081118775191055, weight decay:9.761708228703113e-06\n",
            "val acc:0.12 | lr:1.3664871788033072e-06, weight decay:9.865167917944874e-05\n",
            "val acc:0.13 | lr:3.0445118328049773e-06, weight decay:1.0587115030854996e-05\n",
            "val acc:0.08 | lr:0.00016315573325177866, weight decay:1.1316658715288853e-06\n",
            "val acc:0.03 | lr:0.00017157937071886302, weight decay:2.1046886391707495e-06\n",
            "val acc:0.16 | lr:0.0005041373649417091, weight decay:1.5255415056795747e-07\n",
            "val acc:0.07 | lr:0.00010256069911927058, weight decay:5.537725334679742e-06\n",
            "val acc:0.11 | lr:3.4440459821256676e-06, weight decay:1.3041791095432773e-06\n",
            "val acc:0.12 | lr:6.126714127815067e-06, weight decay:1.989418272932692e-08\n",
            "val acc:0.1 | lr:0.0008382115072241599, weight decay:7.571549586267267e-07\n",
            "val acc:0.26 | lr:0.0012015920735004581, weight decay:1.217972697144714e-05\n",
            "val acc:0.06 | lr:2.7591896095295403e-05, weight decay:2.0538576021289857e-07\n",
            "val acc:0.49 | lr:0.0036808458362153377, weight decay:9.541034523187065e-05\n",
            "val acc:0.09 | lr:1.4555053301113334e-05, weight decay:2.1433778468676924e-07\n",
            "val acc:0.05 | lr:5.986067506130777e-05, weight decay:5.662053357922862e-05\n",
            "val acc:0.15 | lr:1.1998769581773115e-06, weight decay:1.080154655679885e-06\n",
            "val acc:0.07 | lr:1.137338099527428e-06, weight decay:1.142198748933707e-06\n",
            "val acc:0.2 | lr:0.0005889834210089427, weight decay:1.5642941421363846e-05\n",
            "val acc:0.11 | lr:3.476653161340057e-06, weight decay:2.026553132256526e-08\n",
            "val acc:0.1 | lr:0.00019367770792623186, weight decay:3.835654776076895e-06\n",
            "val acc:0.1 | lr:3.923735433942153e-06, weight decay:1.495138415766118e-07\n",
            "val acc:0.11 | lr:3.3783751218352835e-05, weight decay:2.6674126874195187e-05\n",
            "val acc:0.19 | lr:0.0005644712219624874, weight decay:2.458009017008967e-08\n",
            "val acc:0.25 | lr:0.0012542481396592752, weight decay:2.614580720145191e-07\n",
            "val acc:0.08 | lr:3.976429009873414e-06, weight decay:7.955386823836142e-07\n",
            "val acc:0.03 | lr:4.5034866885365405e-06, weight decay:2.153317014114881e-05\n",
            "val acc:0.09 | lr:7.810728945045319e-05, weight decay:2.745548235799655e-06\n",
            "val acc:0.52 | lr:0.00264551451281185, weight decay:5.915184323470123e-08\n",
            "val acc:0.13 | lr:0.00045132518660532503, weight decay:2.3841807391583665e-06\n",
            "val acc:0.13 | lr:0.0005337176915335839, weight decay:1.659832930452937e-08\n",
            "val acc:0.16 | lr:0.0005418241063480967, weight decay:3.305151566692651e-05\n",
            "val acc:0.08 | lr:1.828628733475797e-06, weight decay:2.1551152105614984e-08\n",
            "val acc:0.14 | lr:0.0005556067911668603, weight decay:3.293139817009337e-05\n",
            "val acc:0.17 | lr:9.943925214667232e-05, weight decay:4.098923857006683e-08\n",
            "val acc:0.14 | lr:4.593179936810833e-06, weight decay:5.58067039587585e-07\n",
            "val acc:0.51 | lr:0.005595088846290182, weight decay:3.5208092910459375e-05\n",
            "val acc:0.16 | lr:0.0006374681869507384, weight decay:8.049584679723752e-07\n",
            "val acc:0.14 | lr:9.842226215115154e-05, weight decay:5.354530581736649e-08\n",
            "val acc:0.47 | lr:0.002270070930052435, weight decay:4.94712354398513e-05\n",
            "val acc:0.08 | lr:1.3229284949233145e-06, weight decay:8.138219661968661e-05\n",
            "val acc:0.1 | lr:0.000168761272862762, weight decay:2.3375196977988705e-06\n",
            "val acc:0.14 | lr:9.115467726833825e-06, weight decay:7.022844536551003e-08\n",
            "val acc:0.6 | lr:0.004197622494000602, weight decay:3.2306084523560166e-07\n",
            "val acc:0.23 | lr:0.0005010830553858319, weight decay:5.4398278138718315e-06\n",
            "val acc:0.12 | lr:0.00015208597539847872, weight decay:2.907571557399053e-07\n",
            "val acc:0.11 | lr:2.814134957241112e-06, weight decay:5.371291397379494e-07\n",
            "val acc:0.13 | lr:3.0411976517211815e-06, weight decay:4.116380206313343e-05\n",
            "val acc:0.22 | lr:0.0008482840274781988, weight decay:1.1662381263946365e-07\n",
            "val acc:0.76 | lr:0.008772631492473799, weight decay:3.9805233942577885e-07\n",
            "val acc:0.82 | lr:0.009776247334846193, weight decay:6.834445907098495e-08\n",
            "val acc:0.08 | lr:0.00010346764144182899, weight decay:1.0725459466278187e-07\n",
            "val acc:0.36 | lr:0.0025808712036481003, weight decay:8.728222199859444e-08\n",
            "val acc:0.08 | lr:3.3426653436049953e-05, weight decay:5.136986729309422e-08\n",
            "val acc:0.28 | lr:0.0013465240385938776, weight decay:2.7802208465494403e-06\n",
            "val acc:0.12 | lr:1.0236880644830342e-06, weight decay:6.333770477034738e-08\n",
            "val acc:0.28 | lr:0.0011757088359646364, weight decay:9.598369268314119e-08\n",
            "val acc:0.2 | lr:3.2228136694947174e-05, weight decay:1.5877861412784163e-08\n",
            "val acc:0.1 | lr:6.194381699012797e-06, weight decay:3.8569829682881815e-05\n",
            "val acc:0.25 | lr:0.0005192931065090133, weight decay:2.4751152411679248e-08\n",
            "val acc:0.06 | lr:0.00043094683599881865, weight decay:1.0339971370989164e-06\n",
            "val acc:0.07 | lr:1.1408016745356082e-06, weight decay:1.8678016959261523e-05\n",
            "val acc:0.15 | lr:0.0004998829607100385, weight decay:9.816712512871483e-05\n",
            "val acc:0.15 | lr:0.000912864721918974, weight decay:4.8575529269592194e-08\n",
            "val acc:0.06 | lr:2.2880477355848657e-05, weight decay:1.6258270360206705e-07\n",
            "val acc:0.08 | lr:5.434429802385769e-06, weight decay:7.658392459358992e-07\n",
            "val acc:0.09 | lr:3.2037206609147236e-05, weight decay:5.221729165383733e-07\n",
            "val acc:0.08 | lr:4.8864887657352216e-06, weight decay:1.1109312654374835e-06\n",
            "val acc:0.05 | lr:4.410077155615776e-05, weight decay:9.38971095411455e-06\n",
            "val acc:0.12 | lr:0.00017347314820279105, weight decay:1.6398214176090625e-05\n",
            "val acc:0.11 | lr:1.272226975409108e-06, weight decay:2.041545453704899e-05\n",
            "val acc:0.05 | lr:7.4319082034147185e-06, weight decay:2.8347916513153295e-06\n",
            "val acc:0.74 | lr:0.0057205147844634, weight decay:3.697137714678502e-05\n",
            "val acc:0.14 | lr:1.3284560641813296e-06, weight decay:3.090819829917042e-06\n",
            "=========== Hyper-Parameter Optimization Result ===========\n",
            "Best-1(val acc:0.82) | lr:0.009776247334846193, weight decay:6.834445907098495e-08\n",
            "Best-2(val acc:0.76) | lr:0.008772631492473799, weight decay:3.9805233942577885e-07\n",
            "Best-3(val acc:0.74) | lr:0.0057205147844634, weight decay:3.697137714678502e-05\n",
            "Best-4(val acc:0.6) | lr:0.004197622494000602, weight decay:3.2306084523560166e-07\n",
            "Best-5(val acc:0.52) | lr:0.00264551451281185, weight decay:5.915184323470123e-08\n",
            "Best-6(val acc:0.51) | lr:0.005595088846290182, weight decay:3.5208092910459375e-05\n",
            "Best-7(val acc:0.49) | lr:0.0036808458362153377, weight decay:9.541034523187065e-05\n",
            "Best-8(val acc:0.47) | lr:0.0032460560963214536, weight decay:3.018348608690289e-08\n",
            "Best-9(val acc:0.47) | lr:0.002270070930052435, weight decay:4.94712354398513e-05\n",
            "Best-10(val acc:0.36) | lr:0.0025808712036481003, weight decay:8.728222199859444e-08\n",
            "Best-11(val acc:0.28) | lr:0.0017215927972548811, weight decay:8.069154793600567e-06\n",
            "Best-12(val acc:0.28) | lr:0.0013465240385938776, weight decay:2.7802208465494403e-06\n",
            "Best-13(val acc:0.28) | lr:0.0011757088359646364, weight decay:9.598369268314119e-08\n",
            "Best-14(val acc:0.26) | lr:0.001788214738471175, weight decay:4.002095833961625e-08\n",
            "Best-15(val acc:0.26) | lr:0.0012015920735004581, weight decay:1.217972697144714e-05\n",
            "Best-16(val acc:0.25) | lr:0.0012542481396592752, weight decay:2.614580720145191e-07\n",
            "Best-17(val acc:0.25) | lr:0.0005192931065090133, weight decay:2.4751152411679248e-08\n",
            "Best-18(val acc:0.23) | lr:0.0005010830553858319, weight decay:5.4398278138718315e-06\n",
            "Best-19(val acc:0.22) | lr:0.0008482840274781988, weight decay:1.1662381263946365e-07\n",
            "Best-20(val acc:0.2) | lr:0.0005889834210089427, weight decay:1.5642941421363846e-05\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 20 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IVoUBe-1X2BF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}