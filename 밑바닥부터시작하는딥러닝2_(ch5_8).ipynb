{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "밑바닥부터시작하는딥러닝2 -(ch5-8).ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOFEZz/s19UfgJvDy1x7Iwu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaekyoungkim/floor_DL/blob/main/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94%EB%94%A5%EB%9F%AC%EB%8B%9D2_(ch5_8).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/WegraLee/deep-learning-from-scratch-2.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SKbcaMgyJyw",
        "outputId": "46de50b3-b986-4efa-95c2-d468eeb48c19"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'deep-learning-from-scratch-2'...\n",
            "remote: Enumerating objects: 606, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 606 (delta 1), reused 5 (delta 0), pack-reused 598\u001b[K\n",
            "Receiving objects: 100% (606/606), 29.82 MiB | 51.66 MiB/s, done.\n",
            "Resolving deltas: 100% (361/361), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ch5. 순환신경망(RNN) "
      ],
      "metadata": {
        "id": "GliomcGYkE2_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# feedforward 신경망 : 흐름이 단방향인 신경망, 입력신호가 다음층으로 전달되고 그 신호를 받은 층은 그 다음층으로 전달\n",
        "# feedforward는 구성이 단순하여 구조를 이해하기 쉽고, 많은 문제에 응용할 수 있음\n",
        "# 커다란 단점이 있으니, 바로 시계열 데이터를 잘 다루지 못함\n",
        "# 시계열 데이터의 성질을 충분히 학습할 수 없음 -> RNN등장"
      ],
      "metadata": {
        "id": "qDrEpO0kkHVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 확률과 언어모델 \n",
        "# 언어모델(Language model):단어 나열에 확률을 부여함, 특정한 단어의 시퀀스에 대해서 스 시퀀스가 일어날 가능성을 평가\n",
        "# 이상적으로는 맥락의 단어 순서도 모델이 고려해야함 \n",
        "# CBOW에서는 맥락의 단어 벡터를 은닉층에서 연결하는 방식을 생각해볼 수도있음\n",
        "# 그러나 연결하는 방식을 취하면 맥락의 크기에 비례해 가중치 매개변수도 늘어나게됨\n"
      ],
      "metadata": {
        "id": "LM-gGaxUkny-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 순환하는 신경망 RNN\n",
        "# 순환하기 위해서는 닫힌 경로가 필요함 , 닫힌경로 혹은 순환하는 경로가 존재해야 데이터가 같은 장소를 반복해 왕래\n",
        "# 데이터가 순환하면서 정보가 끊임없이 갱신됨\n",
        "# RNN의 특징은 순환하는 경로가 있다는 것, 이 순환경로를 따라 데이터는 끊임없이 순환\n",
        "# 데이터가 순환되기 때문에 과거의 정보를 기억하는 동시에 최신 데이터로 갱신가능\n",
        "# h_{t} = tanh( h_{t-1} W_{h} + x_{t} * W_{x} +b )\n",
        "# h_{t}(현재의 출력)은 한 시각 이전 출력(h_{t-1})에 기초해 계산됨\n",
        "# RNN은 h라는 상태를 가지고 있으며, 즉 상태를 가지는 계층, 메모리가 있는 계창이라고 함\n",
        "# h_{t}를 은닉상태 혹은 은닉 상태 벡터라고 부름\n"
      ],
      "metadata": {
        "id": "GigwUi6VzIGm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BPTT (backpropagation through time) \n",
        "# 시계열 데이터의 시간크기가 커지는 것에 비례하여 BPTT가 소비하는 컴퓨팅자원도 증가함\n",
        "# 시간 크기가 커지면 역전파 시의 기울기가 불안정해지는것도 문제임\n",
        "# BPTT를 이용해 기울기를 구하려면 매 시각RNN 계층의 중간 데이터를 메모리에 유지해두지 않으면 안됨"
      ],
      "metadata": {
        "id": "PZe8FgYJ-VMM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Truncated BPTT\n",
        "# 너무 길어진 신경망을 적당한 지점에서 잘라내어 작은 신경망 여러개로 만드는 아이디어\n",
        "# 제대로 구현하려면 역전파의 연결만 끊음/순전파의 연결은 반드시 그대로 유지\n",
        "# 계층이 길어짐에 따라 신경망을 하나 통과할때마다 기울기 값이 조금씩 작아져서, 이전시각 T까지 연전파 되기전에 0이될가능성있음"
      ],
      "metadata": {
        "id": "voyxJBcA-rXD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN:\n",
        "  def __init__(self, Wx, Wh, b): # 가중치 2개와 편향1개를 인수로 받음\n",
        "    self.params = [Wx, Wh, b] \n",
        "    self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)] \n",
        "    self.cache = None # 역전파 계산시 사용하는 중간 데이터를 담는역할\n",
        "  \n",
        "  def  forward(self, x, h, h_prev): \n",
        "    Wx, Wh, b = self.params \n",
        "    t= np.matmul(h_prev, Wh) + np.matmul(x, Wx) + b\n",
        "    h_next = np.tanh(t) \n",
        "    self.cache = (x,h_prev,h_next) \n",
        "    return h_next\n",
        "  \n",
        "  def backward(self, dh_next):\n",
        "    Wx, Wh, b = self.params \n",
        "    x, h_prev, h_next = self.cache \n",
        "\n",
        "    dt = dh_next * (1- h_next**2)  # tanh의 역함수\n",
        "    db = np.sum(dt, axis=0) \n",
        "    dWh = np.matmul(h_prev.T, dt) \n",
        "    dh_prev = np.matmul(dt, Wh.T)\n",
        "    dWx = np.matmul(x.T, dt) \n",
        "    dx = np.matmul(dt, Wx.T) \n",
        "\n",
        "    self.grads[0][...] = dWx\n",
        "    self.grads[1][...] = dWh\n",
        "    self.grads[2][...] = db\n",
        "\n",
        "    return dx, dh_prev"
      ],
      "metadata": {
        "id": "Rmg8jUmuTDR4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TimeRNN\n",
        "# 시계열 데이터를 한꺼번에 처리하는 계층\n",
        "# T개분의 시계열 데이터를 한꺼번에 처리하는 계층\n",
        "# RNN계층의 은닉상태를 TimeRNN계층에서 관리하기로함\n",
        "# RNN계층 사이에서 은닉상태를 인계하는 작업을 생각하지 않아도 되는 장점\n",
        "class TimeRNN:\n",
        "  def __init__(self, Wx, Wh, b, stateful=False):\n",
        "    self.params = [Wx, Wh, b] \n",
        "    self.grads = [np.zeros_like(Wx) , np.zeros_like(Wh), np.zeros_like(b)] \n",
        "    self.layers = None \n",
        "\n",
        "    self.h, self.dh = None, None\n",
        "    self.stateful = stateful \n",
        "  \n",
        "  def set_state(self, h):\n",
        "    self.h = h\n",
        "\n",
        "  def reset_state(self): \n",
        "    self.h = None "
      ],
      "metadata": {
        "id": "KhTChkLbXk8B"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RNNLM학습과 평가\n",
        "import sys \n",
        "sys.path.append('/content/deep-learning-from-scratch-2/')\n",
        "import numpy as np\n",
        "from common.time_layers import *"
      ],
      "metadata": {
        "id": "2I3PPWj2CGcV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleRnnlm: \n",
        "  def __init__(self, voca_size, wordvec_size, hidden_size): \n",
        "    V,D,H = vocab_size, wordvec_size, hidden_size \n",
        "    rn = np.random.randn \n",
        "\n",
        "    # 가중치 초기화 : xavier초깃값을 사용 / 이전 계층의 노드가 n개라면 표준편차가 1/roo(n) 인 분포로 초기화\n",
        "    embed_W = (rn(V,D) / 100).astype('f') \n",
        "    rnn_Wx = (rn(D,H) / np.sqrt(D)).astype('f')\n",
        "    rnn_Wh = (rnn(H,H) /np.sqrt(H)).astype('f') \n",
        "    rnn_b = np.zeros(H).astype('f') \n",
        "    affine_W = (rn(H,V) / np.sqrt(H)).astype('f') \n",
        "    affine_b = np.zeros(V).astype('f') \n",
        "\n",
        "    # 계층 생성\n",
        "    self.layers = [TimeEmbedding(embed_W), TimeRNN(rnn_Wx, rnn_Wh, rnn_b, stateful=True),\n",
        "                   TimeAffine(affine_W,  affine_b)]\n",
        "    # truncated BPTT로 학습한다고 가정하여 Stateful = True로 설정 : 이전은닉상태를 계승할 수 있음\n",
        "    self.loss_layer = TImesoftmaxWithLoss()\n",
        "    self.rnn_layer = self.layers[1]\n",
        "    \n",
        "    # 모든 가중치와 기울기를 리스트에 모으기\n",
        "    self.params, self.grads = [], [] \n",
        "    for layer in self.layers :\n",
        "      self.params += layer.params \n",
        "      self.grads += layer.grads \n",
        "  \n",
        "  def forward(self, xs, ts):\n",
        "    for layer in self.layers:\n",
        "      xs = layer.forward(xs) \n",
        "    loss = self.loss_layer.forward(xs, ts)\n",
        "\n",
        "  def backward(self, dout=1):\n",
        "    dout = self.loss_layer.backward(dout) \n",
        "    for layer in reversed(self.layers):\n",
        "      dout = layer.backward(dout) \n",
        "    return dout \n",
        "\n",
        "  def rest_state(self):\n",
        "    self.rnn_layer.reset_state() \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xLyk0Y2MySBP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 언어모델의 평가방법 : perplexity (혼란도) \n",
        "# 확률의 역수\n",
        "# you say good bye -> you 다음 단어 say나올확률이 0.8 이라면 1/0.8=1.25가 perplexity\n",
        "# 혼란도는 작을수록 좋음\n",
        "# 혼란도는 분기수라고 해석할수 있음(number of branches)\n",
        "# 다음에 취할 수 있는 선택사항의 수\n"
      ],
      "metadata": {
        "id": "z2jklRl94Hgq"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RNNLM학습코드\n",
        "import matplotlib.pyplot as plt\n",
        "from common.optimizer import SGD\n",
        "from dataset import ptb \n",
        "from ch05.simple_rnnlm import SimpleRnnlm\n",
        "#하이퍼 파라미터설정\n",
        "batch_size =10\n",
        "wordvec_size =100\n",
        "hidden_size =100 \n",
        "time_size =5 # truncated bptt가 한번에 펼치는 시간 크기\n",
        "lr = 0.1\n",
        "max_epoch = 100\n",
        "\n",
        "# 학습데이터 읽기\n",
        "corpus , word_to_id, id_to_word  = ptb.load_data(\"train\")\n",
        "corpus_size =1000\n",
        "corpus = corpus[:corpus_size] \n",
        "vocab_size = int(max(corpus) +1)"
      ],
      "metadata": {
        "id": "6qxJF6DvCNIw"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHIE6-KwC7q5",
        "outputId": "88a6bf6e-02b4-4417-9221-63b1155fd917"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
              "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
              "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
              "        27,  24,  39,  26,  40,  41,  42,  26,  43,  32,  44,  45,  46,\n",
              "        24,  47,  26,  27,  28,  29,  48,  49,  41,  42,  50,  51,  52,\n",
              "        53,  54,  55,  35,  36,  37,  42,  56,  57,  58,  59,  24,  35,\n",
              "        60,  42,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  35,\n",
              "        71,  72,  42,  73,  74,  75,  35,  46,  42,  76,  77,  64,  78,\n",
              "        79,  80,  27,  28,  81,  82,  83,  24,  32,  61,  84,  26,  40,\n",
              "        85,  26,  62,  78,  86,  32,  26,  87,  88,  89,  90,  64,  78,\n",
              "        91,  92,  93,  94,  95,  96,  97,  82,  98,  24,  26,  99,  32,\n",
              "       100,  42, 101, 102,  26, 103,  93, 104,  66, 105, 106, 107,  26,\n",
              "       108, 109,  26,  67,  68, 108,  27,  24, 110, 111, 112, 113,  83,\n",
              "        79,  80,  35, 114,  81,  32, 115, 116, 117, 108, 118, 119, 101,\n",
              "       120, 121,  42, 122,  35, 123, 124,  64, 125, 101, 126,  64,  32,\n",
              "       127,  24,  35,  26,  26,  98,  56,  40, 128,  29, 129,  24, 130,\n",
              "       131, 132, 133,  28,  81, 134, 135, 136,  42,  61, 137, 138, 139,\n",
              "       140,  24, 141,  40, 142,  61, 108, 143, 144, 145,  24, 146,  26,\n",
              "       147,  32,  82, 148, 149,  32,  76, 113, 150,  42, 138, 151, 152,\n",
              "       153,  42,  32,  66, 105,  24, 130, 154, 142, 155, 156, 152, 157,\n",
              "       158, 159, 160, 161,  98, 162, 163,  26,  42, 164, 119,  26,  73,\n",
              "       165,  24, 166,  26, 167,  35, 168,  42,  82, 169,  32, 170,  73,\n",
              "       165,  48,  32, 171, 172,  42, 173, 174,  48, 164, 174,  24,  32,\n",
              "        26, 175,  98,  61,  54,  63, 108, 176, 177, 178, 108, 179, 180,\n",
              "       181,  32,  68, 108,  32, 182, 183,  48, 184,  87,  35, 185, 186,\n",
              "        42,  26, 108,  27,  24, 169,  27,  64,  27,  27, 187,  66, 105,\n",
              "        87,  32,  68, 113, 188,  32, 189,  98,  24,  75,  27, 190, 148,\n",
              "       191, 192,  87,  32, 193,  27, 154, 194,  79,  80, 195, 196,  32,\n",
              "       197, 198,  24, 199,  42,  32, 200, 201,  76, 154,  26, 202, 203,\n",
              "       195,  87, 204,  26,  73,  24,  32, 205,  42,  27,  74, 169, 206,\n",
              "        26, 207,  73,  48,  26,  54, 208, 209,  80, 197,  32,  82,  98,\n",
              "        24,  32,  26, 210,  40,  35, 211, 212,  75, 213,  42, 214, 148,\n",
              "       215,  26, 202,  98, 166,  26,  24,  32,  72,  42, 207,  73,  74,\n",
              "        75,  32,  76, 160,  32, 216,  26, 217, 180, 218, 219,  64, 220,\n",
              "        32, 221, 181, 138,  61,  76, 149, 108, 222, 223, 224, 225,  98,\n",
              "        24,  32, 226, 227,  40, 228, 229,  26, 230,  26, 231,  54, 232,\n",
              "       233,  87,  26,  64,  65,  32,  67,  68,  24,  32, 212, 234,  30,\n",
              "       235, 213, 148, 236,  93,  32, 237, 238, 239,  32, 240,  42,  61,\n",
              "       203,  26,  79,  26,  80,  32, 241, 242,  42,  61,  26, 243, 108,\n",
              "       244, 172,  48, 245, 246, 166,  26,  98,  24,  32, 237,  40, 247,\n",
              "        42,  32, 248, 223, 249,  93, 250, 251, 154,  35, 209, 252,  42,\n",
              "       253, 181,  32, 254,  26, 255, 256,  34,  26,  93, 159, 257,  34,\n",
              "        26, 258,  64,  26, 259,  26,  35, 260,  42,  26, 160,  32, 174,\n",
              "        42, 261, 262,  42, 122,  24,  79, 241,  26, 255, 159,  26,  48,\n",
              "       159,  79, 263, 264, 229,  32, 265, 166,  26, 266,  24, 108, 267,\n",
              "        32, 268, 269, 270, 271,  35, 272, 273, 152, 274, 275, 276,  42,\n",
              "        61,  24, 229,  27, 277, 275, 278, 276,  42,  26,  61,  30, 220,\n",
              "       279,  24, 133,  27,  76, 160,  35, 218,  93, 280, 180, 181,  32,\n",
              "        66,  68, 113,  77,  64,  61, 108,  32, 183,  24, 281,  42,  32,\n",
              "       218, 113, 282, 283, 284,  32,  26,  54,  63,  24,  76, 285, 286,\n",
              "        26,  26,  42,  32, 287, 288, 289,  35, 290,  26, 291, 108, 292,\n",
              "        48,  26, 255,  48,  26, 293,  32, 294, 255, 108,  35, 295,  63,\n",
              "        64,  65,  68,  24,  76, 296, 297,  42, 298, 299,  93, 300, 301,\n",
              "       302,  42,  32, 218,  88, 303,  26, 304,  26,  32, 305,  24, 141,\n",
              "       119, 142, 306,  93, 307,  42, 213,  76,  48, 308, 309,  26, 202,\n",
              "        98,  26, 310, 311, 312,  42, 313, 314, 181,  26, 230,  26,  24,\n",
              "       315, 316, 154,  64, 317,  93, 318, 319, 320, 321,  27,  28,  81,\n",
              "        24,  78,  69, 142, 322, 152, 143, 323, 324, 118,  24, 325, 152,\n",
              "       326, 327, 328, 329,  64, 330, 331, 332,  93, 333, 308, 334, 335,\n",
              "       336, 108, 337, 338,  24,  32, 339, 340, 341, 342,  42,  32,  27,\n",
              "       343, 328, 344, 229,  26, 119, 345, 346, 347, 348,  35, 349,  42,\n",
              "        35,  72, 350,  64,  27,  27, 169,  27,  27, 181,  32, 351, 352,\n",
              "       353,  24, 341, 325, 354, 355,  42, 356,  48,  93,  32, 357, 342,\n",
              "       358, 181,  35, 114,  24, 339, 359,  42,  32, 328, 360, 361,  26,\n",
              "       229,  35, 362,  64,  27, 363,  32, 364, 365, 182, 366, 258,  64,\n",
              "       367, 119,  24, 368, 369, 159, 370,  64, 371, 372, 337, 338, 373,\n",
              "       374, 375, 333, 308,  64, 376, 377, 209, 338, 181,  35, 368, 378,\n",
              "        24, 379, 369, 159, 380,  35, 381,  42, 382, 338, 373, 333, 308,\n",
              "       383, 384, 209, 338, 385,  24,  32, 339, 359, 181, 328, 386, 387,\n",
              "        64, 388, 380, 229, 307,  64, 220,  35, 389, 390, 373, 213, 308,\n",
              "       391,  32, 392, 192, 393,  35,  71, 350, 181,  32, 114,  27, 363,\n",
              "        24, 394,  98,  26,  26,  26, 395,  42, 345, 346, 347, 325, 396,\n",
              "        26,  95, 397, 134, 374,  26, 398, 373,  42, 399, 400, 108, 401,\n",
              "       337, 338,  24,  32, 342, 152, 402, 403, 404, 188, 160, 405, 119,\n",
              "       406, 181, 407, 408,  64,  27,  27, 169,  27,  27,  24, 409, 399,\n",
              "       336, 108, 325, 410, 411,  64, 412, 413, 289, 345, 328,  24, 414,\n",
              "        42,  32,  27, 343, 328, 415, 229, 416,  27, 187, 417,  32])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IU6gK2E4DOaM",
        "outputId": "fd050cbd-765c-4027-d534-6984fd45ae1c"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size # 76"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snJoDB6jC8IU",
        "outputId": "a139165e-aaf5-4eb9-c0c5-8ce383c5c2d9"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "418"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xs = corpus[:-1];xs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEFD1x1JDKKN",
        "outputId": "82ae2ca2-47a5-444a-fea7-5e874037dcab"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
              "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
              "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
              "        27,  24,  39,  26,  40,  41,  42,  26,  43,  32,  44,  45,  46,\n",
              "        24,  47,  26,  27,  28,  29,  48,  49,  41,  42,  50,  51,  52,\n",
              "        53,  54,  55,  35,  36,  37,  42,  56,  57,  58,  59,  24,  35,\n",
              "        60,  42,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  35,\n",
              "        71,  72,  42,  73,  74,  75,  35,  46,  42,  76,  77,  64,  78,\n",
              "        79,  80,  27,  28,  81,  82,  83,  24,  32,  61,  84,  26,  40,\n",
              "        85,  26,  62,  78,  86,  32,  26,  87,  88,  89,  90,  64,  78,\n",
              "        91,  92,  93,  94,  95,  96,  97,  82,  98,  24,  26,  99,  32,\n",
              "       100,  42, 101, 102,  26, 103,  93, 104,  66, 105, 106, 107,  26,\n",
              "       108, 109,  26,  67,  68, 108,  27,  24, 110, 111, 112, 113,  83,\n",
              "        79,  80,  35, 114,  81,  32, 115, 116, 117, 108, 118, 119, 101,\n",
              "       120, 121,  42, 122,  35, 123, 124,  64, 125, 101, 126,  64,  32,\n",
              "       127,  24,  35,  26,  26,  98,  56,  40, 128,  29, 129,  24, 130,\n",
              "       131, 132, 133,  28,  81, 134, 135, 136,  42,  61, 137, 138, 139,\n",
              "       140,  24, 141,  40, 142,  61, 108, 143, 144, 145,  24, 146,  26,\n",
              "       147,  32,  82, 148, 149,  32,  76, 113, 150,  42, 138, 151, 152,\n",
              "       153,  42,  32,  66, 105,  24, 130, 154, 142, 155, 156, 152, 157,\n",
              "       158, 159, 160, 161,  98, 162, 163,  26,  42, 164, 119,  26,  73,\n",
              "       165,  24, 166,  26, 167,  35, 168,  42,  82, 169,  32, 170,  73,\n",
              "       165,  48,  32, 171, 172,  42, 173, 174,  48, 164, 174,  24,  32,\n",
              "        26, 175,  98,  61,  54,  63, 108, 176, 177, 178, 108, 179, 180,\n",
              "       181,  32,  68, 108,  32, 182, 183,  48, 184,  87,  35, 185, 186,\n",
              "        42,  26, 108,  27,  24, 169,  27,  64,  27,  27, 187,  66, 105,\n",
              "        87,  32,  68, 113, 188,  32, 189,  98,  24,  75,  27, 190, 148,\n",
              "       191, 192,  87,  32, 193,  27, 154, 194,  79,  80, 195, 196,  32,\n",
              "       197, 198,  24, 199,  42,  32, 200, 201,  76, 154,  26, 202, 203,\n",
              "       195,  87, 204,  26,  73,  24,  32, 205,  42,  27,  74, 169, 206,\n",
              "        26, 207,  73,  48,  26,  54, 208, 209,  80, 197,  32,  82,  98,\n",
              "        24,  32,  26, 210,  40,  35, 211, 212,  75, 213,  42, 214, 148,\n",
              "       215,  26, 202,  98, 166,  26,  24,  32,  72,  42, 207,  73,  74,\n",
              "        75,  32,  76, 160,  32, 216,  26, 217, 180, 218, 219,  64, 220,\n",
              "        32, 221, 181, 138,  61,  76, 149, 108, 222, 223, 224, 225,  98,\n",
              "        24,  32, 226, 227,  40, 228, 229,  26, 230,  26, 231,  54, 232,\n",
              "       233,  87,  26,  64,  65,  32,  67,  68,  24,  32, 212, 234,  30,\n",
              "       235, 213, 148, 236,  93,  32, 237, 238, 239,  32, 240,  42,  61,\n",
              "       203,  26,  79,  26,  80,  32, 241, 242,  42,  61,  26, 243, 108,\n",
              "       244, 172,  48, 245, 246, 166,  26,  98,  24,  32, 237,  40, 247,\n",
              "        42,  32, 248, 223, 249,  93, 250, 251, 154,  35, 209, 252,  42,\n",
              "       253, 181,  32, 254,  26, 255, 256,  34,  26,  93, 159, 257,  34,\n",
              "        26, 258,  64,  26, 259,  26,  35, 260,  42,  26, 160,  32, 174,\n",
              "        42, 261, 262,  42, 122,  24,  79, 241,  26, 255, 159,  26,  48,\n",
              "       159,  79, 263, 264, 229,  32, 265, 166,  26, 266,  24, 108, 267,\n",
              "        32, 268, 269, 270, 271,  35, 272, 273, 152, 274, 275, 276,  42,\n",
              "        61,  24, 229,  27, 277, 275, 278, 276,  42,  26,  61,  30, 220,\n",
              "       279,  24, 133,  27,  76, 160,  35, 218,  93, 280, 180, 181,  32,\n",
              "        66,  68, 113,  77,  64,  61, 108,  32, 183,  24, 281,  42,  32,\n",
              "       218, 113, 282, 283, 284,  32,  26,  54,  63,  24,  76, 285, 286,\n",
              "        26,  26,  42,  32, 287, 288, 289,  35, 290,  26, 291, 108, 292,\n",
              "        48,  26, 255,  48,  26, 293,  32, 294, 255, 108,  35, 295,  63,\n",
              "        64,  65,  68,  24,  76, 296, 297,  42, 298, 299,  93, 300, 301,\n",
              "       302,  42,  32, 218,  88, 303,  26, 304,  26,  32, 305,  24, 141,\n",
              "       119, 142, 306,  93, 307,  42, 213,  76,  48, 308, 309,  26, 202,\n",
              "        98,  26, 310, 311, 312,  42, 313, 314, 181,  26, 230,  26,  24,\n",
              "       315, 316, 154,  64, 317,  93, 318, 319, 320, 321,  27,  28,  81,\n",
              "        24,  78,  69, 142, 322, 152, 143, 323, 324, 118,  24, 325, 152,\n",
              "       326, 327, 328, 329,  64, 330, 331, 332,  93, 333, 308, 334, 335,\n",
              "       336, 108, 337, 338,  24,  32, 339, 340, 341, 342,  42,  32,  27,\n",
              "       343, 328, 344, 229,  26, 119, 345, 346, 347, 348,  35, 349,  42,\n",
              "        35,  72, 350,  64,  27,  27, 169,  27,  27, 181,  32, 351, 352,\n",
              "       353,  24, 341, 325, 354, 355,  42, 356,  48,  93,  32, 357, 342,\n",
              "       358, 181,  35, 114,  24, 339, 359,  42,  32, 328, 360, 361,  26,\n",
              "       229,  35, 362,  64,  27, 363,  32, 364, 365, 182, 366, 258,  64,\n",
              "       367, 119,  24, 368, 369, 159, 370,  64, 371, 372, 337, 338, 373,\n",
              "       374, 375, 333, 308,  64, 376, 377, 209, 338, 181,  35, 368, 378,\n",
              "        24, 379, 369, 159, 380,  35, 381,  42, 382, 338, 373, 333, 308,\n",
              "       383, 384, 209, 338, 385,  24,  32, 339, 359, 181, 328, 386, 387,\n",
              "        64, 388, 380, 229, 307,  64, 220,  35, 389, 390, 373, 213, 308,\n",
              "       391,  32, 392, 192, 393,  35,  71, 350, 181,  32, 114,  27, 363,\n",
              "        24, 394,  98,  26,  26,  26, 395,  42, 345, 346, 347, 325, 396,\n",
              "        26,  95, 397, 134, 374,  26, 398, 373,  42, 399, 400, 108, 401,\n",
              "       337, 338,  24,  32, 342, 152, 402, 403, 404, 188, 160, 405, 119,\n",
              "       406, 181, 407, 408,  64,  27,  27, 169,  27,  27,  24, 409, 399,\n",
              "       336, 108, 325, 410, 411,  64, 412, 413, 289, 345, 328,  24, 414,\n",
              "        42,  32,  27, 343, 328, 415, 229, 416,  27, 187, 417])"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ts = corpus[1:];ts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ai_7-yqTFUEA",
        "outputId": "27bd085c-0668-4583-d6d6-b76e874b8f01"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
              "        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
              "        27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  27,\n",
              "        24,  39,  26,  40,  41,  42,  26,  43,  32,  44,  45,  46,  24,\n",
              "        47,  26,  27,  28,  29,  48,  49,  41,  42,  50,  51,  52,  53,\n",
              "        54,  55,  35,  36,  37,  42,  56,  57,  58,  59,  24,  35,  60,\n",
              "        42,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  35,  71,\n",
              "        72,  42,  73,  74,  75,  35,  46,  42,  76,  77,  64,  78,  79,\n",
              "        80,  27,  28,  81,  82,  83,  24,  32,  61,  84,  26,  40,  85,\n",
              "        26,  62,  78,  86,  32,  26,  87,  88,  89,  90,  64,  78,  91,\n",
              "        92,  93,  94,  95,  96,  97,  82,  98,  24,  26,  99,  32, 100,\n",
              "        42, 101, 102,  26, 103,  93, 104,  66, 105, 106, 107,  26, 108,\n",
              "       109,  26,  67,  68, 108,  27,  24, 110, 111, 112, 113,  83,  79,\n",
              "        80,  35, 114,  81,  32, 115, 116, 117, 108, 118, 119, 101, 120,\n",
              "       121,  42, 122,  35, 123, 124,  64, 125, 101, 126,  64,  32, 127,\n",
              "        24,  35,  26,  26,  98,  56,  40, 128,  29, 129,  24, 130, 131,\n",
              "       132, 133,  28,  81, 134, 135, 136,  42,  61, 137, 138, 139, 140,\n",
              "        24, 141,  40, 142,  61, 108, 143, 144, 145,  24, 146,  26, 147,\n",
              "        32,  82, 148, 149,  32,  76, 113, 150,  42, 138, 151, 152, 153,\n",
              "        42,  32,  66, 105,  24, 130, 154, 142, 155, 156, 152, 157, 158,\n",
              "       159, 160, 161,  98, 162, 163,  26,  42, 164, 119,  26,  73, 165,\n",
              "        24, 166,  26, 167,  35, 168,  42,  82, 169,  32, 170,  73, 165,\n",
              "        48,  32, 171, 172,  42, 173, 174,  48, 164, 174,  24,  32,  26,\n",
              "       175,  98,  61,  54,  63, 108, 176, 177, 178, 108, 179, 180, 181,\n",
              "        32,  68, 108,  32, 182, 183,  48, 184,  87,  35, 185, 186,  42,\n",
              "        26, 108,  27,  24, 169,  27,  64,  27,  27, 187,  66, 105,  87,\n",
              "        32,  68, 113, 188,  32, 189,  98,  24,  75,  27, 190, 148, 191,\n",
              "       192,  87,  32, 193,  27, 154, 194,  79,  80, 195, 196,  32, 197,\n",
              "       198,  24, 199,  42,  32, 200, 201,  76, 154,  26, 202, 203, 195,\n",
              "        87, 204,  26,  73,  24,  32, 205,  42,  27,  74, 169, 206,  26,\n",
              "       207,  73,  48,  26,  54, 208, 209,  80, 197,  32,  82,  98,  24,\n",
              "        32,  26, 210,  40,  35, 211, 212,  75, 213,  42, 214, 148, 215,\n",
              "        26, 202,  98, 166,  26,  24,  32,  72,  42, 207,  73,  74,  75,\n",
              "        32,  76, 160,  32, 216,  26, 217, 180, 218, 219,  64, 220,  32,\n",
              "       221, 181, 138,  61,  76, 149, 108, 222, 223, 224, 225,  98,  24,\n",
              "        32, 226, 227,  40, 228, 229,  26, 230,  26, 231,  54, 232, 233,\n",
              "        87,  26,  64,  65,  32,  67,  68,  24,  32, 212, 234,  30, 235,\n",
              "       213, 148, 236,  93,  32, 237, 238, 239,  32, 240,  42,  61, 203,\n",
              "        26,  79,  26,  80,  32, 241, 242,  42,  61,  26, 243, 108, 244,\n",
              "       172,  48, 245, 246, 166,  26,  98,  24,  32, 237,  40, 247,  42,\n",
              "        32, 248, 223, 249,  93, 250, 251, 154,  35, 209, 252,  42, 253,\n",
              "       181,  32, 254,  26, 255, 256,  34,  26,  93, 159, 257,  34,  26,\n",
              "       258,  64,  26, 259,  26,  35, 260,  42,  26, 160,  32, 174,  42,\n",
              "       261, 262,  42, 122,  24,  79, 241,  26, 255, 159,  26,  48, 159,\n",
              "        79, 263, 264, 229,  32, 265, 166,  26, 266,  24, 108, 267,  32,\n",
              "       268, 269, 270, 271,  35, 272, 273, 152, 274, 275, 276,  42,  61,\n",
              "        24, 229,  27, 277, 275, 278, 276,  42,  26,  61,  30, 220, 279,\n",
              "        24, 133,  27,  76, 160,  35, 218,  93, 280, 180, 181,  32,  66,\n",
              "        68, 113,  77,  64,  61, 108,  32, 183,  24, 281,  42,  32, 218,\n",
              "       113, 282, 283, 284,  32,  26,  54,  63,  24,  76, 285, 286,  26,\n",
              "        26,  42,  32, 287, 288, 289,  35, 290,  26, 291, 108, 292,  48,\n",
              "        26, 255,  48,  26, 293,  32, 294, 255, 108,  35, 295,  63,  64,\n",
              "        65,  68,  24,  76, 296, 297,  42, 298, 299,  93, 300, 301, 302,\n",
              "        42,  32, 218,  88, 303,  26, 304,  26,  32, 305,  24, 141, 119,\n",
              "       142, 306,  93, 307,  42, 213,  76,  48, 308, 309,  26, 202,  98,\n",
              "        26, 310, 311, 312,  42, 313, 314, 181,  26, 230,  26,  24, 315,\n",
              "       316, 154,  64, 317,  93, 318, 319, 320, 321,  27,  28,  81,  24,\n",
              "        78,  69, 142, 322, 152, 143, 323, 324, 118,  24, 325, 152, 326,\n",
              "       327, 328, 329,  64, 330, 331, 332,  93, 333, 308, 334, 335, 336,\n",
              "       108, 337, 338,  24,  32, 339, 340, 341, 342,  42,  32,  27, 343,\n",
              "       328, 344, 229,  26, 119, 345, 346, 347, 348,  35, 349,  42,  35,\n",
              "        72, 350,  64,  27,  27, 169,  27,  27, 181,  32, 351, 352, 353,\n",
              "        24, 341, 325, 354, 355,  42, 356,  48,  93,  32, 357, 342, 358,\n",
              "       181,  35, 114,  24, 339, 359,  42,  32, 328, 360, 361,  26, 229,\n",
              "        35, 362,  64,  27, 363,  32, 364, 365, 182, 366, 258,  64, 367,\n",
              "       119,  24, 368, 369, 159, 370,  64, 371, 372, 337, 338, 373, 374,\n",
              "       375, 333, 308,  64, 376, 377, 209, 338, 181,  35, 368, 378,  24,\n",
              "       379, 369, 159, 380,  35, 381,  42, 382, 338, 373, 333, 308, 383,\n",
              "       384, 209, 338, 385,  24,  32, 339, 359, 181, 328, 386, 387,  64,\n",
              "       388, 380, 229, 307,  64, 220,  35, 389, 390, 373, 213, 308, 391,\n",
              "        32, 392, 192, 393,  35,  71, 350, 181,  32, 114,  27, 363,  24,\n",
              "       394,  98,  26,  26,  26, 395,  42, 345, 346, 347, 325, 396,  26,\n",
              "        95, 397, 134, 374,  26, 398, 373,  42, 399, 400, 108, 401, 337,\n",
              "       338,  24,  32, 342, 152, 402, 403, 404, 188, 160, 405, 119, 406,\n",
              "       181, 407, 408,  64,  27,  27, 169,  27,  27,  24, 409, 399, 336,\n",
              "       108, 325, 410, 411,  64, 412, 413, 289, 345, 328,  24, 414,  42,\n",
              "        32,  27, 343, 328, 415, 229, 416,  27, 187, 417,  32])"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_size = len(xs) \n",
        "print('말뭉치크기 :%d, 어휘수 : %d'% (corpus_size, vocab_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsUi4S4rFXpz",
        "outputId": "aeba5e3a-badd-4f99-874b-2b7cba7420c5"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "말뭉치크기 :1000, 어휘수 : 418\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_iters = data_size // (batch_size* time_size) ;max_iters #  // 몫만 나옴"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Htqpez7tFeL2",
        "outputId": "fed2fff2-66e7-4f59-c61b-c7fe12263865"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "time_idx = 0\n",
        "total_loss= 0\n",
        "loss_count= 0\n",
        "ppl_list =[]\n"
      ],
      "metadata": {
        "id": "o0uY5YShGhQb"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모델생성\n",
        "model = SimpleRnnlm(vocab_size, wordvec_size, hidden_size) \n",
        "optimizer= SGD(lr)"
      ],
      "metadata": {
        "id": "VktxvxUwGsPr"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jump = (corpus_size -1) // batch_size ; jump"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ml37cQ0oGjiM",
        "outputId": "2cf0d26c-569d-44d4-fd14-af1eb20d4b8c"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "99"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "offsets = [i * jump for i in range(batch_size)] ;offsets\n",
        "# 데이터를 읽기 시작하는 위치"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-Ayy09aHAP2",
        "outputId": "3b7abd1c-2230-49c1-f157-75cd4db258d4"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 99, 198, 297, 396, 495, 594, 693, 792, 891]"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(max_epoch):\n",
        "  for iter in range(max_iters):\n",
        "    batch_x = np.empty((batch_size, time_size) , dtype = 'i')\n",
        "    batch_t = np.empty((batch_size, time_size) , dtype = 'i')\n",
        "    for t in range(time_size): \n",
        "      for i, offset in enumerate(offsets): \n",
        "        batch_x[i,t] = xs[(offset+time_idx) % data_size]\n",
        "        batch_t[i,t] = ts[(offset+time_idx) % data_size]\n",
        "      time_idx += 1 \n",
        "    \n",
        "    # 기울기를 구하여 매개변수 갱신\n",
        "    loss = model.forward(batch_x, batch_t) \n",
        "    model.backward() \n",
        "    optimizer.update(model.params, model.grads)\n",
        "    total_loss += loss \n",
        "    loss_count += 1\n",
        "\n",
        "  \n",
        "  # 에폭마다 퍼플렉시티 평가\n",
        "  ppl = np.exp(total_loss / loss_count) \n",
        "  print('에폭  %d : 퍼플렉시티 : %.2f'% (epoch+1, ppl))\n",
        "  ppl_list.append(float(ppl))\n",
        "  total_loss, loss_count = 0,0 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuxedpciHEv7",
        "outputId": "89783b38-a942-491b-fc9b-0a77c86f0e86"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "에폭  1 : 퍼플렉시티 : 367.58\n",
            "에폭  2 : 퍼플렉시티 : 240.71\n",
            "에폭  3 : 퍼플렉시티 : 218.79\n",
            "에폭  4 : 퍼플렉시티 : 212.52\n",
            "에폭  5 : 퍼플렉시티 : 204.71\n",
            "에폭  6 : 퍼플렉시티 : 201.40\n",
            "에폭  7 : 퍼플렉시티 : 198.36\n",
            "에폭  8 : 퍼플렉시티 : 196.15\n",
            "에폭  9 : 퍼플렉시티 : 191.70\n",
            "에폭  10 : 퍼플렉시티 : 193.31\n",
            "에폭  11 : 퍼플렉시티 : 189.69\n",
            "에폭  12 : 퍼플렉시티 : 192.35\n",
            "에폭  13 : 퍼플렉시티 : 190.70\n",
            "에폭  14 : 퍼플렉시티 : 190.84\n",
            "에폭  15 : 퍼플렉시티 : 190.35\n",
            "에폭  16 : 퍼플렉시티 : 186.66\n",
            "에폭  17 : 퍼플렉시티 : 184.52\n",
            "에폭  18 : 퍼플렉시티 : 181.39\n",
            "에폭  19 : 퍼플렉시티 : 182.73\n",
            "에폭  20 : 퍼플렉시티 : 183.42\n",
            "에폭  21 : 퍼플렉시티 : 182.03\n",
            "에폭  22 : 퍼플렉시티 : 178.09\n",
            "에폭  23 : 퍼플렉시티 : 175.23\n",
            "에폭  24 : 퍼플렉시티 : 176.35\n",
            "에폭  25 : 퍼플렉시티 : 174.85\n",
            "에폭  26 : 퍼플렉시티 : 176.07\n",
            "에폭  27 : 퍼플렉시티 : 170.54\n",
            "에폭  28 : 퍼플렉시티 : 168.82\n",
            "에폭  29 : 퍼플렉시티 : 166.02\n",
            "에폭  30 : 퍼플렉시티 : 160.37\n",
            "에폭  31 : 퍼플렉시티 : 161.62\n",
            "에폭  32 : 퍼플렉시티 : 157.35\n",
            "에폭  33 : 퍼플렉시티 : 158.20\n",
            "에폭  34 : 퍼플렉시티 : 150.91\n",
            "에폭  35 : 퍼플렉시티 : 150.37\n",
            "에폭  36 : 퍼플렉시티 : 145.22\n",
            "에폭  37 : 퍼플렉시티 : 140.26\n",
            "에폭  38 : 퍼플렉시티 : 140.20\n",
            "에폭  39 : 퍼플렉시티 : 131.90\n",
            "에폭  40 : 퍼플렉시티 : 128.22\n",
            "에폭  41 : 퍼플렉시티 : 129.28\n",
            "에폭  42 : 퍼플렉시티 : 121.13\n",
            "에폭  43 : 퍼플렉시티 : 116.17\n",
            "에폭  44 : 퍼플렉시티 : 112.63\n",
            "에폭  45 : 퍼플렉시티 : 108.07\n",
            "에폭  46 : 퍼플렉시티 : 107.16\n",
            "에폭  47 : 퍼플렉시티 : 102.05\n",
            "에폭  48 : 퍼플렉시티 : 96.92\n",
            "에폭  49 : 퍼플렉시티 : 93.71\n",
            "에폭  50 : 퍼플렉시티 : 89.36\n",
            "에폭  51 : 퍼플렉시티 : 88.00\n",
            "에폭  52 : 퍼플렉시티 : 82.51\n",
            "에폭  53 : 퍼플렉시티 : 78.55\n",
            "에폭  54 : 퍼플렉시티 : 74.95\n",
            "에폭  55 : 퍼플렉시티 : 71.71\n",
            "에폭  56 : 퍼플렉시티 : 68.05\n",
            "에폭  57 : 퍼플렉시티 : 63.71\n",
            "에폭  58 : 퍼플렉시티 : 63.52\n",
            "에폭  59 : 퍼플렉시티 : 59.48\n",
            "에폭  60 : 퍼플렉시티 : 54.99\n",
            "에폭  61 : 퍼플렉시티 : 53.66\n",
            "에폭  62 : 퍼플렉시티 : 51.13\n",
            "에폭  63 : 퍼플렉시티 : 46.46\n",
            "에폭  64 : 퍼플렉시티 : 45.44\n",
            "에폭  65 : 퍼플렉시티 : 42.27\n",
            "에폭  66 : 퍼플렉시티 : 40.70\n",
            "에폭  67 : 퍼플렉시티 : 38.99\n",
            "에폭  68 : 퍼플렉시티 : 35.46\n",
            "에폭  69 : 퍼플렉시티 : 33.99\n",
            "에폭  70 : 퍼플렉시티 : 33.01\n",
            "에폭  71 : 퍼플렉시티 : 30.65\n",
            "에폭  72 : 퍼플렉시티 : 29.26\n",
            "에폭  73 : 퍼플렉시티 : 27.59\n",
            "에폭  74 : 퍼플렉시티 : 25.61\n",
            "에폭  75 : 퍼플렉시티 : 25.03\n",
            "에폭  76 : 퍼플렉시티 : 23.14\n",
            "에폭  77 : 퍼플렉시티 : 21.56\n",
            "에폭  78 : 퍼플렉시티 : 21.31\n",
            "에폭  79 : 퍼플렉시티 : 19.82\n",
            "에폭  80 : 퍼플렉시티 : 19.15\n",
            "에폭  81 : 퍼플렉시티 : 18.20\n",
            "에폭  82 : 퍼플렉시티 : 18.00\n",
            "에폭  83 : 퍼플렉시티 : 15.73\n",
            "에폭  84 : 퍼플렉시티 : 14.82\n",
            "에폭  85 : 퍼플렉시티 : 14.11\n",
            "에폭  86 : 퍼플렉시티 : 14.00\n",
            "에폭  87 : 퍼플렉시티 : 13.00\n",
            "에폭  88 : 퍼플렉시티 : 11.99\n",
            "에폭  89 : 퍼플렉시티 : 11.48\n",
            "에폭  90 : 퍼플렉시티 : 11.17\n",
            "에폭  91 : 퍼플렉시티 : 10.95\n",
            "에폭  92 : 퍼플렉시티 : 10.41\n",
            "에폭  93 : 퍼플렉시티 : 9.42\n",
            "에폭  94 : 퍼플렉시티 : 9.23\n",
            "에폭  95 : 퍼플렉시티 : 8.88\n",
            "에폭  96 : 퍼플렉시티 : 8.10\n",
            "에폭  97 : 퍼플렉시티 : 7.69\n",
            "에폭  98 : 퍼플렉시티 : 7.38\n",
            "에폭  99 : 퍼플렉시티 : 6.92\n",
            "에폭  100 : 퍼플렉시티 : 6.55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "from common.trainer import RnnlmTrainer\n",
        "\n",
        "# 모델 생성\n",
        "model = SimpleRnnlm(vocab_size, wordvec_size, hidden_size)\n",
        "optimizer = SGD(lr)\n",
        "trainer = RnnlmTrainer(model, optimizer)\n",
        "\n",
        "trainer.fit(xs, ts, max_epoch, batch_size, time_size)\n",
        "trainer.plot()\n",
        "\n",
        "# model / optimizer를 주어 초기화함\n",
        "# fit() 메서드를 호출해 학습을 수행함\n",
        "# 미니배치를 순차적으로 만들고, 모델의 순전파와 역전파를 호출하고 옵티마이저로 가중치를 계산\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FELgQPMPNH2R",
        "outputId": "844ae843-1085-4a07-b08c-1daef2e31a40"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| 에폭 1 |  반복 1 / 19 | 시간 0[s] | 퍼플렉서티 416.17\n",
            "| 에폭 2 |  반복 1 / 19 | 시간 0[s] | 퍼플렉서티 386.21\n",
            "| 에폭 3 |  반복 1 / 19 | 시간 0[s] | 퍼플렉서티 267.26\n",
            "| 에폭 4 |  반복 1 / 19 | 시간 0[s] | 퍼플렉서티 222.67\n",
            "| 에폭 5 |  반복 1 / 19 | 시간 0[s] | 퍼플렉서티 211.44\n",
            "| 에폭 6 |  반복 1 / 19 | 시간 0[s] | 퍼플렉서티 210.26\n",
            "| 에폭 7 |  반복 1 / 19 | 시간 0[s] | 퍼플렉서티 201.32\n",
            "| 에폭 8 |  반복 1 / 19 | 시간 0[s] | 퍼플렉서티 200.22\n",
            "| 에폭 9 |  반복 1 / 19 | 시간 0[s] | 퍼플렉서티 195.65\n",
            "| 에폭 10 |  반복 1 / 19 | 시간 0[s] | 퍼플렉서티 190.39\n",
            "| 에폭 11 |  반복 1 / 19 | 시간 1[s] | 퍼플렉서티 193.55\n",
            "| 에폭 12 |  반복 1 / 19 | 시간 1[s] | 퍼플렉서티 189.58\n",
            "| 에폭 13 |  반복 1 / 19 | 시간 1[s] | 퍼플렉서티 192.75\n",
            "| 에폭 14 |  반복 1 / 19 | 시간 1[s] | 퍼플렉서티 187.18\n",
            "| 에폭 15 |  반복 1 / 19 | 시간 1[s] | 퍼플렉서티 186.85\n",
            "| 에폭 16 |  반복 1 / 19 | 시간 1[s] | 퍼플렉서티 191.16\n",
            "| 에폭 17 |  반복 1 / 19 | 시간 1[s] | 퍼플렉서티 189.19\n",
            "| 에폭 18 |  반복 1 / 19 | 시간 1[s] | 퍼플렉서티 184.39\n",
            "| 에폭 19 |  반복 1 / 19 | 시간 1[s] | 퍼플렉서티 180.28\n",
            "| 에폭 20 |  반복 1 / 19 | 시간 1[s] | 퍼플렉서티 181.61\n",
            "| 에폭 21 |  반복 1 / 19 | 시간 2[s] | 퍼플렉서티 178.34\n",
            "| 에폭 22 |  반복 1 / 19 | 시간 2[s] | 퍼플렉서티 176.56\n",
            "| 에폭 23 |  반복 1 / 19 | 시간 2[s] | 퍼플렉서티 177.70\n",
            "| 에폭 24 |  반복 1 / 19 | 시간 2[s] | 퍼플렉서티 174.99\n",
            "| 에폭 25 |  반복 1 / 19 | 시간 2[s] | 퍼플렉서티 168.22\n",
            "| 에폭 26 |  반복 1 / 19 | 시간 2[s] | 퍼플렉서티 170.15\n",
            "| 에폭 27 |  반복 1 / 19 | 시간 2[s] | 퍼플렉서티 167.65\n",
            "| 에폭 28 |  반복 1 / 19 | 시간 2[s] | 퍼플렉서티 166.46\n",
            "| 에폭 29 |  반복 1 / 19 | 시간 2[s] | 퍼플렉서티 159.04\n",
            "| 에폭 30 |  반복 1 / 19 | 시간 2[s] | 퍼플렉서티 155.73\n",
            "| 에폭 31 |  반복 1 / 19 | 시간 3[s] | 퍼플렉서티 153.54\n",
            "| 에폭 32 |  반복 1 / 19 | 시간 3[s] | 퍼플렉서티 147.16\n",
            "| 에폭 33 |  반복 1 / 19 | 시간 3[s] | 퍼플렉서티 145.30\n",
            "| 에폭 34 |  반복 1 / 19 | 시간 3[s] | 퍼플렉서티 143.51\n",
            "| 에폭 35 |  반복 1 / 19 | 시간 3[s] | 퍼플렉서티 136.99\n",
            "| 에폭 36 |  반복 1 / 19 | 시간 3[s] | 퍼플렉서티 132.29\n",
            "| 에폭 37 |  반복 1 / 19 | 시간 3[s] | 퍼플렉서티 132.24\n",
            "| 에폭 38 |  반복 1 / 19 | 시간 3[s] | 퍼플렉서티 124.04\n",
            "| 에폭 39 |  반복 1 / 19 | 시간 3[s] | 퍼플렉서티 119.60\n",
            "| 에폭 40 |  반복 1 / 19 | 시간 3[s] | 퍼플렉서티 113.89\n",
            "| 에폭 41 |  반복 1 / 19 | 시간 4[s] | 퍼플렉서티 108.55\n",
            "| 에폭 42 |  반복 1 / 19 | 시간 4[s] | 퍼플렉서티 107.26\n",
            "| 에폭 43 |  반복 1 / 19 | 시간 4[s] | 퍼플렉서티 101.92\n",
            "| 에폭 44 |  반복 1 / 19 | 시간 4[s] | 퍼플렉서티 98.70\n",
            "| 에폭 45 |  반복 1 / 19 | 시간 4[s] | 퍼플렉서티 90.14\n",
            "| 에폭 46 |  반복 1 / 19 | 시간 4[s] | 퍼플렉서티 86.94\n",
            "| 에폭 47 |  반복 1 / 19 | 시간 4[s] | 퍼플렉서티 83.97\n",
            "| 에폭 48 |  반복 1 / 19 | 시간 4[s] | 퍼플렉서티 80.56\n",
            "| 에폭 49 |  반복 1 / 19 | 시간 4[s] | 퍼플렉서티 77.01\n",
            "| 에폭 50 |  반복 1 / 19 | 시간 4[s] | 퍼플렉서티 71.22\n",
            "| 에폭 51 |  반복 1 / 19 | 시간 5[s] | 퍼플렉서티 69.14\n",
            "| 에폭 52 |  반복 1 / 19 | 시간 5[s] | 퍼플렉서티 65.05\n",
            "| 에폭 53 |  반복 1 / 19 | 시간 5[s] | 퍼플렉서티 61.78\n",
            "| 에폭 54 |  반복 1 / 19 | 시간 5[s] | 퍼플렉서티 58.74\n",
            "| 에폭 55 |  반복 1 / 19 | 시간 5[s] | 퍼플렉서티 54.54\n",
            "| 에폭 56 |  반복 1 / 19 | 시간 5[s] | 퍼플렉서티 51.06\n",
            "| 에폭 57 |  반복 1 / 19 | 시간 5[s] | 퍼플렉서티 50.90\n",
            "| 에폭 58 |  반복 1 / 19 | 시간 5[s] | 퍼플렉서티 46.39\n",
            "| 에폭 59 |  반복 1 / 19 | 시간 5[s] | 퍼플렉서티 44.81\n",
            "| 에폭 60 |  반복 1 / 19 | 시간 5[s] | 퍼플렉서티 41.85\n",
            "| 에폭 61 |  반복 1 / 19 | 시간 6[s] | 퍼플렉서티 38.54\n",
            "| 에폭 62 |  반복 1 / 19 | 시간 6[s] | 퍼플렉서티 36.62\n",
            "| 에폭 63 |  반복 1 / 19 | 시간 6[s] | 퍼플렉서티 35.61\n",
            "| 에폭 64 |  반복 1 / 19 | 시간 6[s] | 퍼플렉서티 34.11\n",
            "| 에폭 65 |  반복 1 / 19 | 시간 6[s] | 퍼플렉서티 32.14\n",
            "| 에폭 66 |  반복 1 / 19 | 시간 6[s] | 퍼플렉서티 29.83\n",
            "| 에폭 67 |  반복 1 / 19 | 시간 6[s] | 퍼플렉서티 27.54\n",
            "| 에폭 68 |  반복 1 / 19 | 시간 6[s] | 퍼플렉서티 26.38\n",
            "| 에폭 69 |  반복 1 / 19 | 시간 6[s] | 퍼플렉서티 26.12\n",
            "| 에폭 70 |  반복 1 / 19 | 시간 6[s] | 퍼플렉서티 23.27\n",
            "| 에폭 71 |  반복 1 / 19 | 시간 6[s] | 퍼플렉서티 22.62\n",
            "| 에폭 72 |  반복 1 / 19 | 시간 7[s] | 퍼플렉서티 21.11\n",
            "| 에폭 73 |  반복 1 / 19 | 시간 7[s] | 퍼플렉서티 19.84\n",
            "| 에폭 74 |  반복 1 / 19 | 시간 7[s] | 퍼플렉서티 18.39\n",
            "| 에폭 75 |  반복 1 / 19 | 시간 7[s] | 퍼플렉서티 17.96\n",
            "| 에폭 76 |  반복 1 / 19 | 시간 7[s] | 퍼플렉서티 16.82\n",
            "| 에폭 77 |  반복 1 / 19 | 시간 7[s] | 퍼플렉서티 16.65\n",
            "| 에폭 78 |  반복 1 / 19 | 시간 7[s] | 퍼플렉서티 15.54\n",
            "| 에폭 79 |  반복 1 / 19 | 시간 7[s] | 퍼플렉서티 14.35\n",
            "| 에폭 80 |  반복 1 / 19 | 시간 7[s] | 퍼플렉서티 13.33\n",
            "| 에폭 81 |  반복 1 / 19 | 시간 7[s] | 퍼플렉서티 12.48\n",
            "| 에폭 82 |  반복 1 / 19 | 시간 7[s] | 퍼플렉서티 12.27\n",
            "| 에폭 83 |  반복 1 / 19 | 시간 8[s] | 퍼플렉서티 11.72\n",
            "| 에폭 84 |  반복 1 / 19 | 시간 8[s] | 퍼플렉서티 11.51\n",
            "| 에폭 85 |  반복 1 / 19 | 시간 8[s] | 퍼플렉서티 10.82\n",
            "| 에폭 86 |  반복 1 / 19 | 시간 8[s] | 퍼플렉서티 10.05\n",
            "| 에폭 87 |  반복 1 / 19 | 시간 8[s] | 퍼플렉서티 9.36\n",
            "| 에폭 88 |  반복 1 / 19 | 시간 8[s] | 퍼플렉서티 8.84\n",
            "| 에폭 89 |  반복 1 / 19 | 시간 8[s] | 퍼플렉서티 8.64\n",
            "| 에폭 90 |  반복 1 / 19 | 시간 8[s] | 퍼플렉서티 8.22\n",
            "| 에폭 91 |  반복 1 / 19 | 시간 8[s] | 퍼플렉서티 7.83\n",
            "| 에폭 92 |  반복 1 / 19 | 시간 8[s] | 퍼플렉서티 7.35\n",
            "| 에폭 93 |  반복 1 / 19 | 시간 9[s] | 퍼플렉서티 7.13\n",
            "| 에폭 94 |  반복 1 / 19 | 시간 9[s] | 퍼플렉서티 6.66\n",
            "| 에폭 95 |  반복 1 / 19 | 시간 9[s] | 퍼플렉서티 6.40\n",
            "| 에폭 96 |  반복 1 / 19 | 시간 9[s] | 퍼플렉서티 6.09\n",
            "| 에폭 97 |  반복 1 / 19 | 시간 9[s] | 퍼플렉서티 5.78\n",
            "| 에폭 98 |  반복 1 / 19 | 시간 9[s] | 퍼플렉서티 5.82\n",
            "| 에폭 99 |  반복 1 / 19 | 시간 9[s] | 퍼플렉서티 5.42\n",
            "| 에폭 100 |  반복 1 / 19 | 시간 9[s] | 퍼플렉서티 5.32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48152 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48373 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54140 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54540 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47113 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49436 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54000 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48152 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48373 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54140 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54540 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47113 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49436 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54000 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "time_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "968YsMogP3Uq",
        "outputId": "ada9f2fc-4575-492f-c944-a7ec838ac8e0"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpCq-OKJNaCF",
        "outputId": "43e5704b-c646-41b5-c962-520fd10461b4"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qo2jpzTsP6M9",
        "outputId": "65df9475-14da-4089-c2b5-dc96641e30b3"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
              "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
              "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
              "        27,  24,  39,  26,  40,  41,  42,  26,  43,  32,  44,  45,  46,\n",
              "        24,  47,  26,  27,  28,  29,  48,  49,  41,  42,  50,  51,  52,\n",
              "        53,  54,  55,  35,  36,  37,  42,  56,  57,  58,  59,  24,  35,\n",
              "        60,  42,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  35,\n",
              "        71,  72,  42,  73,  74,  75,  35,  46,  42,  76,  77,  64,  78,\n",
              "        79,  80,  27,  28,  81,  82,  83,  24,  32,  61,  84,  26,  40,\n",
              "        85,  26,  62,  78,  86,  32,  26,  87,  88,  89,  90,  64,  78,\n",
              "        91,  92,  93,  94,  95,  96,  97,  82,  98,  24,  26,  99,  32,\n",
              "       100,  42, 101, 102,  26, 103,  93, 104,  66, 105, 106, 107,  26,\n",
              "       108, 109,  26,  67,  68, 108,  27,  24, 110, 111, 112, 113,  83,\n",
              "        79,  80,  35, 114,  81,  32, 115, 116, 117, 108, 118, 119, 101,\n",
              "       120, 121,  42, 122,  35, 123, 124,  64, 125, 101, 126,  64,  32,\n",
              "       127,  24,  35,  26,  26,  98,  56,  40, 128,  29, 129,  24, 130,\n",
              "       131, 132, 133,  28,  81, 134, 135, 136,  42,  61, 137, 138, 139,\n",
              "       140,  24, 141,  40, 142,  61, 108, 143, 144, 145,  24, 146,  26,\n",
              "       147,  32,  82, 148, 149,  32,  76, 113, 150,  42, 138, 151, 152,\n",
              "       153,  42,  32,  66, 105,  24, 130, 154, 142, 155, 156, 152, 157,\n",
              "       158, 159, 160, 161,  98, 162, 163,  26,  42, 164, 119,  26,  73,\n",
              "       165,  24, 166,  26, 167,  35, 168,  42,  82, 169,  32, 170,  73,\n",
              "       165,  48,  32, 171, 172,  42, 173, 174,  48, 164, 174,  24,  32,\n",
              "        26, 175,  98,  61,  54,  63, 108, 176, 177, 178, 108, 179, 180,\n",
              "       181,  32,  68, 108,  32, 182, 183,  48, 184,  87,  35, 185, 186,\n",
              "        42,  26, 108,  27,  24, 169,  27,  64,  27,  27, 187,  66, 105,\n",
              "        87,  32,  68, 113, 188,  32, 189,  98,  24,  75,  27, 190, 148,\n",
              "       191, 192,  87,  32, 193,  27, 154, 194,  79,  80, 195, 196,  32,\n",
              "       197, 198,  24, 199,  42,  32, 200, 201,  76, 154,  26, 202, 203,\n",
              "       195,  87, 204,  26,  73,  24,  32, 205,  42,  27,  74, 169, 206,\n",
              "        26, 207,  73,  48,  26,  54, 208, 209,  80, 197,  32,  82,  98,\n",
              "        24,  32,  26, 210,  40,  35, 211, 212,  75, 213,  42, 214, 148,\n",
              "       215,  26, 202,  98, 166,  26,  24,  32,  72,  42, 207,  73,  74,\n",
              "        75,  32,  76, 160,  32, 216,  26, 217, 180, 218, 219,  64, 220,\n",
              "        32, 221, 181, 138,  61,  76, 149, 108, 222, 223, 224, 225,  98,\n",
              "        24,  32, 226, 227,  40, 228, 229,  26, 230,  26, 231,  54, 232,\n",
              "       233,  87,  26,  64,  65,  32,  67,  68,  24,  32, 212, 234,  30,\n",
              "       235, 213, 148, 236,  93,  32, 237, 238, 239,  32, 240,  42,  61,\n",
              "       203,  26,  79,  26,  80,  32, 241, 242,  42,  61,  26, 243, 108,\n",
              "       244, 172,  48, 245, 246, 166,  26,  98,  24,  32, 237,  40, 247,\n",
              "        42,  32, 248, 223, 249,  93, 250, 251, 154,  35, 209, 252,  42,\n",
              "       253, 181,  32, 254,  26, 255, 256,  34,  26,  93, 159, 257,  34,\n",
              "        26, 258,  64,  26, 259,  26,  35, 260,  42,  26, 160,  32, 174,\n",
              "        42, 261, 262,  42, 122,  24,  79, 241,  26, 255, 159,  26,  48,\n",
              "       159,  79, 263, 264, 229,  32, 265, 166,  26, 266,  24, 108, 267,\n",
              "        32, 268, 269, 270, 271,  35, 272, 273, 152, 274, 275, 276,  42,\n",
              "        61,  24, 229,  27, 277, 275, 278, 276,  42,  26,  61,  30, 220,\n",
              "       279,  24, 133,  27,  76, 160,  35, 218,  93, 280, 180, 181,  32,\n",
              "        66,  68, 113,  77,  64,  61, 108,  32, 183,  24, 281,  42,  32,\n",
              "       218, 113, 282, 283, 284,  32,  26,  54,  63,  24,  76, 285, 286,\n",
              "        26,  26,  42,  32, 287, 288, 289,  35, 290,  26, 291, 108, 292,\n",
              "        48,  26, 255,  48,  26, 293,  32, 294, 255, 108,  35, 295,  63,\n",
              "        64,  65,  68,  24,  76, 296, 297,  42, 298, 299,  93, 300, 301,\n",
              "       302,  42,  32, 218,  88, 303,  26, 304,  26,  32, 305,  24, 141,\n",
              "       119, 142, 306,  93, 307,  42, 213,  76,  48, 308, 309,  26, 202,\n",
              "        98,  26, 310, 311, 312,  42, 313, 314, 181,  26, 230,  26,  24,\n",
              "       315, 316, 154,  64, 317,  93, 318, 319, 320, 321,  27,  28,  81,\n",
              "        24,  78,  69, 142, 322, 152, 143, 323, 324, 118,  24, 325, 152,\n",
              "       326, 327, 328, 329,  64, 330, 331, 332,  93, 333, 308, 334, 335,\n",
              "       336, 108, 337, 338,  24,  32, 339, 340, 341, 342,  42,  32,  27,\n",
              "       343, 328, 344, 229,  26, 119, 345, 346, 347, 348,  35, 349,  42,\n",
              "        35,  72, 350,  64,  27,  27, 169,  27,  27, 181,  32, 351, 352,\n",
              "       353,  24, 341, 325, 354, 355,  42, 356,  48,  93,  32, 357, 342,\n",
              "       358, 181,  35, 114,  24, 339, 359,  42,  32, 328, 360, 361,  26,\n",
              "       229,  35, 362,  64,  27, 363,  32, 364, 365, 182, 366, 258,  64,\n",
              "       367, 119,  24, 368, 369, 159, 370,  64, 371, 372, 337, 338, 373,\n",
              "       374, 375, 333, 308,  64, 376, 377, 209, 338, 181,  35, 368, 378,\n",
              "        24, 379, 369, 159, 380,  35, 381,  42, 382, 338, 373, 333, 308,\n",
              "       383, 384, 209, 338, 385,  24,  32, 339, 359, 181, 328, 386, 387,\n",
              "        64, 388, 380, 229, 307,  64, 220,  35, 389, 390, 373, 213, 308,\n",
              "       391,  32, 392, 192, 393,  35,  71, 350, 181,  32, 114,  27, 363,\n",
              "        24, 394,  98,  26,  26,  26, 395,  42, 345, 346, 347, 325, 396,\n",
              "        26,  95, 397, 134, 374,  26, 398, 373,  42, 399, 400, 108, 401,\n",
              "       337, 338,  24,  32, 342, 152, 402, 403, 404, 188, 160, 405, 119,\n",
              "       406, 181, 407, 408,  64,  27,  27, 169,  27,  27,  24, 409, 399,\n",
              "       336, 108, 325, 410, 411,  64, 412, 413, 289, 345, 328,  24, 414,\n",
              "        42,  32,  27, 343, 328, 415, 229, 416,  27, 187, 417])"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6ZGBdL7P-FF",
        "outputId": "31c6ddfa-e00f-4fd6-cc91-6cc4666d6153"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
              "        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
              "        27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  27,\n",
              "        24,  39,  26,  40,  41,  42,  26,  43,  32,  44,  45,  46,  24,\n",
              "        47,  26,  27,  28,  29,  48,  49,  41,  42,  50,  51,  52,  53,\n",
              "        54,  55,  35,  36,  37,  42,  56,  57,  58,  59,  24,  35,  60,\n",
              "        42,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  35,  71,\n",
              "        72,  42,  73,  74,  75,  35,  46,  42,  76,  77,  64,  78,  79,\n",
              "        80,  27,  28,  81,  82,  83,  24,  32,  61,  84,  26,  40,  85,\n",
              "        26,  62,  78,  86,  32,  26,  87,  88,  89,  90,  64,  78,  91,\n",
              "        92,  93,  94,  95,  96,  97,  82,  98,  24,  26,  99,  32, 100,\n",
              "        42, 101, 102,  26, 103,  93, 104,  66, 105, 106, 107,  26, 108,\n",
              "       109,  26,  67,  68, 108,  27,  24, 110, 111, 112, 113,  83,  79,\n",
              "        80,  35, 114,  81,  32, 115, 116, 117, 108, 118, 119, 101, 120,\n",
              "       121,  42, 122,  35, 123, 124,  64, 125, 101, 126,  64,  32, 127,\n",
              "        24,  35,  26,  26,  98,  56,  40, 128,  29, 129,  24, 130, 131,\n",
              "       132, 133,  28,  81, 134, 135, 136,  42,  61, 137, 138, 139, 140,\n",
              "        24, 141,  40, 142,  61, 108, 143, 144, 145,  24, 146,  26, 147,\n",
              "        32,  82, 148, 149,  32,  76, 113, 150,  42, 138, 151, 152, 153,\n",
              "        42,  32,  66, 105,  24, 130, 154, 142, 155, 156, 152, 157, 158,\n",
              "       159, 160, 161,  98, 162, 163,  26,  42, 164, 119,  26,  73, 165,\n",
              "        24, 166,  26, 167,  35, 168,  42,  82, 169,  32, 170,  73, 165,\n",
              "        48,  32, 171, 172,  42, 173, 174,  48, 164, 174,  24,  32,  26,\n",
              "       175,  98,  61,  54,  63, 108, 176, 177, 178, 108, 179, 180, 181,\n",
              "        32,  68, 108,  32, 182, 183,  48, 184,  87,  35, 185, 186,  42,\n",
              "        26, 108,  27,  24, 169,  27,  64,  27,  27, 187,  66, 105,  87,\n",
              "        32,  68, 113, 188,  32, 189,  98,  24,  75,  27, 190, 148, 191,\n",
              "       192,  87,  32, 193,  27, 154, 194,  79,  80, 195, 196,  32, 197,\n",
              "       198,  24, 199,  42,  32, 200, 201,  76, 154,  26, 202, 203, 195,\n",
              "        87, 204,  26,  73,  24,  32, 205,  42,  27,  74, 169, 206,  26,\n",
              "       207,  73,  48,  26,  54, 208, 209,  80, 197,  32,  82,  98,  24,\n",
              "        32,  26, 210,  40,  35, 211, 212,  75, 213,  42, 214, 148, 215,\n",
              "        26, 202,  98, 166,  26,  24,  32,  72,  42, 207,  73,  74,  75,\n",
              "        32,  76, 160,  32, 216,  26, 217, 180, 218, 219,  64, 220,  32,\n",
              "       221, 181, 138,  61,  76, 149, 108, 222, 223, 224, 225,  98,  24,\n",
              "        32, 226, 227,  40, 228, 229,  26, 230,  26, 231,  54, 232, 233,\n",
              "        87,  26,  64,  65,  32,  67,  68,  24,  32, 212, 234,  30, 235,\n",
              "       213, 148, 236,  93,  32, 237, 238, 239,  32, 240,  42,  61, 203,\n",
              "        26,  79,  26,  80,  32, 241, 242,  42,  61,  26, 243, 108, 244,\n",
              "       172,  48, 245, 246, 166,  26,  98,  24,  32, 237,  40, 247,  42,\n",
              "        32, 248, 223, 249,  93, 250, 251, 154,  35, 209, 252,  42, 253,\n",
              "       181,  32, 254,  26, 255, 256,  34,  26,  93, 159, 257,  34,  26,\n",
              "       258,  64,  26, 259,  26,  35, 260,  42,  26, 160,  32, 174,  42,\n",
              "       261, 262,  42, 122,  24,  79, 241,  26, 255, 159,  26,  48, 159,\n",
              "        79, 263, 264, 229,  32, 265, 166,  26, 266,  24, 108, 267,  32,\n",
              "       268, 269, 270, 271,  35, 272, 273, 152, 274, 275, 276,  42,  61,\n",
              "        24, 229,  27, 277, 275, 278, 276,  42,  26,  61,  30, 220, 279,\n",
              "        24, 133,  27,  76, 160,  35, 218,  93, 280, 180, 181,  32,  66,\n",
              "        68, 113,  77,  64,  61, 108,  32, 183,  24, 281,  42,  32, 218,\n",
              "       113, 282, 283, 284,  32,  26,  54,  63,  24,  76, 285, 286,  26,\n",
              "        26,  42,  32, 287, 288, 289,  35, 290,  26, 291, 108, 292,  48,\n",
              "        26, 255,  48,  26, 293,  32, 294, 255, 108,  35, 295,  63,  64,\n",
              "        65,  68,  24,  76, 296, 297,  42, 298, 299,  93, 300, 301, 302,\n",
              "        42,  32, 218,  88, 303,  26, 304,  26,  32, 305,  24, 141, 119,\n",
              "       142, 306,  93, 307,  42, 213,  76,  48, 308, 309,  26, 202,  98,\n",
              "        26, 310, 311, 312,  42, 313, 314, 181,  26, 230,  26,  24, 315,\n",
              "       316, 154,  64, 317,  93, 318, 319, 320, 321,  27,  28,  81,  24,\n",
              "        78,  69, 142, 322, 152, 143, 323, 324, 118,  24, 325, 152, 326,\n",
              "       327, 328, 329,  64, 330, 331, 332,  93, 333, 308, 334, 335, 336,\n",
              "       108, 337, 338,  24,  32, 339, 340, 341, 342,  42,  32,  27, 343,\n",
              "       328, 344, 229,  26, 119, 345, 346, 347, 348,  35, 349,  42,  35,\n",
              "        72, 350,  64,  27,  27, 169,  27,  27, 181,  32, 351, 352, 353,\n",
              "        24, 341, 325, 354, 355,  42, 356,  48,  93,  32, 357, 342, 358,\n",
              "       181,  35, 114,  24, 339, 359,  42,  32, 328, 360, 361,  26, 229,\n",
              "        35, 362,  64,  27, 363,  32, 364, 365, 182, 366, 258,  64, 367,\n",
              "       119,  24, 368, 369, 159, 370,  64, 371, 372, 337, 338, 373, 374,\n",
              "       375, 333, 308,  64, 376, 377, 209, 338, 181,  35, 368, 378,  24,\n",
              "       379, 369, 159, 380,  35, 381,  42, 382, 338, 373, 333, 308, 383,\n",
              "       384, 209, 338, 385,  24,  32, 339, 359, 181, 328, 386, 387,  64,\n",
              "       388, 380, 229, 307,  64, 220,  35, 389, 390, 373, 213, 308, 391,\n",
              "        32, 392, 192, 393,  35,  71, 350, 181,  32, 114,  27, 363,  24,\n",
              "       394,  98,  26,  26,  26, 395,  42, 345, 346, 347, 325, 396,  26,\n",
              "        95, 397, 134, 374,  26, 398, 373,  42, 399, 400, 108, 401, 337,\n",
              "       338,  24,  32, 342, 152, 402, 403, 404, 188, 160, 405, 119, 406,\n",
              "       181, 407, 408,  64,  27,  27, 169,  27,  27,  24, 409, 399, 336,\n",
              "       108, 325, 410, 411,  64, 412, 413, 289, 345, 328,  24, 414,  42,\n",
              "        32,  27, 343, 328, 415, 229, 416,  27, 187, 417,  32])"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hMD_KkJMP-3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ch6 . 게이트가 추가된 RNN \n"
      ],
      "metadata": {
        "id": "oq8DJkp9QTPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN의 문제 : 시간적으로 멀리 떨어진, 장기 의존 관계를 잘 학습할 수 없음\n",
        "# RNN대신 LSTM, GRU라는 계층이 주로 사용됨\n",
        "# LSTM,GRU는 GATE라는 구조가 더해져 있음\n",
        "# 이 게이트 덕분에 시계열 데이터의 장기 의존관계를 학습 가능\n",
        "# RNN의 큰 문제점 : 기울기 소실 또는 기울기 폭발"
      ],
      "metadata": {
        "id": "r40EiL2IQVxs"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "N= 2 # 미니배치 크기\n",
        "H = 3 # 은닉상태 벡터의 차원수\n",
        "T= 20 # 시계열 데이터의 길이\n",
        "dh = np.ones((N,H)) \n",
        "np.random.seed(3) \n",
        "Wh = np.random.randn(H,H) # 3X3\n",
        "norm_list = [] \n",
        "for t in range(T):\n",
        "  dh = np.matmul(dh,Wh.T) \n",
        "  norm = np.sqrt(np.sum(dh**2)) / N\n",
        "  norm_list.append(norm)"
      ],
      "metadata": {
        "id": "uB4yroSpSMy8"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "norm_list\n",
        "# 기울기 폭발이 일어남-> 오버플로우 -> nan값 발생"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCWHoErA8XjA",
        "outputId": "b4573b20-974e-4245-dc8c-eca3c92bb5aa"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.4684068094579303,\n",
              " 3.335704974161037,\n",
              " 4.783279375373183,\n",
              " 6.2795873320876145,\n",
              " 8.080776465019055,\n",
              " 10.25116303229294,\n",
              " 12.9360635066099,\n",
              " 16.276861327786712,\n",
              " 20.454829618345983,\n",
              " 25.688972842084684,\n",
              " 32.25315718048336,\n",
              " 40.48895641683869,\n",
              " 50.824407307019094,\n",
              " 63.79612654485427,\n",
              " 80.07737014308985,\n",
              " 100.51298922051251,\n",
              " 126.16331847536827,\n",
              " 158.3592064825883,\n",
              " 198.77107967611957,\n",
              " 249.495615421267]"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "N= 2 # 미니배치 크기\n",
        "H = 3 # 은닉상태 벡터의 차원수\n",
        "T= 20 # 시계열 데이터의 길이\n",
        "dh = np.ones((N,H)) \n",
        "np.random.seed(3) \n",
        "Wh = np.random.randn(H,H) * 0.5 # 3X3\n",
        "norm_list = [] \n",
        "for t in range(T):\n",
        "  dh = np.matmul(dh,Wh.T) \n",
        "  norm = np.sqrt(np.sum(dh**2)) / N\n",
        "  norm_list.append(norm)\n",
        "norm_list\n",
        "# 기울기 손실 -> 가중치 매개변수가 더이상 갱신 안됨 -> 장기 의존관계를 학습할수 없음"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tQIKZKx8bah",
        "outputId": "a7e3b8ae-c4b7-4e63-d2c5-fcb5e746eeec"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.2342034047289652,\n",
              " 0.8339262435402592,\n",
              " 0.5979099219216478,\n",
              " 0.3924742082554759,\n",
              " 0.25252426453184545,\n",
              " 0.16017442237957719,\n",
              " 0.10106299614538984,\n",
              " 0.06358148956166684,\n",
              " 0.039950839098332,\n",
              " 0.025086887541098325,\n",
              " 0.015748611904532892,\n",
              " 0.009884999125204758,\n",
              " 0.006204151282595104,\n",
              " 0.003893806551809953,\n",
              " 0.002443767399386287,\n",
              " 0.0015337065005571367,\n",
              " 0.0009625497320203268,\n",
              " 0.0006040924319556743,\n",
              " 0.00037912574706291117,\n",
              " 0.00023793756048323344]"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 기울기 소실/폭발  발생이유 : Wh를 T번 반복해서 곱했기 때문임"
      ],
      "metadata": {
        "id": "Wu3HRlgmCnLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 기울기 폭발 대책\n",
        "# 기울기 클리핑(Clipping)  : 문턱을 통과하면 기울기를 수정해줌\n",
        "dW1 = np.random.rand(3,3)*10 ;dW1 \n",
        "dW2 = np.random.rand(3,3)*10 ;dW2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27yTcmYMCePH",
        "outputId": "7e21ff3e-d1a1-450e-b540-1c8e04f028f2"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[9.75995422, 6.72383676, 9.02834109],\n",
              "       [8.45750871, 3.77994041, 0.92217009],\n",
              "       [6.53410903, 5.57840762, 3.61564763]])"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grads= [dW1,dW2] \n",
        "max_norm = 5\n",
        "def clip_grads(grads, max_norm): \n",
        "  total_norm = 0\n",
        "  for grad in grads : \n",
        "    total_norm += np.sum(grad**2) \n",
        "  \n",
        "  total_norm = np.sqrt(total_norm )\n",
        "\n",
        "  rate = max_norm / (total_norm +1e-6) \n",
        "\n",
        "  if rate < 1: \n",
        "    for grad in grads : \n",
        "      grad *= rate"
      ],
      "metadata": {
        "id": "yNf9rnNvEt8a"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 기울기 소실과 LSTM \n",
        "# 게이트가 추가된 RNN이 나오게된 배경\n",
        "# LSTM으로 기울기 소실 극복\n",
        "# RNN에 기억셀 C가 추가됨(LSTM전용의 기억 메커니즘)\n",
        "# 기억셀 : 데이터를 자기 자신으로만 주고받는것\n",
        "# LSTM게층 내에서만 완결되고 다른 계층으로는 출력하지 않는것\n",
        "# Ct : 시각 t에서의 LSTM의 기억이 저장, 과거로부터 시간 t까지에 필요한 모든 정보가 저장돼잇다고 가정함(그렇게 되도록 학습)\n",
        "# 필요한 정보를 모두 간직한 이 기억을 바탕으로 외부 계층에 은닉상태 h_{t}를 출력\n",
        "# 이때 출력하는 h_{t} 는 기억셀의 값을tanh함수로 변환한값\n",
        "# Ct : C_{t-1}, h_{t-1}, x_{t} 가 입력으로 들어감\n",
        "# 갱신된 Ct를 사용해 은닉상태 h_{t} 계산\n",
        "# LSTM의 게이트는 열닫 뿐만 아니라 어느정도 열지를 조절 가능 \n",
        "# 게이트를 얼마나 열까라는 것도 데이터로부터 자동으로 학습하게됨\n",
        "# tanh(Ct) 에 게이트를 적용하게됨 / tanh(Ct)의 각 원소들이 다음 은닉상태에 얼마나 중요한가를 조정함\n",
        "# 열림상태는 입력 x_{t}, h_{t-1}로부터 구함\n",
        "# forget게이트 : 기억셀에 무엇을 잊을까를 지시해야함\n",
        "# C_{t-1}에서 불필요한 기억을 잊게 해주는 게이트가 필요함(망각 게이트, f로 표현함)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "XedUbL7iGq0U",
        "outputId": "fa273ad0-2ae6-4853-ab59-1f387d23b13c"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-100-08e6cbab2f8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'grad' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qbch-xh1GrnC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}