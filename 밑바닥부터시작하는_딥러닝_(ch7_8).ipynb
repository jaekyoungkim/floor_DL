{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "밑바닥부터시작하는 딥러닝 (ch7-8).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "authorship_tag": "ABX9TyNU65LhgvilxmMej+X0eNho",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaekyoungkim/floor_DL/blob/main/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94_%EB%94%A5%EB%9F%AC%EB%8B%9D_(ch7_8).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIdfKiAob-F1",
        "outputId": "ee610690-f16d-499e-d0af-40b77af7f6f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'deeplearning_from_scratch'...\n",
            "remote: Enumerating objects: 405, done.\u001b[K\n",
            "remote: Total 405 (delta 0), reused 0 (delta 0), pack-reused 405\u001b[K\n",
            "Receiving objects: 100% (405/405), 58.06 MiB | 36.72 MiB/s, done.\n",
            "Resolving deltas: 100% (74/74), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/youbeebee/deeplearning_from_scratch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ba9DLVhfcB3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ch7. 합성곱 신경망 "
      ],
      "metadata": {
        "id": "RNRp05B0cHmx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 합성곱 계층과 풀링계층이 새롭게 등장\n",
        "# affine- relu - affine-relu -.... - softmax\n",
        "# conv - relu - pooling - conv - relu - pooling -... affine- relu- affine- softmax\n",
        "# CNN에서는 패딩, 스트라이드 등 CNN고유의 용어가 등장\n",
        "# 계층 사이에 3차원 데이터같이 입체적인 데이터가 흐른다는점에서 완전연결 신경망과 다름\n"
      ],
      "metadata": {
        "id": "FiE_YJm9cKvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 완전연결 계층의 문제점\n",
        "# 완전연결 계층에서는 인접하는 계층의 뉴런이 모두 연결되고 출력의 수는 임의로 정할 수 있음\n",
        "# 데이터의 형상이 무시되는 단점이 생김\n",
        "# 3차원 데이터를 1차원으로 평탄화 해주면서 발생하는 문제\n",
        "# CNN은 형상을 유지함, 이미지도 3차원 데이터로 입력받으며, 마찬가지로 다음 계층에도 3차원 데이터로 전달함\n",
        "# CNN에서는 계층의 입출력 데이터를 특징맵이라고 함\n",
        "# 합성곱 계층의 입력데이터를 입력 특징맵, 출력데이터를 출력 특징맵이라고 한다\n"
      ],
      "metadata": {
        "id": "kXvgdWjheKOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 패딩 : 입력데이터 주변을 특정값으로 채우기 , 공간적 크기를 고정한채로 다음계층에 전달할수있게 해줌\n",
        "# 스트라이드 : 필터를 적용하는 위치의 간격"
      ],
      "metadata": {
        "id": "XxYJU-Hpk-zU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  배치처리\n",
        "#  풀링계층 : 풀링의 윈도우 크기 = 스트라이드크기\n",
        "#  풀링의 특징 : 채널수가 변하지 않음, 입력의 변화에 영향을 적게 받음, 학습해야할 매개변수가 없다\n"
      ],
      "metadata": {
        "id": "Dg7dQatWd08j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 합성곱 플링 계층 구현하기\n",
        "# 4차원 배열\n",
        "import numpy as np\n",
        "x = np.random.rand(10,1, 28,28) #무작위로 데이터 생성\n",
        "x.shape\n",
        "x[0].shape\n",
        "x[1].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ny3shV6WdvLK",
        "outputId": "38f2f7d2-9b4e-45ff-dc09-bfccf66f23e0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[0,0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHGEjCOUhIW7",
        "outputId": "d5db324a-cab7-48b8-820b-633f5899917e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4 차원 접근이 복잡할것 같지만, im2col트릭이 문제를 단순하게 만들어줌(image to column의 약자 이미지에서 행렬로라는 의미)\n",
        "# 합성곱 연산을 곧이곧대로 구현하려면 for문을 겹겹이써야함\n",
        "# numpy에 for문을 사용하면 성능이 떨어진다는 단점도 있음\n",
        "# 입력데이터를 필터링 하기 좋게 전개하는 함수\n",
        "# 3차원 데이터에 im2col를 적용하면 2차원 행렬로 바뀜"
      ],
      "metadata": {
        "id": "4QTcS6T7hKg_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#im2col(input_data, filter_h, filter_w,  stride=1, pad= 0)\n",
        "# common/util.py참고\n",
        "# input_data : (데이터수, 채널수, 높이, 너비)의 4차원배열로 이루어짐\n"
      ],
      "metadata": {
        "id": "l5UA830shU5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, os\n",
        "sys.path.append(\"/content/deeplearning_from_scratch/\")\n",
        "from common.util import im2col"
      ],
      "metadata": {
        "id": "RJMoT4Y0j3qA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = np.random.rand(1,3,7,7)  # 배치크기 1(데이터1개), 채널3개, 높이 너비= 7 x 7\n",
        "col1= im2col(x1,5,5,stride=1,pad=0) # 두번쨰 차원의 원소는 75개(채널3, 필터크기5x5)\n",
        "print(col1.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHB2BXE_kL-D",
        "outputId": "6f0df2d3-2994-4ed1-b9a4-3b340928505f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9, 75)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "col1 # 3*3=9, 3*5*5=75"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKdff2_skWh-",
        "outputId": "86ccc79e-8a50-4360-d9a1-2832fa3b276a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.23931721e-01, 2.50419918e-01, 1.10036661e-01, 9.66465039e-01,\n",
              "        5.99934343e-01, 4.96488111e-01, 9.70485328e-01, 6.22639121e-01,\n",
              "        6.41823295e-04, 2.52628590e-01, 6.47995612e-02, 6.13451878e-01,\n",
              "        3.17308281e-01, 6.29205098e-01, 6.74424859e-01, 9.71586933e-01,\n",
              "        7.25060723e-01, 6.01281918e-01, 2.52865363e-01, 3.38751024e-01,\n",
              "        1.27112171e-02, 9.49780730e-01, 3.89230104e-01, 1.71918783e-01,\n",
              "        8.33151123e-01, 9.17889078e-01, 5.52914682e-01, 2.31869345e-01,\n",
              "        3.62503178e-01, 3.40423189e-01, 1.42976369e-01, 9.12469877e-01,\n",
              "        7.93308626e-02, 7.30035911e-01, 3.57133170e-01, 8.85373295e-01,\n",
              "        7.31330644e-01, 5.05775871e-01, 4.59809687e-01, 1.81127375e-01,\n",
              "        3.91634591e-01, 8.73668772e-01, 9.39103221e-01, 5.17795207e-01,\n",
              "        4.67631455e-02, 9.76765813e-01, 4.89711406e-01, 7.26308455e-01,\n",
              "        3.74951167e-01, 5.32568493e-01, 6.64912087e-01, 2.28195476e-01,\n",
              "        3.53732447e-01, 7.95247631e-01, 7.73474868e-01, 9.46827610e-01,\n",
              "        4.91306244e-01, 5.25145391e-03, 1.90544789e-01, 2.43508120e-01,\n",
              "        9.65811815e-01, 6.49138961e-01, 2.70849534e-01, 2.86991114e-01,\n",
              "        8.72459012e-01, 1.53271291e-01, 3.66078957e-01, 7.98461472e-01,\n",
              "        1.74894490e-01, 7.86436698e-01, 5.27664827e-02, 1.83090363e-01,\n",
              "        5.18934169e-01, 6.69322765e-01, 6.33949464e-01],\n",
              "       [2.50419918e-01, 1.10036661e-01, 9.66465039e-01, 5.99934343e-01,\n",
              "        9.37710460e-01, 9.70485328e-01, 6.22639121e-01, 6.41823295e-04,\n",
              "        2.52628590e-01, 7.92513385e-01, 6.13451878e-01, 3.17308281e-01,\n",
              "        6.29205098e-01, 6.74424859e-01, 9.51608899e-01, 7.25060723e-01,\n",
              "        6.01281918e-01, 2.52865363e-01, 3.38751024e-01, 6.38543338e-01,\n",
              "        9.49780730e-01, 3.89230104e-01, 1.71918783e-01, 8.33151123e-01,\n",
              "        1.70530762e-01, 5.52914682e-01, 2.31869345e-01, 3.62503178e-01,\n",
              "        3.40423189e-01, 1.07787349e-01, 9.12469877e-01, 7.93308626e-02,\n",
              "        7.30035911e-01, 3.57133170e-01, 1.24911972e-01, 7.31330644e-01,\n",
              "        5.05775871e-01, 4.59809687e-01, 1.81127375e-01, 7.73500316e-01,\n",
              "        8.73668772e-01, 9.39103221e-01, 5.17795207e-01, 4.67631455e-02,\n",
              "        9.41804551e-01, 4.89711406e-01, 7.26308455e-01, 3.74951167e-01,\n",
              "        5.32568493e-01, 9.84634208e-01, 2.28195476e-01, 3.53732447e-01,\n",
              "        7.95247631e-01, 7.73474868e-01, 6.02103061e-01, 4.91306244e-01,\n",
              "        5.25145391e-03, 1.90544789e-01, 2.43508120e-01, 8.77616915e-01,\n",
              "        6.49138961e-01, 2.70849534e-01, 2.86991114e-01, 8.72459012e-01,\n",
              "        6.95852323e-01, 3.66078957e-01, 7.98461472e-01, 1.74894490e-01,\n",
              "        7.86436698e-01, 8.01196077e-01, 1.83090363e-01, 5.18934169e-01,\n",
              "        6.69322765e-01, 6.33949464e-01, 3.56520969e-01],\n",
              "       [1.10036661e-01, 9.66465039e-01, 5.99934343e-01, 9.37710460e-01,\n",
              "        9.47336883e-01, 6.22639121e-01, 6.41823295e-04, 2.52628590e-01,\n",
              "        7.92513385e-01, 1.05106454e-01, 3.17308281e-01, 6.29205098e-01,\n",
              "        6.74424859e-01, 9.51608899e-01, 2.40537605e-01, 6.01281918e-01,\n",
              "        2.52865363e-01, 3.38751024e-01, 6.38543338e-01, 3.03354500e-01,\n",
              "        3.89230104e-01, 1.71918783e-01, 8.33151123e-01, 1.70530762e-01,\n",
              "        7.52820226e-02, 2.31869345e-01, 3.62503178e-01, 3.40423189e-01,\n",
              "        1.07787349e-01, 2.50053852e-01, 7.93308626e-02, 7.30035911e-01,\n",
              "        3.57133170e-01, 1.24911972e-01, 1.21333385e-01, 5.05775871e-01,\n",
              "        4.59809687e-01, 1.81127375e-01, 7.73500316e-01, 6.21625076e-01,\n",
              "        9.39103221e-01, 5.17795207e-01, 4.67631455e-02, 9.41804551e-01,\n",
              "        9.31027501e-01, 7.26308455e-01, 3.74951167e-01, 5.32568493e-01,\n",
              "        9.84634208e-01, 9.13390713e-01, 3.53732447e-01, 7.95247631e-01,\n",
              "        7.73474868e-01, 6.02103061e-01, 6.48167982e-01, 5.25145391e-03,\n",
              "        1.90544789e-01, 2.43508120e-01, 8.77616915e-01, 7.49031872e-01,\n",
              "        2.70849534e-01, 2.86991114e-01, 8.72459012e-01, 6.95852323e-01,\n",
              "        7.56833655e-02, 7.98461472e-01, 1.74894490e-01, 7.86436698e-01,\n",
              "        8.01196077e-01, 7.53984617e-01, 5.18934169e-01, 6.69322765e-01,\n",
              "        6.33949464e-01, 3.56520969e-01, 8.79156649e-01],\n",
              "       [4.96488111e-01, 9.70485328e-01, 6.22639121e-01, 6.41823295e-04,\n",
              "        2.52628590e-01, 6.47995612e-02, 6.13451878e-01, 3.17308281e-01,\n",
              "        6.29205098e-01, 6.74424859e-01, 9.71586933e-01, 7.25060723e-01,\n",
              "        6.01281918e-01, 2.52865363e-01, 3.38751024e-01, 1.27112171e-02,\n",
              "        9.49780730e-01, 3.89230104e-01, 1.71918783e-01, 8.33151123e-01,\n",
              "        3.30776088e-01, 8.02441572e-01, 4.83774127e-01, 5.16059245e-01,\n",
              "        2.79885872e-01, 1.42976369e-01, 9.12469877e-01, 7.93308626e-02,\n",
              "        7.30035911e-01, 3.57133170e-01, 8.85373295e-01, 7.31330644e-01,\n",
              "        5.05775871e-01, 4.59809687e-01, 1.81127375e-01, 3.91634591e-01,\n",
              "        8.73668772e-01, 9.39103221e-01, 5.17795207e-01, 4.67631455e-02,\n",
              "        9.76765813e-01, 4.89711406e-01, 7.26308455e-01, 3.74951167e-01,\n",
              "        5.32568493e-01, 4.34237256e-01, 9.33938302e-01, 6.11752853e-01,\n",
              "        6.97903653e-01, 1.24134798e-01, 9.46827610e-01, 4.91306244e-01,\n",
              "        5.25145391e-03, 1.90544789e-01, 2.43508120e-01, 9.65811815e-01,\n",
              "        6.49138961e-01, 2.70849534e-01, 2.86991114e-01, 8.72459012e-01,\n",
              "        1.53271291e-01, 3.66078957e-01, 7.98461472e-01, 1.74894490e-01,\n",
              "        7.86436698e-01, 5.27664827e-02, 1.83090363e-01, 5.18934169e-01,\n",
              "        6.69322765e-01, 6.33949464e-01, 5.71542469e-01, 5.99423688e-01,\n",
              "        1.98300550e-01, 4.79825649e-01, 2.09547773e-01],\n",
              "       [9.70485328e-01, 6.22639121e-01, 6.41823295e-04, 2.52628590e-01,\n",
              "        7.92513385e-01, 6.13451878e-01, 3.17308281e-01, 6.29205098e-01,\n",
              "        6.74424859e-01, 9.51608899e-01, 7.25060723e-01, 6.01281918e-01,\n",
              "        2.52865363e-01, 3.38751024e-01, 6.38543338e-01, 9.49780730e-01,\n",
              "        3.89230104e-01, 1.71918783e-01, 8.33151123e-01, 1.70530762e-01,\n",
              "        8.02441572e-01, 4.83774127e-01, 5.16059245e-01, 2.79885872e-01,\n",
              "        5.97070888e-02, 9.12469877e-01, 7.93308626e-02, 7.30035911e-01,\n",
              "        3.57133170e-01, 1.24911972e-01, 7.31330644e-01, 5.05775871e-01,\n",
              "        4.59809687e-01, 1.81127375e-01, 7.73500316e-01, 8.73668772e-01,\n",
              "        9.39103221e-01, 5.17795207e-01, 4.67631455e-02, 9.41804551e-01,\n",
              "        4.89711406e-01, 7.26308455e-01, 3.74951167e-01, 5.32568493e-01,\n",
              "        9.84634208e-01, 9.33938302e-01, 6.11752853e-01, 6.97903653e-01,\n",
              "        1.24134798e-01, 3.23882256e-01, 4.91306244e-01, 5.25145391e-03,\n",
              "        1.90544789e-01, 2.43508120e-01, 8.77616915e-01, 6.49138961e-01,\n",
              "        2.70849534e-01, 2.86991114e-01, 8.72459012e-01, 6.95852323e-01,\n",
              "        3.66078957e-01, 7.98461472e-01, 1.74894490e-01, 7.86436698e-01,\n",
              "        8.01196077e-01, 1.83090363e-01, 5.18934169e-01, 6.69322765e-01,\n",
              "        6.33949464e-01, 3.56520969e-01, 5.99423688e-01, 1.98300550e-01,\n",
              "        4.79825649e-01, 2.09547773e-01, 2.78933598e-01],\n",
              "       [6.22639121e-01, 6.41823295e-04, 2.52628590e-01, 7.92513385e-01,\n",
              "        1.05106454e-01, 3.17308281e-01, 6.29205098e-01, 6.74424859e-01,\n",
              "        9.51608899e-01, 2.40537605e-01, 6.01281918e-01, 2.52865363e-01,\n",
              "        3.38751024e-01, 6.38543338e-01, 3.03354500e-01, 3.89230104e-01,\n",
              "        1.71918783e-01, 8.33151123e-01, 1.70530762e-01, 7.52820226e-02,\n",
              "        4.83774127e-01, 5.16059245e-01, 2.79885872e-01, 5.97070888e-02,\n",
              "        5.80694126e-01, 7.93308626e-02, 7.30035911e-01, 3.57133170e-01,\n",
              "        1.24911972e-01, 1.21333385e-01, 5.05775871e-01, 4.59809687e-01,\n",
              "        1.81127375e-01, 7.73500316e-01, 6.21625076e-01, 9.39103221e-01,\n",
              "        5.17795207e-01, 4.67631455e-02, 9.41804551e-01, 9.31027501e-01,\n",
              "        7.26308455e-01, 3.74951167e-01, 5.32568493e-01, 9.84634208e-01,\n",
              "        9.13390713e-01, 6.11752853e-01, 6.97903653e-01, 1.24134798e-01,\n",
              "        3.23882256e-01, 2.59615846e-01, 5.25145391e-03, 1.90544789e-01,\n",
              "        2.43508120e-01, 8.77616915e-01, 7.49031872e-01, 2.70849534e-01,\n",
              "        2.86991114e-01, 8.72459012e-01, 6.95852323e-01, 7.56833655e-02,\n",
              "        7.98461472e-01, 1.74894490e-01, 7.86436698e-01, 8.01196077e-01,\n",
              "        7.53984617e-01, 5.18934169e-01, 6.69322765e-01, 6.33949464e-01,\n",
              "        3.56520969e-01, 8.79156649e-01, 1.98300550e-01, 4.79825649e-01,\n",
              "        2.09547773e-01, 2.78933598e-01, 7.61387680e-01],\n",
              "       [6.47995612e-02, 6.13451878e-01, 3.17308281e-01, 6.29205098e-01,\n",
              "        6.74424859e-01, 9.71586933e-01, 7.25060723e-01, 6.01281918e-01,\n",
              "        2.52865363e-01, 3.38751024e-01, 1.27112171e-02, 9.49780730e-01,\n",
              "        3.89230104e-01, 1.71918783e-01, 8.33151123e-01, 3.30776088e-01,\n",
              "        8.02441572e-01, 4.83774127e-01, 5.16059245e-01, 2.79885872e-01,\n",
              "        8.51348173e-01, 7.92365791e-01, 5.52411799e-02, 3.20661098e-01,\n",
              "        1.11299185e-01, 8.85373295e-01, 7.31330644e-01, 5.05775871e-01,\n",
              "        4.59809687e-01, 1.81127375e-01, 3.91634591e-01, 8.73668772e-01,\n",
              "        9.39103221e-01, 5.17795207e-01, 4.67631455e-02, 9.76765813e-01,\n",
              "        4.89711406e-01, 7.26308455e-01, 3.74951167e-01, 5.32568493e-01,\n",
              "        4.34237256e-01, 9.33938302e-01, 6.11752853e-01, 6.97903653e-01,\n",
              "        1.24134798e-01, 4.63123765e-01, 9.61360674e-01, 5.97597153e-01,\n",
              "        9.77522430e-01, 7.95114233e-02, 9.65811815e-01, 6.49138961e-01,\n",
              "        2.70849534e-01, 2.86991114e-01, 8.72459012e-01, 1.53271291e-01,\n",
              "        3.66078957e-01, 7.98461472e-01, 1.74894490e-01, 7.86436698e-01,\n",
              "        5.27664827e-02, 1.83090363e-01, 5.18934169e-01, 6.69322765e-01,\n",
              "        6.33949464e-01, 5.71542469e-01, 5.99423688e-01, 1.98300550e-01,\n",
              "        4.79825649e-01, 2.09547773e-01, 9.68920911e-01, 3.68654840e-01,\n",
              "        5.83694101e-01, 4.47627526e-02, 8.15719192e-01],\n",
              "       [6.13451878e-01, 3.17308281e-01, 6.29205098e-01, 6.74424859e-01,\n",
              "        9.51608899e-01, 7.25060723e-01, 6.01281918e-01, 2.52865363e-01,\n",
              "        3.38751024e-01, 6.38543338e-01, 9.49780730e-01, 3.89230104e-01,\n",
              "        1.71918783e-01, 8.33151123e-01, 1.70530762e-01, 8.02441572e-01,\n",
              "        4.83774127e-01, 5.16059245e-01, 2.79885872e-01, 5.97070888e-02,\n",
              "        7.92365791e-01, 5.52411799e-02, 3.20661098e-01, 1.11299185e-01,\n",
              "        2.51197787e-01, 7.31330644e-01, 5.05775871e-01, 4.59809687e-01,\n",
              "        1.81127375e-01, 7.73500316e-01, 8.73668772e-01, 9.39103221e-01,\n",
              "        5.17795207e-01, 4.67631455e-02, 9.41804551e-01, 4.89711406e-01,\n",
              "        7.26308455e-01, 3.74951167e-01, 5.32568493e-01, 9.84634208e-01,\n",
              "        9.33938302e-01, 6.11752853e-01, 6.97903653e-01, 1.24134798e-01,\n",
              "        3.23882256e-01, 9.61360674e-01, 5.97597153e-01, 9.77522430e-01,\n",
              "        7.95114233e-02, 1.25620894e-01, 6.49138961e-01, 2.70849534e-01,\n",
              "        2.86991114e-01, 8.72459012e-01, 6.95852323e-01, 3.66078957e-01,\n",
              "        7.98461472e-01, 1.74894490e-01, 7.86436698e-01, 8.01196077e-01,\n",
              "        1.83090363e-01, 5.18934169e-01, 6.69322765e-01, 6.33949464e-01,\n",
              "        3.56520969e-01, 5.99423688e-01, 1.98300550e-01, 4.79825649e-01,\n",
              "        2.09547773e-01, 2.78933598e-01, 3.68654840e-01, 5.83694101e-01,\n",
              "        4.47627526e-02, 8.15719192e-01, 1.08533638e-02],\n",
              "       [3.17308281e-01, 6.29205098e-01, 6.74424859e-01, 9.51608899e-01,\n",
              "        2.40537605e-01, 6.01281918e-01, 2.52865363e-01, 3.38751024e-01,\n",
              "        6.38543338e-01, 3.03354500e-01, 3.89230104e-01, 1.71918783e-01,\n",
              "        8.33151123e-01, 1.70530762e-01, 7.52820226e-02, 4.83774127e-01,\n",
              "        5.16059245e-01, 2.79885872e-01, 5.97070888e-02, 5.80694126e-01,\n",
              "        5.52411799e-02, 3.20661098e-01, 1.11299185e-01, 2.51197787e-01,\n",
              "        4.83355022e-01, 5.05775871e-01, 4.59809687e-01, 1.81127375e-01,\n",
              "        7.73500316e-01, 6.21625076e-01, 9.39103221e-01, 5.17795207e-01,\n",
              "        4.67631455e-02, 9.41804551e-01, 9.31027501e-01, 7.26308455e-01,\n",
              "        3.74951167e-01, 5.32568493e-01, 9.84634208e-01, 9.13390713e-01,\n",
              "        6.11752853e-01, 6.97903653e-01, 1.24134798e-01, 3.23882256e-01,\n",
              "        2.59615846e-01, 5.97597153e-01, 9.77522430e-01, 7.95114233e-02,\n",
              "        1.25620894e-01, 1.39537529e-01, 2.70849534e-01, 2.86991114e-01,\n",
              "        8.72459012e-01, 6.95852323e-01, 7.56833655e-02, 7.98461472e-01,\n",
              "        1.74894490e-01, 7.86436698e-01, 8.01196077e-01, 7.53984617e-01,\n",
              "        5.18934169e-01, 6.69322765e-01, 6.33949464e-01, 3.56520969e-01,\n",
              "        8.79156649e-01, 1.98300550e-01, 4.79825649e-01, 2.09547773e-01,\n",
              "        2.78933598e-01, 7.61387680e-01, 5.83694101e-01, 4.47627526e-02,\n",
              "        8.15719192e-01, 1.08533638e-02, 4.38925748e-02]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x2=np.random.rand(10,3,7,7)\n",
        "col2=im2col(x2,5,5,stride=1, pad=0)\n",
        "print(col2.shape)  # 10*3*3 = 90, 3*5*5 = 75"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feYb9o2rkifL",
        "outputId": "e6efa1f1-660e-4e02-99b5-dbc6a2409126"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(90, 75)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#im2col을 사용하여 합성곱 계층 구현\n",
        "class Convolution:\n",
        "  def __init__(self, W,b, stride=1, pad=0):\n",
        "    self.W = W\n",
        "    self.b = b\n",
        "    self.stride = stride\n",
        "    self.pad = pad\n",
        "  \n",
        "  def forward(self,x):\n",
        "    FN, C, FH,FW = self.W.shape  # 필터의 형상, 4차원, FN필터개수, C채널, FH필터높이, FW필터너비\n",
        "    N, C, H,W = x.shape\n",
        "    out_h = int(1+ (H+2*self.pad - FH) / self.stride)\n",
        "    out_w = int(1+ (W+2*self.pad - FW) / self.stride)\n",
        "\n",
        "    col = im2col(x,FH,FW, self.stride, self.pad)\n",
        "    col_W = self.W.shape(FN, -1).T\n",
        "    out = np.dot(col, col_w) + self.b\n",
        "\n",
        "    out = out.reshape(N,out_h, out_w, -1).transpose(0,3,1,2) # N, H, W, C -> N,C,H,W\n",
        "    return out"
      ],
      "metadata": {
        "id": "dQ7AZzcFku79"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 역전파에서는 IM2COL를 역으로 처리, col2im함수를 사용하면됨"
      ],
      "metadata": {
        "id": "2JoysOTQkYix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 풀링계층구현하기\n",
        "# im2col를 사용해서 처리가능, but풀링의 경우, 채널쪽이 독립적이라는점이 합성곱 계층때와 다름\n",
        "class Pooling:\n",
        "  def __init__(self, pool_h, pool_w, stride=1, pad= 0):\n",
        "    self.pool_h = pool_h\n",
        "    self.pool_w = pool_w\n",
        "    self.stride= stride\n",
        "    self.pad = pad\n",
        "  \n",
        "  def forward(self,x):\n",
        "    N,C,H,W = x.shape\n",
        "    out_h = int(1+ (H-self.pool_h) / self.stride)\n",
        "    out_w = int(1+ (W-self.pool_w) / self.stride)\n",
        "\n",
        "    # 전개(1)\n",
        "    col = im2col(x, self.pool_h, self.pool_w, self.stride,  self.pad)\n",
        "    col = col.reshape(-1, self.pool_h * self.pool_w)\n",
        "\n",
        "    # 최댓값(2)\n",
        "    out = np.max(col, axis=1)  # max pooling\n",
        "\n",
        "    # 성형(3)\n",
        "    out = out.reshape(N, out_h, out_w, C).transpose(0,3,1,2)\n",
        "    return out"
      ],
      "metadata": {
        "id": "mRLgns6Fos3N"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 풀링 계층 구현은 다음과 같이 세단계로 진행됨\n",
        "# 1. 입력데이터를 전개\n",
        "# 2. 행별최댓값을 구함 ex. np.max(x, axis=1) : 입력 x의 1번째 차원의 축마다 최댓값을 구함\n",
        "# 3. 적절한 모양으로 성형"
      ],
      "metadata": {
        "id": "sLTY-JuE7iqo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import OrderedDict\n",
        "# CNN구현하기\n",
        "# 이미지 > CONV < RELU > POOLING > AFFINE > RELU > AFFINE > SOFTMAX\n",
        "# Input dim : 입력데이터의 차원(채널수, 높이, 너비)\n",
        "# conv_param : 합성곱 계층의 하이퍼파라미터(딕셔너리)\n",
        "# - filter_num : 필터수\n",
        "# - filter_size : 필터크기\n",
        "# - stride : 스트라이드\n",
        "# - pad : 패딩\n",
        "# - hidden_size : 은닉층의 뉴런수\n",
        "# - output_size : 출력층의 뉴런수\n",
        "# - weight_int_std : 초기화때의 가중치 표준편차\n",
        "\n",
        "class SimpleConvNet:\n",
        "  def __init__(self, input_dim = (1,28,28), conv_param = {\"filter_num\":30 , \"filter_size\": 5, \"pad\": 0 , \"stride\": 1}, hidden_size= 100, output_size= 10, weight_init_std = 0.01):\n",
        "    filter_num = conv_param['filter_num']\n",
        "    filter_size = conv_param['filter_size']\n",
        "    filter_pad = conv_param['pad']\n",
        "    filter_stride = conv_param['stride']\n",
        "    input_size = input_dim[1]\n",
        "    conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
        "    pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
        "    self_params = {}\n",
        "    self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
        "    self.params['b1'] = np.zeros(filter_num)\n",
        "    self.params['W2'] = weight_init_std * np.random.randn(pool_output_size, hidden_size)\n",
        "    self.params['b2'] = np.zeros(hidden_size)\n",
        "    self.params['W3'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "    self.params['b3'] = np.zeros(output_size)\n",
        "    # cnn을 구성하는 계층들을 생성하기\n",
        "    self.layers = OrderedDict()\n",
        "    self.layers['CONV1'] = Convolution(self.params['W1'], self.params['b1'], conv_param['stride'], conv_param['pad'])\n",
        "    self.layers['Relu1'] = Relu()\n",
        "    self.layers['Pool1'] = Pooling(pool_h = 2, pool_w = 2, stride= 2)\n",
        "    self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
        "    self.layers['Relu2'] = Relu()\n",
        "    self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
        "    self.lasy_layer = SoftmaxWithLoss()\n",
        "  \n",
        "  # predict method는 초기화때 layers에 추가한 계층을 맨 앞에서부터 차례로 forward메서드를 호출하며 그 결과를 다음 계층에 전달함\n",
        "  def predict(self, x):\n",
        "    for layer in self.layers.values():\n",
        "      x = layer.forward(x)\n",
        "      return x\n",
        "\n",
        "  def loss(self, x, t):  # x는 입력데이터, t는 정답 레이블\n",
        "    y= self.predict(x)\n",
        "    return self.last_layer.forward(y,t)\n",
        "  \n",
        "  def gradient(self,x,t):\n",
        "    #순전파\n",
        "    self.loss(x,t)\n",
        "\n",
        "    #역전파\n",
        "    dout = 1\n",
        "    dout = self.last_layer.backward(dout)\n",
        "\n",
        "    layers = list(self.layers.values())\n",
        "    layers.reverse()\n",
        "    for layer in layers:\n",
        "      dout = layer.backward(dout)\n",
        "\n",
        "    #결과저장\n",
        "    grads = {}\n",
        "    grads['W1'] = self.layers['Conv1'].dW\n",
        "    grads['b1'] = self.layers['Conv1'].db\n",
        "    grads['W2'] = self.layers['Affine1'].dW\n",
        "    grads['b2'] = self.layers['Affine1'].db\n",
        "    grads['W3'] = self.layers['Affine2'].dW\n",
        "    grads['b3'] = self.layers['Affine2'].db\n",
        "    \n",
        "    return grads\n",
        "# 매개변수의 기울기는 오차역전파법으로 구함\n",
        "# 순전파와 역전파를 반복수행\n",
        "# grads라는 딕셔너리 변수에 각 가중치 매개변수의 기울기를 저장함\n"
      ],
      "metadata": {
        "id": "sqzhVJKq7iuD"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# simpleconvnet을 mnist데이터셋으로 학습\n",
        "# CH7 TRAIN_CONVET.PY참고\n",
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
        "sys.path.append('/content/deeplearning_from_scratch/ch7.CNN')\n"
      ],
      "metadata": {
        "id": "gueUz4hBIepf"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "from simple_convnet import SimpleConvNet\n",
        "from common.trainer import Trainer\n",
        "\n",
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
        "\n",
        "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
        "#x_train, t_train = x_train[:5000], t_train[:5000]\n",
        "#x_test, t_test = x_test[:1000], t_test[:1000]\n",
        "\n",
        "max_epochs = 20\n",
        "\n",
        "network = SimpleConvNet(input_dim=(1,28,28), \n",
        "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
        "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
        "                        \n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                  epochs=max_epochs, mini_batch_size=100,\n",
        "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
        "                  evaluate_sample_num_per_epoch=1000)\n",
        "trainer.train()\n",
        "\n",
        "# 매개변수 보존\n",
        "network.save_params(\"params.pkl\")\n",
        "print(\"Saved Network Parameters!\")\n",
        "\n",
        "# 그래프 그리기\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
        "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ll2v6HbOL_r-",
        "outputId": "fc143146-9f5d-4578-efe5-3b1175429fde"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "train loss:0.019392444394995066\n",
            "train loss:0.004135690340614343\n",
            "train loss:0.009324616031519939\n",
            "train loss:0.005744985830035898\n",
            "train loss:0.003328392330623366\n",
            "train loss:0.013066499643976154\n",
            "train loss:0.0014176984612452792\n",
            "train loss:0.006220194147391345\n",
            "train loss:0.010332250803650229\n",
            "train loss:0.00503429986504592\n",
            "train loss:0.0036228165880035352\n",
            "train loss:0.021936387409033253\n",
            "train loss:0.0021173853119107046\n",
            "train loss:0.0016195963101028584\n",
            "train loss:0.001988216000458889\n",
            "train loss:0.013662284671417693\n",
            "train loss:0.008604104264543687\n",
            "train loss:0.004307149009277844\n",
            "train loss:0.0026697955690706244\n",
            "train loss:0.0018901915509700726\n",
            "train loss:0.0058089991624926725\n",
            "train loss:0.004438599550405119\n",
            "train loss:0.003709305450591388\n",
            "train loss:0.002513922819429707\n",
            "train loss:0.014919442697811917\n",
            "train loss:0.00613640114963925\n",
            "train loss:0.0009033960041952912\n",
            "train loss:0.009678617948239251\n",
            "train loss:0.0005986182924849371\n",
            "train loss:0.0024998861124454846\n",
            "train loss:0.0038222296402050317\n",
            "train loss:0.0024073561610626847\n",
            "train loss:0.0009954861528199042\n",
            "train loss:0.014460664113425439\n",
            "train loss:0.008841894762570924\n",
            "train loss:0.0029333349641538966\n",
            "train loss:0.0024036586317300905\n",
            "train loss:0.01352770854282993\n",
            "train loss:0.0017666484117509088\n",
            "train loss:0.0005198401072880547\n",
            "train loss:0.007823556433655537\n",
            "train loss:0.0013351975537715692\n",
            "train loss:0.0246861063997062\n",
            "train loss:0.00017994055816367944\n",
            "train loss:0.0007294472341409494\n",
            "train loss:0.0018660344274982466\n",
            "train loss:0.004002337810779091\n",
            "train loss:0.006372449419681185\n",
            "train loss:0.009428850636681935\n",
            "train loss:0.004465292468362729\n",
            "train loss:0.008322507646516061\n",
            "train loss:0.004870161943414657\n",
            "train loss:0.005581455842200228\n",
            "train loss:0.0102103262696802\n",
            "train loss:0.002570117594351482\n",
            "train loss:0.05148874310278714\n",
            "train loss:0.00337091425847036\n",
            "train loss:0.0031463598790904606\n",
            "train loss:0.00045938834107508433\n",
            "train loss:0.0004446903473205943\n",
            "train loss:0.003269243886473134\n",
            "train loss:0.0096637123463216\n",
            "train loss:0.00359306601014496\n",
            "train loss:0.0007602014425675116\n",
            "train loss:0.006939348209632136\n",
            "train loss:0.003642281026869088\n",
            "train loss:0.005165646648289882\n",
            "train loss:0.0003447461447152203\n",
            "train loss:0.0008033181287591944\n",
            "train loss:0.0016411638385054614\n",
            "train loss:0.0033556902648656083\n",
            "train loss:0.0022073385042846653\n",
            "train loss:0.04215801938027578\n",
            "train loss:0.006599326646450839\n",
            "train loss:0.0035811487844606515\n",
            "train loss:0.01957771966111332\n",
            "train loss:0.0011092922645498445\n",
            "train loss:0.0007561010187153004\n",
            "train loss:0.0023561573696313626\n",
            "train loss:0.0004879140701754295\n",
            "train loss:0.0012410954421873367\n",
            "train loss:0.0017924416265786317\n",
            "train loss:0.001022768033140729\n",
            "train loss:0.0069589468943418555\n",
            "train loss:0.00906758669434807\n",
            "train loss:0.0011123401476494286\n",
            "train loss:0.011183579009545133\n",
            "train loss:0.007050430517740524\n",
            "train loss:0.0008434929630955246\n",
            "train loss:0.0024269942629373707\n",
            "train loss:0.0013269206312842403\n",
            "train loss:0.00034791600137209377\n",
            "train loss:0.0023188168684018682\n",
            "train loss:0.008998277308434497\n",
            "train loss:0.012177741003298888\n",
            "train loss:0.0004841328034303306\n",
            "train loss:0.004545773907273315\n",
            "train loss:0.00047054661713407444\n",
            "train loss:0.0016184571028048246\n",
            "train loss:0.000200454428805288\n",
            "train loss:0.012033664848600667\n",
            "train loss:0.0010495344556750328\n",
            "train loss:0.00039531067124000543\n",
            "train loss:0.0007321710721229399\n",
            "train loss:0.000300459359079378\n",
            "train loss:0.0023334348613780315\n",
            "train loss:0.0004480937585444922\n",
            "train loss:0.0001950818073219505\n",
            "train loss:0.0013232144765510073\n",
            "train loss:0.008184581320557698\n",
            "train loss:0.0010610313547080254\n",
            "train loss:0.008785611732917219\n",
            "train loss:0.0037640786697507008\n",
            "train loss:0.0018872800145294905\n",
            "train loss:0.0008694527400880497\n",
            "train loss:0.006960667691100469\n",
            "train loss:0.005806156047921615\n",
            "train loss:0.005567988749387517\n",
            "train loss:0.0007932614966978072\n",
            "train loss:0.003485782967775196\n",
            "train loss:0.001502871793906139\n",
            "train loss:0.02396618914155761\n",
            "train loss:0.004856438505954846\n",
            "train loss:0.0037482379832842667\n",
            "train loss:0.003911329085232668\n",
            "train loss:0.0055018234795332485\n",
            "train loss:0.006250373108562817\n",
            "train loss:0.006048477475012182\n",
            "train loss:0.009909780062427312\n",
            "train loss:0.0010739293782165923\n",
            "train loss:0.0019401410050766078\n",
            "train loss:7.273234222432689e-05\n",
            "train loss:0.00908503411353221\n",
            "train loss:0.014048954673394325\n",
            "train loss:0.0028259675774914682\n",
            "train loss:0.0027132385111043058\n",
            "train loss:0.009817258642508077\n",
            "train loss:0.010033792686043\n",
            "train loss:0.0022569520247597875\n",
            "train loss:0.005407776166902046\n",
            "train loss:0.0003168769324879591\n",
            "train loss:0.004169123315646002\n",
            "train loss:0.0011797560058051048\n",
            "train loss:0.0028434885818501153\n",
            "train loss:0.004785328509908563\n",
            "train loss:0.0019534684095419046\n",
            "train loss:0.002155830584129137\n",
            "train loss:0.004431277565960469\n",
            "train loss:0.007347564263056343\n",
            "train loss:0.00020401034427711685\n",
            "train loss:0.01552622524538981\n",
            "train loss:0.001826900229925808\n",
            "train loss:0.003916025152313566\n",
            "train loss:0.0004828041380085429\n",
            "train loss:0.003215967455162428\n",
            "train loss:0.0002658671864463314\n",
            "train loss:0.00727814745452815\n",
            "train loss:0.0008005494664582952\n",
            "train loss:0.0014406844110130285\n",
            "train loss:0.003398756172131518\n",
            "train loss:0.0014223789964120245\n",
            "train loss:0.000694224730033835\n",
            "train loss:0.007792923219818615\n",
            "train loss:0.018658273374118774\n",
            "train loss:0.0027707608116745355\n",
            "train loss:0.0013003427922085827\n",
            "train loss:0.00034363895635313834\n",
            "train loss:0.0021617502403674933\n",
            "train loss:0.0004103917271143199\n",
            "train loss:0.02757579812125635\n",
            "train loss:0.0007648642612899889\n",
            "train loss:0.016119747054886648\n",
            "train loss:0.0016232288002456857\n",
            "train loss:0.004495672050405362\n",
            "train loss:0.003081239578836191\n",
            "train loss:0.0009552993555362847\n",
            "train loss:0.017402328920623485\n",
            "train loss:0.00018017034401390024\n",
            "train loss:0.001843917538111518\n",
            "train loss:0.0023868151502387394\n",
            "train loss:0.003142725361198104\n",
            "train loss:0.003421989431497227\n",
            "train loss:0.0641792112401991\n",
            "train loss:0.0022231291887639046\n",
            "train loss:0.0011655360079516224\n",
            "train loss:0.0024798678840120187\n",
            "train loss:0.0008863865447441419\n",
            "train loss:0.00044131692412064034\n",
            "train loss:0.006006605890527012\n",
            "train loss:0.0017542521938282074\n",
            "=== epoch:13, train acc:0.995, test acc:0.985 ===\n",
            "train loss:0.005927899242538979\n",
            "train loss:0.003392764650476038\n",
            "train loss:0.04211995023785555\n",
            "train loss:0.0012222400355949134\n",
            "train loss:0.0022242199624086033\n",
            "train loss:0.0024564056492204353\n",
            "train loss:0.0037332863966004037\n",
            "train loss:0.0014754437138457196\n",
            "train loss:0.0015683530844054563\n",
            "train loss:0.006597707171485888\n",
            "train loss:0.0023207750754788503\n",
            "train loss:0.004199810123116201\n",
            "train loss:0.00351298165754586\n",
            "train loss:0.002501755971104217\n",
            "train loss:0.003573823675637655\n",
            "train loss:0.0026889796666660186\n",
            "train loss:0.009520651841076571\n",
            "train loss:0.0005890883237035038\n",
            "train loss:0.00018005042389808722\n",
            "train loss:0.0030118588254232\n",
            "train loss:0.0030433811792980035\n",
            "train loss:0.005675987805429529\n",
            "train loss:0.00406257592480554\n",
            "train loss:0.0012099414949605673\n",
            "train loss:0.0016720382913068908\n",
            "train loss:0.012324200074557107\n",
            "train loss:0.0004509428373859462\n",
            "train loss:0.00023984872967268062\n",
            "train loss:0.0014050611931334112\n",
            "train loss:0.0015891979969112047\n",
            "train loss:0.00204368906041957\n",
            "train loss:0.002387865033932431\n",
            "train loss:0.007446970671418928\n",
            "train loss:0.003610386580919427\n",
            "train loss:0.0015593964816585345\n",
            "train loss:0.0004065241253658345\n",
            "train loss:0.009003802311446436\n",
            "train loss:0.012212789388925616\n",
            "train loss:0.013441141590641916\n",
            "train loss:0.002781970201762706\n",
            "train loss:0.0033384217590202185\n",
            "train loss:0.0022465825866122818\n",
            "train loss:0.001218366766410489\n",
            "train loss:0.0014181318902341605\n",
            "train loss:0.002011928199612414\n",
            "train loss:0.0036744868913901117\n",
            "train loss:0.0002533375619467741\n",
            "train loss:0.004117683100532134\n",
            "train loss:0.009001351072049687\n",
            "train loss:0.010345661347021756\n",
            "train loss:0.001512150945149206\n",
            "train loss:0.0007031286086931869\n",
            "train loss:0.000929416399257858\n",
            "train loss:0.009562388978632642\n",
            "train loss:0.0005344584296725811\n",
            "train loss:0.0010012204568437357\n",
            "train loss:0.0021195965227877923\n",
            "train loss:0.00035232855159495613\n",
            "train loss:0.0029762325062461887\n",
            "train loss:0.00016404952020815795\n",
            "train loss:0.004221958636417559\n",
            "train loss:0.0002807779909545587\n",
            "train loss:0.006769583635220046\n",
            "train loss:0.012085917274675236\n",
            "train loss:0.00261226533395103\n",
            "train loss:0.00027303332936485966\n",
            "train loss:0.016654954878028004\n",
            "train loss:0.0008011628703436944\n",
            "train loss:0.012539319117730714\n",
            "train loss:0.002305919751377034\n",
            "train loss:0.004489013108901278\n",
            "train loss:0.010153469057077964\n",
            "train loss:0.013016757444039496\n",
            "train loss:0.0006491057159587927\n",
            "train loss:0.046093117447764204\n",
            "train loss:0.0008343219903365612\n",
            "train loss:0.002797942658156782\n",
            "train loss:0.00212908628571319\n",
            "train loss:0.0023462933193227602\n",
            "train loss:0.0018642783746135464\n",
            "train loss:0.0002766221497971327\n",
            "train loss:0.00755791326890985\n",
            "train loss:0.00469964569451369\n",
            "train loss:0.003510952824521546\n",
            "train loss:0.008042853273099042\n",
            "train loss:0.0024163489757803823\n",
            "train loss:0.002993980489264949\n",
            "train loss:0.005069626842844931\n",
            "train loss:0.0024752386024445914\n",
            "train loss:0.004842992162457773\n",
            "train loss:0.0011371452106600296\n",
            "train loss:0.0075948867061538805\n",
            "train loss:0.002648268770179721\n",
            "train loss:0.0014344067556650419\n",
            "train loss:0.005707533783490595\n",
            "train loss:0.0005381998499359576\n",
            "train loss:0.006629925314383636\n",
            "train loss:0.00043802672394160696\n",
            "train loss:0.0009386764371066083\n",
            "train loss:0.0009137787600823073\n",
            "train loss:0.005115114200557333\n",
            "train loss:0.007987143718510774\n",
            "train loss:0.008177617612107155\n",
            "train loss:0.00339654149402986\n",
            "train loss:0.0024787159380524966\n",
            "train loss:0.002517447923144735\n",
            "train loss:0.004903848725312068\n",
            "train loss:0.006418831070126516\n",
            "train loss:0.002101213433127193\n",
            "train loss:0.0004514959019326747\n",
            "train loss:0.005049192720872663\n",
            "train loss:0.0004429189410004276\n",
            "train loss:0.0005158058735123404\n",
            "train loss:0.0006338906708774719\n",
            "train loss:0.0003681821428632167\n",
            "train loss:0.004699088188891005\n",
            "train loss:0.013272281104230588\n",
            "train loss:0.00026912217394497345\n",
            "train loss:0.001878093143004785\n",
            "train loss:0.002747152321003223\n",
            "train loss:0.02478624415290208\n",
            "train loss:0.005043141640820575\n",
            "train loss:0.00046802437361057747\n",
            "train loss:0.0027999242529068453\n",
            "train loss:0.002996432132615844\n",
            "train loss:0.000832822974415957\n",
            "train loss:0.036580703118633\n",
            "train loss:0.00047628551949913735\n",
            "train loss:0.0008305343081477109\n",
            "train loss:0.002360905264910422\n",
            "train loss:0.0004609018603809769\n",
            "train loss:0.0013822843624701842\n",
            "train loss:0.0024359476886614798\n",
            "train loss:0.009690164858980934\n",
            "train loss:0.005477116514531077\n",
            "train loss:0.0003057258093188849\n",
            "train loss:0.0006131898483130502\n",
            "train loss:0.02235576248069239\n",
            "train loss:0.0014426264827440503\n",
            "train loss:0.02075910216829293\n",
            "train loss:0.004773423315456925\n",
            "train loss:0.006977722413950302\n",
            "train loss:0.0010025534674748535\n",
            "train loss:0.0067059773971643935\n",
            "train loss:0.001623461439231203\n",
            "train loss:0.0009966263164263272\n",
            "train loss:0.0024768343651991372\n",
            "train loss:0.004367374236885525\n",
            "train loss:0.0006467751196894702\n",
            "train loss:0.0009044886938935833\n",
            "train loss:0.011306466688295292\n",
            "train loss:0.002094586843725786\n",
            "train loss:0.0023496000026320325\n",
            "train loss:0.004201142570381736\n",
            "train loss:0.0021555517977503765\n",
            "train loss:0.0012639820531714753\n",
            "train loss:3.69130768752243e-05\n",
            "train loss:0.005264247780202657\n",
            "train loss:0.0011880281550525084\n",
            "train loss:0.004375698394080712\n",
            "train loss:0.008956584590642257\n",
            "train loss:0.0027764816119850316\n",
            "train loss:0.0004046201311624874\n",
            "train loss:0.0009465358145755175\n",
            "train loss:0.005951851491836047\n",
            "train loss:0.0017272119884071302\n",
            "train loss:0.0005277140484984899\n",
            "train loss:0.0035480053716790316\n",
            "train loss:0.0021687006920505774\n",
            "train loss:0.0015306801375702366\n",
            "train loss:0.0034390914072758455\n",
            "train loss:0.0009719478872288372\n",
            "train loss:0.01652506088747094\n",
            "train loss:0.005221153449524951\n",
            "train loss:0.0010606059406296528\n",
            "train loss:0.011104325255006254\n",
            "train loss:0.00035322085921240864\n",
            "train loss:9.890928130991373e-05\n",
            "train loss:0.0014604943779776209\n",
            "train loss:0.00039867114156783636\n",
            "train loss:0.004456850949055784\n",
            "train loss:0.00406552498853631\n",
            "train loss:0.017338159946838846\n",
            "train loss:0.002667005615226296\n",
            "train loss:0.015523763960879837\n",
            "train loss:0.002859608838684048\n",
            "train loss:0.0007772918349392835\n",
            "train loss:0.003298341197742894\n",
            "train loss:0.018789413419124015\n",
            "train loss:0.009369032232931703\n",
            "train loss:0.0007221434581341813\n",
            "train loss:0.0023198207113183473\n",
            "train loss:0.004132520664461532\n",
            "train loss:0.003188810830807039\n",
            "train loss:0.005181683823559339\n",
            "train loss:0.002171229265655063\n",
            "train loss:0.01790358739803485\n",
            "train loss:0.002842147082459667\n",
            "train loss:0.0030035002174649823\n",
            "train loss:0.0008592947656194841\n",
            "train loss:0.000497286580126408\n",
            "train loss:0.0021739024611588595\n",
            "train loss:0.005657995628394813\n",
            "train loss:0.000631882390277327\n",
            "train loss:0.0016298899725172222\n",
            "train loss:0.0011310726389440323\n",
            "train loss:0.0011025685699829625\n",
            "train loss:0.0007993579435599469\n",
            "train loss:0.00026903195297368513\n",
            "train loss:0.0001172944350193233\n",
            "train loss:0.002355003328164686\n",
            "train loss:0.0034206712969159924\n",
            "train loss:0.01018418540310683\n",
            "train loss:0.0017319394018058998\n",
            "train loss:0.0014698882245177789\n",
            "train loss:0.00234770670943432\n",
            "train loss:0.0022635661130866363\n",
            "train loss:0.004580468199240043\n",
            "train loss:0.0017310149874989755\n",
            "train loss:0.006777409906999172\n",
            "train loss:0.0009663029734128188\n",
            "train loss:0.01017992803003512\n",
            "train loss:0.0021669427885281055\n",
            "train loss:0.00482071266367337\n",
            "train loss:0.0006126033324441577\n",
            "train loss:0.0012465987130395682\n",
            "train loss:0.0009797314614576327\n",
            "train loss:0.000437423995630802\n",
            "train loss:0.0006141889519103124\n",
            "train loss:0.0018114893099155087\n",
            "train loss:0.0009840218863109915\n",
            "train loss:0.0020879676775989162\n",
            "train loss:0.002267359911072986\n",
            "train loss:0.001122236987540586\n",
            "train loss:0.00039015543132891827\n",
            "train loss:0.0035737825169758763\n",
            "train loss:0.0007138710830566096\n",
            "train loss:0.0019529113588823144\n",
            "train loss:0.0021185715285798458\n",
            "train loss:0.002296615309675613\n",
            "train loss:0.0018476527719454598\n",
            "train loss:0.0036097815608770677\n",
            "train loss:0.002450905737176108\n",
            "train loss:0.0013545713353190728\n",
            "train loss:0.00023338384254893406\n",
            "train loss:0.0017887677246339296\n",
            "train loss:0.0005280639975384137\n",
            "train loss:0.002481704163876517\n",
            "train loss:0.004947600955407913\n",
            "train loss:0.0009003356632238593\n",
            "train loss:0.0013443011966370508\n",
            "train loss:0.004558488277192887\n",
            "train loss:0.0002021334937069159\n",
            "train loss:0.005235851057505319\n",
            "train loss:0.00029785603241555276\n",
            "train loss:0.004501203988913025\n",
            "train loss:0.001182949755257974\n",
            "train loss:0.0008708579005853053\n",
            "train loss:0.0011720265297761906\n",
            "train loss:0.003439661647198424\n",
            "train loss:0.00012334508248142446\n",
            "train loss:0.0010598266451132391\n",
            "train loss:0.027071086076078263\n",
            "train loss:0.0016429818215943973\n",
            "train loss:0.001735710411228958\n",
            "train loss:0.00037394521586869606\n",
            "train loss:0.023872026388588416\n",
            "train loss:0.007156711428073857\n",
            "train loss:0.00333159989525175\n",
            "train loss:0.002766175260106581\n",
            "train loss:0.0011025508007996042\n",
            "train loss:0.003897538440526712\n",
            "train loss:0.003208495891612664\n",
            "train loss:0.0014254142449126473\n",
            "train loss:0.0071103968753590645\n",
            "train loss:0.002158675552363078\n",
            "train loss:0.0019084094635227807\n",
            "train loss:0.0005399650686849657\n",
            "train loss:0.0008180291685946747\n",
            "train loss:0.0010883155630537651\n",
            "train loss:0.00048224759573083786\n",
            "train loss:0.001996242125314222\n",
            "train loss:0.02248274071057163\n",
            "train loss:0.037711378556705186\n",
            "train loss:0.002052028686462608\n",
            "train loss:0.000232165473777084\n",
            "train loss:0.0004952304038049162\n",
            "train loss:0.004209088473088414\n",
            "train loss:0.0024851746625352287\n",
            "train loss:0.01767606777372059\n",
            "train loss:0.004580610121100072\n",
            "train loss:0.0010740208425319949\n",
            "train loss:0.006726181345089606\n",
            "train loss:0.00957794763065915\n",
            "train loss:0.0025687759184530694\n",
            "train loss:0.002598738080645372\n",
            "train loss:0.0034270676992723455\n",
            "train loss:0.005895113678403428\n",
            "train loss:0.0023847470324308466\n",
            "train loss:0.00529384049513861\n",
            "train loss:0.019862585617530296\n",
            "train loss:0.0057969397986609794\n",
            "train loss:0.0014916062558277992\n",
            "train loss:0.007428820357413201\n",
            "train loss:0.0007446959094747351\n",
            "train loss:0.0024619065723047737\n",
            "train loss:0.002114501342778506\n",
            "train loss:0.004861951963422031\n",
            "train loss:0.0013657908132353588\n",
            "train loss:0.0025682581657742614\n",
            "train loss:0.0031553032420071803\n",
            "train loss:0.0037227534369040412\n",
            "train loss:0.0012483705501153716\n",
            "train loss:0.0012997928092598474\n",
            "train loss:0.003556355837304832\n",
            "train loss:0.007261371971801208\n",
            "train loss:0.00038309070012843943\n",
            "train loss:0.0022007316857532624\n",
            "train loss:0.004148809063061529\n",
            "train loss:0.0019706287749436465\n",
            "train loss:0.0003244957906726961\n",
            "train loss:0.008119306484081398\n",
            "train loss:0.003319767501372375\n",
            "train loss:0.0022548352283186038\n",
            "train loss:0.0014020806306650655\n",
            "train loss:0.003225521790611759\n",
            "train loss:0.00339316224540672\n",
            "train loss:0.002520783138704125\n",
            "train loss:0.005098319098193585\n",
            "train loss:0.0015127416737028409\n",
            "train loss:0.0010902171198575105\n",
            "train loss:0.0005287354749007033\n",
            "train loss:0.004389380464401731\n",
            "train loss:0.0018853269557196923\n",
            "train loss:0.0034515434304714194\n",
            "train loss:0.0020401853587913342\n",
            "train loss:0.00029186349579271294\n",
            "train loss:0.000584910871050701\n",
            "train loss:0.001465624252392013\n",
            "train loss:0.002678359794207394\n",
            "train loss:0.0008714520594699946\n",
            "train loss:0.0024495078104234767\n",
            "train loss:0.0007932054578608228\n",
            "train loss:0.0014694832286481813\n",
            "train loss:0.0011888624826026965\n",
            "train loss:0.0008281152264030901\n",
            "train loss:0.00034127131683392426\n",
            "train loss:0.0008414188286207157\n",
            "train loss:0.009097085786555657\n",
            "train loss:0.002791410907404124\n",
            "train loss:0.0031832744408098025\n",
            "train loss:0.0014680431180105849\n",
            "train loss:0.0007566826389118833\n",
            "train loss:0.003663890252740984\n",
            "train loss:0.0032974547707793783\n",
            "train loss:0.00027776943479156693\n",
            "train loss:0.0005419856576708419\n",
            "train loss:0.0026655130005416527\n",
            "train loss:0.00311601835101782\n",
            "train loss:0.0025083190159002635\n",
            "train loss:0.004271540477825861\n",
            "train loss:0.00124278897635741\n",
            "train loss:0.005440165947018856\n",
            "train loss:0.0005107549629925538\n",
            "train loss:0.00077624310065642\n",
            "train loss:0.0014582075664997726\n",
            "train loss:0.00018610271779768452\n",
            "train loss:0.0025909225946830145\n",
            "train loss:0.0005683143518638795\n",
            "train loss:0.0013389503626572777\n",
            "train loss:0.0089866433491382\n",
            "train loss:0.0018585985231083472\n",
            "train loss:0.014811165463142293\n",
            "train loss:0.0005024076737420798\n",
            "train loss:0.0012951326640302072\n",
            "train loss:0.01377476578747777\n",
            "train loss:0.000710389421724977\n",
            "train loss:0.014391949639670226\n",
            "train loss:0.0015538414791248728\n",
            "train loss:0.000914706107121014\n",
            "train loss:0.007060016710213055\n",
            "train loss:0.00236916295163733\n",
            "train loss:0.005189112937523197\n",
            "train loss:0.0007606592017642535\n",
            "train loss:0.0024245611720895054\n",
            "train loss:0.012051904114544362\n",
            "train loss:0.001430526471963202\n",
            "train loss:0.005471325588839687\n",
            "train loss:0.005307410249825912\n",
            "train loss:0.0025817081787469585\n",
            "train loss:0.011627741290665112\n",
            "train loss:0.0003015301947847489\n",
            "train loss:0.0003170673621710192\n",
            "train loss:0.007427614623845545\n",
            "train loss:0.0012820842907473359\n",
            "train loss:0.0006534202056306805\n",
            "train loss:0.0020620267037439206\n",
            "train loss:0.0012851489481490125\n",
            "train loss:0.0009933220750672248\n",
            "train loss:0.003607280190905204\n",
            "train loss:0.0011773892877656052\n",
            "train loss:0.0011113464115114593\n",
            "train loss:0.00435911302800772\n",
            "train loss:0.008175260499026863\n",
            "train loss:0.015205558493110949\n",
            "train loss:0.0047550363995205415\n",
            "train loss:0.0016318293060090276\n",
            "train loss:0.008808402177143236\n",
            "train loss:0.0008676134212761355\n",
            "train loss:0.11155885057722631\n",
            "train loss:0.0055736666286790824\n",
            "train loss:0.005745811882360974\n",
            "train loss:0.007310335191577245\n",
            "train loss:0.0006108170120661053\n",
            "train loss:0.00585959468532237\n",
            "train loss:0.003350076292968855\n",
            "train loss:0.035686283982609285\n",
            "train loss:0.0010514776894076556\n",
            "train loss:0.008561056896052343\n",
            "train loss:0.00483551374559465\n",
            "train loss:0.003283227777506541\n",
            "train loss:0.007790634726091936\n",
            "train loss:0.006737610899951969\n",
            "train loss:0.0033776078771630747\n",
            "train loss:0.004806117464042044\n",
            "train loss:0.004882539779818369\n",
            "train loss:0.009969988582211728\n",
            "train loss:0.0038488396012260344\n",
            "train loss:0.005422526204628568\n",
            "train loss:0.020974528393885\n",
            "train loss:0.017870253985790972\n",
            "train loss:0.003605260790253398\n",
            "train loss:0.0015859173913203046\n",
            "train loss:0.02252210375576758\n",
            "train loss:0.0024682701825579035\n",
            "train loss:0.0023865402375010327\n",
            "train loss:0.004008468152793784\n",
            "train loss:0.003694418397598838\n",
            "train loss:0.011455331908487732\n",
            "train loss:0.0009719068493168871\n",
            "train loss:0.0002860039562879233\n",
            "train loss:0.0011336623375092678\n",
            "train loss:0.00046249141041045154\n",
            "train loss:0.001196997301622564\n",
            "train loss:0.0004707202863409493\n",
            "train loss:0.0011511709221982199\n",
            "train loss:0.004398321197406164\n",
            "train loss:0.00267888106644839\n",
            "train loss:0.002539794662212308\n",
            "train loss:0.0013444066577437044\n",
            "train loss:0.0004997276850799674\n",
            "train loss:0.00236448248067377\n",
            "train loss:0.025291116608831276\n",
            "train loss:0.003213923342648844\n",
            "train loss:0.0009707755675181153\n",
            "train loss:0.00785079046594492\n",
            "train loss:0.0029757387082263325\n",
            "train loss:0.010235528956217014\n",
            "train loss:0.008428384129466815\n",
            "train loss:0.0005169445844702305\n",
            "train loss:0.0009100396132720779\n",
            "train loss:0.0018922560726770336\n",
            "train loss:0.001712175794709461\n",
            "train loss:0.00284225485950826\n",
            "train loss:0.00925364090957036\n",
            "train loss:0.0031279845280391956\n",
            "train loss:0.0016623127691536863\n",
            "train loss:0.003617814862913412\n",
            "train loss:0.05647404254843282\n",
            "train loss:0.03298517409742702\n",
            "train loss:0.017054391750554884\n",
            "train loss:0.0021975700058657318\n",
            "train loss:0.002397396396706305\n",
            "train loss:0.017915630886209906\n",
            "train loss:0.0028168071256495413\n",
            "train loss:0.0004110849399342619\n",
            "train loss:0.0030525971166963494\n",
            "train loss:0.004493134536794519\n",
            "train loss:0.0013914457398222187\n",
            "train loss:0.0054105614352180885\n",
            "train loss:0.0030226714317443566\n",
            "train loss:0.013337264341363073\n",
            "train loss:0.0021190202212891325\n",
            "train loss:0.003831300478279505\n",
            "train loss:0.0027928964476409864\n",
            "train loss:0.01898233698245937\n",
            "train loss:0.0005219253898857835\n",
            "train loss:0.0025480677009778034\n",
            "train loss:0.00455797676268295\n",
            "train loss:0.00970055887875174\n",
            "train loss:0.0010885067674526985\n",
            "train loss:0.0013007758666188243\n",
            "train loss:0.009109029468654934\n",
            "train loss:0.02594950028716622\n",
            "train loss:0.02326360390237848\n",
            "train loss:0.00036161251179695755\n",
            "train loss:0.013467562270493666\n",
            "train loss:0.0010032522703754523\n",
            "train loss:0.00490822361019866\n",
            "train loss:0.005393644299581719\n",
            "train loss:0.030003219204370647\n",
            "train loss:0.009168662735300918\n",
            "train loss:0.00442616264580107\n",
            "train loss:0.0007774977530992684\n",
            "train loss:0.0003535330773871626\n",
            "train loss:0.0037062073941506067\n",
            "train loss:0.013730696316769167\n",
            "train loss:0.006570527678731452\n",
            "train loss:0.0002711527992531578\n",
            "train loss:0.00030046955628031005\n",
            "train loss:6.877388396120647e-05\n",
            "train loss:0.00013201696460856549\n",
            "train loss:0.0016499715614136505\n",
            "train loss:0.003722269273281437\n",
            "train loss:0.000131430053233769\n",
            "train loss:0.009276841247527956\n",
            "train loss:0.0030295189544408624\n",
            "train loss:0.0017993346233893632\n",
            "train loss:0.0011160909753098617\n",
            "train loss:0.0005956577761921026\n",
            "train loss:0.003615478277022967\n",
            "train loss:0.0002160459916601023\n",
            "train loss:0.0096674356510736\n",
            "train loss:0.038673845020238844\n",
            "train loss:0.002479690410780937\n",
            "train loss:0.003643794690982668\n",
            "train loss:0.0009684193032529706\n",
            "train loss:0.01076945753233403\n",
            "train loss:0.0015471712264625798\n",
            "train loss:0.0015721238032113288\n",
            "train loss:0.0003403549240369152\n",
            "train loss:0.0011832066640512881\n",
            "train loss:0.00030678625174639746\n",
            "train loss:0.004598997960235096\n",
            "train loss:0.002161651738942893\n",
            "train loss:0.004300661323188376\n",
            "train loss:0.001473745197732606\n",
            "train loss:0.0009607035580135013\n",
            "train loss:0.0019012749894488361\n",
            "train loss:0.0023385195432406684\n",
            "train loss:0.013804936419765056\n",
            "train loss:0.0011693977690779408\n",
            "train loss:0.0033931641467227014\n",
            "train loss:0.010481406327956388\n",
            "train loss:0.00914434319253606\n",
            "train loss:0.0007192101597568757\n",
            "train loss:0.014478242402753923\n",
            "train loss:0.0024926710973246654\n",
            "train loss:0.00046440488125438497\n",
            "train loss:0.000750398964174843\n",
            "train loss:0.0030710167268222298\n",
            "train loss:0.001697668368986873\n",
            "train loss:0.0055198976323306125\n",
            "train loss:0.000986576982515974\n",
            "train loss:0.0005291014015910366\n",
            "train loss:0.0014520771890683217\n",
            "train loss:0.0026499302417256937\n",
            "train loss:0.0054186759075526445\n",
            "train loss:0.0003986155932647102\n",
            "train loss:0.0064410814801043855\n",
            "train loss:0.007476416301161436\n",
            "train loss:0.00041787313880614434\n",
            "train loss:0.0024083643805213302\n",
            "train loss:0.004113585063615106\n",
            "train loss:0.0004999134218042809\n",
            "train loss:0.013420913551561817\n",
            "train loss:0.0005720070090064689\n",
            "train loss:0.002240200096607121\n",
            "train loss:0.008694919182052051\n",
            "train loss:0.007287204198632282\n",
            "train loss:0.004029220289025751\n",
            "train loss:0.0031142925817979937\n",
            "train loss:0.0025400123744340826\n",
            "train loss:0.002289490275173933\n",
            "train loss:0.0017213372598304402\n",
            "train loss:0.014501720347992065\n",
            "train loss:0.0027924300909488815\n",
            "train loss:0.0035078672265436766\n",
            "train loss:0.0022936892964688553\n",
            "train loss:0.0005185681148988262\n",
            "train loss:0.0016157209286827789\n",
            "train loss:0.005073537102810387\n",
            "train loss:0.003653585429221651\n",
            "train loss:0.0027988732619352702\n",
            "train loss:0.0028894633061842577\n",
            "train loss:0.002427542766214375\n",
            "train loss:0.002941343345264426\n",
            "train loss:0.002162570629167991\n",
            "train loss:0.0015249721669837605\n",
            "train loss:0.0017067638315826678\n",
            "train loss:0.007230380686741039\n",
            "train loss:0.024043664185233304\n",
            "train loss:0.0003033099737842343\n",
            "train loss:0.0050742287448889275\n",
            "train loss:0.005614524457774478\n",
            "train loss:0.0017157190435641162\n",
            "train loss:0.001554146436702931\n",
            "train loss:0.002004449352471783\n",
            "train loss:0.003958782013203649\n",
            "train loss:0.011017724213420378\n",
            "=== epoch:14, train acc:0.996, test acc:0.986 ===\n",
            "train loss:0.032444610081308035\n",
            "train loss:0.0013418684420689875\n",
            "train loss:0.0019010853962807062\n",
            "train loss:0.0016109116247503671\n",
            "train loss:0.001339452070706038\n",
            "train loss:0.03535667518478468\n",
            "train loss:0.004331471020956344\n",
            "train loss:0.0002479040527794905\n",
            "train loss:0.0025089815304874074\n",
            "train loss:0.001787569255554909\n",
            "train loss:0.008216877406752006\n",
            "train loss:0.011983831330097212\n",
            "train loss:0.008490676396447495\n",
            "train loss:0.0037983652591864663\n",
            "train loss:0.005505769396040909\n",
            "train loss:0.0026196838688938785\n",
            "train loss:0.006476186182350165\n",
            "train loss:0.005481270159486879\n",
            "train loss:0.004070307508650569\n",
            "train loss:0.0019749763355949145\n",
            "train loss:0.0026399032032788663\n",
            "train loss:0.002495284611386647\n",
            "train loss:0.00031272784002522375\n",
            "train loss:0.006524365421371704\n",
            "train loss:0.0007276255322775059\n",
            "train loss:0.014423578971314915\n",
            "train loss:0.00024606216418405\n",
            "train loss:0.002781074182257919\n",
            "train loss:0.0033304818088592215\n",
            "train loss:0.0017561820481363508\n",
            "train loss:0.006651602145325742\n",
            "train loss:0.0011571164301157684\n",
            "train loss:0.0029380913861325837\n",
            "train loss:0.005365507560156099\n",
            "train loss:0.00034681119786037726\n",
            "train loss:0.001110619832899707\n",
            "train loss:0.002523911354785819\n",
            "train loss:0.005696072917606153\n",
            "train loss:0.00016853448146520532\n",
            "train loss:0.001157595111475089\n",
            "train loss:0.0008451431435042616\n",
            "train loss:0.0020625260598287312\n",
            "train loss:0.008216701324780635\n",
            "train loss:0.004597237595986645\n",
            "train loss:0.0006898706421419466\n",
            "train loss:0.00037607905426461264\n",
            "train loss:0.0018233527321103323\n",
            "train loss:0.002985854642943742\n",
            "train loss:0.003839193133035915\n",
            "train loss:0.00032484593901034787\n",
            "train loss:0.010899702978504294\n",
            "train loss:0.004493619512407805\n",
            "train loss:0.0016587987394697586\n",
            "train loss:0.01451503634133503\n",
            "train loss:0.0019369308289178076\n",
            "train loss:0.004970100731147341\n",
            "train loss:0.0052580536489092375\n",
            "train loss:0.0011206741591562187\n",
            "train loss:0.0038738058248536946\n",
            "train loss:0.00023884440015885773\n",
            "train loss:0.00045557842795632353\n",
            "train loss:0.007048090141880572\n",
            "train loss:0.0006967353629294519\n",
            "train loss:0.0008053040638538677\n",
            "train loss:0.003553835641992457\n",
            "train loss:0.005415807757614125\n",
            "train loss:0.006046757908378401\n",
            "train loss:0.002120892807073317\n",
            "train loss:0.005758943499345882\n",
            "train loss:0.0010841854409412237\n",
            "train loss:0.002307135479018932\n",
            "train loss:0.004740841981866101\n",
            "train loss:0.002008130858217054\n",
            "train loss:0.004781211751257908\n",
            "train loss:0.0007249933169098978\n",
            "train loss:0.0025424982546095394\n",
            "train loss:0.0040497023895708795\n",
            "train loss:0.00045801072370051614\n",
            "train loss:0.0010815607320660016\n",
            "train loss:0.0018202768564431432\n",
            "train loss:0.00244398125405307\n",
            "train loss:0.0004688222582385808\n",
            "train loss:0.000504586337531073\n",
            "train loss:0.002456458785467067\n",
            "train loss:0.002645880495146985\n",
            "train loss:0.0005170004750715021\n",
            "train loss:0.0021417468574358115\n",
            "train loss:0.005749676131772071\n",
            "train loss:0.025746692452923697\n",
            "train loss:0.001880935691356244\n",
            "train loss:0.009865593089313864\n",
            "train loss:0.006447962186422043\n",
            "train loss:0.003374955272927445\n",
            "train loss:0.002374884715895807\n",
            "train loss:0.00028407844071978006\n",
            "train loss:0.007592958988350155\n",
            "train loss:0.00026753988459524136\n",
            "train loss:0.00022081001686711225\n",
            "train loss:0.00042565014653402213\n",
            "train loss:0.0030341856306697235\n",
            "train loss:0.0032075209219716867\n",
            "train loss:0.00393815399453885\n",
            "train loss:0.00026815797017474446\n",
            "train loss:0.007030239117358157\n",
            "train loss:0.00026303487379517\n",
            "train loss:0.0027328701728882906\n",
            "train loss:0.0009770864394759674\n",
            "train loss:0.002062706625290166\n",
            "train loss:0.007337769394235278\n",
            "train loss:0.007774025997435768\n",
            "train loss:0.0031788886667125384\n",
            "train loss:0.00021818599963442393\n",
            "train loss:0.004944054849356086\n",
            "train loss:0.006591995930207768\n",
            "train loss:0.0005942137159412897\n",
            "train loss:0.0025332732899389946\n",
            "train loss:0.00017675544184955324\n",
            "train loss:0.0050818963094588394\n",
            "train loss:0.0015276585502475846\n",
            "train loss:0.0014701768340772187\n",
            "train loss:0.0009788028867713718\n",
            "train loss:0.00218400083896031\n",
            "train loss:0.00035067329927410666\n",
            "train loss:0.0021430557672030195\n",
            "train loss:0.003201466404161481\n",
            "train loss:0.004321129931374543\n",
            "train loss:0.0003353200918636107\n",
            "train loss:0.010753472498665404\n",
            "train loss:0.001056978766016476\n",
            "train loss:0.00293513542797417\n",
            "train loss:0.0022765689327501745\n",
            "train loss:0.0013811299750952546\n",
            "train loss:0.0006745833649880601\n",
            "train loss:0.0017327701741003366\n",
            "train loss:0.0007355494778150896\n",
            "train loss:0.001495707669381941\n",
            "train loss:0.0009507168456239273\n",
            "train loss:0.0016766308985036104\n",
            "train loss:0.005670795660471754\n",
            "train loss:0.012551377484270314\n",
            "train loss:0.010264017782477848\n",
            "train loss:0.0030278156682369195\n",
            "train loss:0.0001829803749484587\n",
            "train loss:0.0020485904028481484\n",
            "train loss:0.0006006867237061973\n",
            "train loss:0.07695280742276317\n",
            "train loss:0.0044926374010842065\n",
            "train loss:0.00047843430891030984\n",
            "train loss:0.009464708760182785\n",
            "train loss:0.00017372217715685927\n",
            "train loss:0.0013867143877061593\n",
            "train loss:0.0022179503137604363\n",
            "train loss:0.0025885345649405327\n",
            "train loss:0.00042112410816255247\n",
            "train loss:0.0014509489345904479\n",
            "train loss:0.002890961817041935\n",
            "train loss:0.0034066553233671752\n",
            "train loss:0.0015403375727627352\n",
            "train loss:0.0005684733780759976\n",
            "train loss:0.0013668155347367457\n",
            "train loss:0.007956708401838972\n",
            "train loss:0.0002062085895348259\n",
            "train loss:0.0008158787491215512\n",
            "train loss:0.0006567394680053163\n",
            "train loss:0.007171743710276228\n",
            "train loss:0.0009377577042708272\n",
            "train loss:0.00036722447633226045\n",
            "train loss:0.006190423145419915\n",
            "train loss:0.002611272342471383\n",
            "train loss:7.708228772659254e-05\n",
            "train loss:8.956477667416397e-05\n",
            "train loss:0.0031035396713993916\n",
            "train loss:0.002453319928524917\n",
            "train loss:0.0007845997995852476\n",
            "train loss:0.0008715491088656594\n",
            "train loss:0.0006474523313412125\n",
            "train loss:0.004196570641807026\n",
            "train loss:0.0009433677196859734\n",
            "train loss:0.0006066658204792771\n",
            "train loss:0.0017057191293902993\n",
            "train loss:0.022493949964599448\n",
            "train loss:0.000550081681217666\n",
            "train loss:0.002467444380205545\n",
            "train loss:0.000737726995915672\n",
            "train loss:0.00391579239577674\n",
            "train loss:0.0035187198726858195\n",
            "train loss:0.0011641548662024138\n",
            "train loss:0.0021915979668187453\n",
            "train loss:0.005836109153012263\n",
            "train loss:0.006879820451215392\n",
            "train loss:0.007899499579611443\n",
            "train loss:0.001148504901720119\n",
            "train loss:0.0011603663336883687\n",
            "train loss:0.004966001495660313\n",
            "train loss:0.0024358832500627936\n",
            "train loss:0.0006098530538744444\n",
            "train loss:0.0030450313899270665\n",
            "train loss:0.0022348983609322506\n",
            "train loss:0.004174810152315674\n",
            "train loss:0.0004057506714327121\n",
            "train loss:0.0012433070119967267\n",
            "train loss:0.0008732744296523941\n",
            "train loss:0.0020936572900529483\n",
            "train loss:0.006579964932787258\n",
            "train loss:0.0005280968800627857\n",
            "train loss:0.000698201878682396\n",
            "train loss:0.00041332522322791394\n",
            "train loss:0.006303486179396042\n",
            "train loss:0.0010421855823476225\n",
            "train loss:0.00038454358020330965\n",
            "train loss:0.005867890300672109\n",
            "train loss:0.0010051237098163003\n",
            "train loss:0.00036810202889313775\n",
            "train loss:0.009990304823804214\n",
            "train loss:0.0004918889365194223\n",
            "train loss:0.002661969919180551\n",
            "train loss:0.005700086332724681\n",
            "train loss:0.000488059166495811\n",
            "train loss:0.0009990925397695704\n",
            "train loss:0.00017508061989645154\n",
            "train loss:0.0003525160211760137\n",
            "train loss:0.0014883023767746245\n",
            "train loss:0.00103534210715037\n",
            "train loss:0.00038062492633884274\n",
            "train loss:0.0024900862939615923\n",
            "train loss:0.0007617289505901549\n",
            "train loss:0.0004038860380609846\n",
            "train loss:0.0015911313345389727\n",
            "train loss:0.00016934714773463058\n",
            "train loss:0.001863420355205563\n",
            "train loss:0.002998654036985377\n",
            "train loss:0.0007060705843644215\n",
            "train loss:0.00036676530416094514\n",
            "train loss:0.014475142565401755\n",
            "train loss:0.0009377157434640154\n",
            "train loss:0.00415423304960405\n",
            "train loss:0.00015885643070054676\n",
            "train loss:0.0007824703436663612\n",
            "train loss:0.0006771544571828752\n",
            "train loss:0.00904336342942571\n",
            "train loss:8.175548438007997e-05\n",
            "train loss:0.004724051015134975\n",
            "train loss:0.009838789939590252\n",
            "train loss:0.0022145990404139042\n",
            "train loss:0.00033628918287764313\n",
            "train loss:0.001793017479269522\n",
            "train loss:0.00017150322266136088\n",
            "train loss:0.0004900881494054219\n",
            "train loss:0.007599513986993287\n",
            "train loss:0.0004016624299703957\n",
            "train loss:0.0009720050052493185\n",
            "train loss:0.001009713821840176\n",
            "train loss:0.0019301144970507611\n",
            "train loss:0.0009496644805888072\n",
            "train loss:0.0025960494042495726\n",
            "train loss:0.0003679405829074471\n",
            "train loss:0.001319897217408729\n",
            "train loss:0.00041099383788977786\n",
            "train loss:0.001976415001699456\n",
            "train loss:0.0015817306292024355\n",
            "train loss:0.002918370206805308\n",
            "train loss:0.0009806414004841185\n",
            "train loss:0.0003123982591002098\n",
            "train loss:0.013449570667529309\n",
            "train loss:0.008054931895773509\n",
            "train loss:0.0004438512531445716\n",
            "train loss:0.0018083140147044307\n",
            "train loss:0.0008078972150598933\n",
            "train loss:0.0025305199796984957\n",
            "train loss:0.0003067721326643635\n",
            "train loss:0.0019342017633836414\n",
            "train loss:0.0023485410701533614\n",
            "train loss:0.0008230074584178898\n",
            "train loss:0.004524784045213884\n",
            "train loss:0.0017386698090048197\n",
            "train loss:0.006308626525137413\n",
            "train loss:0.003333859342936004\n",
            "train loss:0.001228128917772233\n",
            "train loss:0.003316373736976427\n",
            "train loss:0.01033620254741342\n",
            "train loss:0.011000036137985381\n",
            "train loss:0.0038097606487600723\n",
            "train loss:0.003908711953212379\n",
            "train loss:0.0027795302105474884\n",
            "train loss:0.0017191930295731136\n",
            "train loss:0.0005267535132981966\n",
            "train loss:0.000872785382900281\n",
            "train loss:0.0014922634558741032\n",
            "train loss:0.00026241624709303935\n",
            "train loss:0.0019509797095020967\n",
            "train loss:0.0014409234082526712\n",
            "train loss:0.00010724687221361573\n",
            "train loss:0.0002770046766003637\n",
            "train loss:0.002596809939585791\n",
            "train loss:0.00023387180175938978\n",
            "train loss:0.019214064596961494\n",
            "train loss:0.0006403880574400062\n",
            "train loss:0.0011188197159760788\n",
            "train loss:0.004962087722547299\n",
            "train loss:0.0026648589459155867\n",
            "train loss:0.0005841775792680992\n",
            "train loss:0.000884083971315291\n",
            "train loss:0.011793740094683463\n",
            "train loss:0.002146828564315877\n",
            "train loss:0.0020906534674729666\n",
            "train loss:0.0023483080936907517\n",
            "train loss:0.0018484882853756104\n",
            "train loss:0.0008293950077579156\n",
            "train loss:0.002996006031450752\n",
            "train loss:0.0014180337257010964\n",
            "train loss:0.00025923320988838054\n",
            "train loss:0.002269585238724665\n",
            "train loss:0.0007182599997720697\n",
            "train loss:0.0005572748518017189\n",
            "train loss:0.0020106871884194434\n",
            "train loss:0.0009836152338849517\n",
            "train loss:0.0002990978175295692\n",
            "train loss:0.0014091082404296728\n",
            "train loss:0.0011356379068008131\n",
            "train loss:0.0013331831577047576\n",
            "train loss:0.0005010256104063703\n",
            "train loss:0.0023852623606106394\n",
            "train loss:0.0004305121394383165\n",
            "train loss:0.000622672557831547\n",
            "train loss:0.0018372739262493148\n",
            "train loss:0.0004457161379880935\n",
            "train loss:0.002467413811070832\n",
            "train loss:0.0003910752406066515\n",
            "train loss:0.002019398146596277\n",
            "train loss:0.0009778917088919049\n",
            "train loss:0.0031536330983620462\n",
            "train loss:0.001626880996463175\n",
            "train loss:0.0006703775905115453\n",
            "train loss:0.007019725205227069\n",
            "train loss:0.0004006141885436142\n",
            "train loss:0.0008991966684284433\n",
            "train loss:0.00033633611453508857\n",
            "train loss:0.0010402348690731504\n",
            "train loss:7.472107555725246e-05\n",
            "train loss:0.0015443756825166972\n",
            "train loss:0.00305833130247326\n",
            "train loss:0.0011293696362323876\n",
            "train loss:0.00032179124312267063\n",
            "train loss:0.003839628996240194\n",
            "train loss:0.0010286985583905627\n",
            "train loss:0.00079462064792236\n",
            "train loss:0.0009637436874490988\n",
            "train loss:0.0018451724002005467\n",
            "train loss:0.0016368217919487285\n",
            "train loss:0.0026252827523293993\n",
            "train loss:0.00041712415048886735\n",
            "train loss:0.002617879239307543\n",
            "train loss:0.004868737866310362\n",
            "train loss:0.00040997936398409616\n",
            "train loss:0.008895048792411767\n",
            "train loss:0.0008187243256623926\n",
            "train loss:3.9559498779018615e-05\n",
            "train loss:0.004317222754693069\n",
            "train loss:0.005544118656391853\n",
            "train loss:0.0006827595226554928\n",
            "train loss:0.014714573829000098\n",
            "train loss:0.00012393178753364025\n",
            "train loss:0.002043504296681947\n",
            "train loss:0.0016673325694727375\n",
            "train loss:0.00815908444317299\n",
            "train loss:0.00012675810286806125\n",
            "train loss:0.00044551973843462244\n",
            "train loss:0.0037807588316173613\n",
            "train loss:0.001199688878047801\n",
            "train loss:0.0009034799255520636\n",
            "train loss:0.00047052091691146666\n",
            "train loss:0.01577439192677237\n",
            "train loss:0.0017466140409460847\n",
            "train loss:0.010789097298095912\n",
            "train loss:0.0013124146610615658\n",
            "train loss:0.0013334087988759319\n",
            "train loss:0.00033361813050558674\n",
            "train loss:0.011491662820408295\n",
            "train loss:0.0009235983407844559\n",
            "train loss:0.0007462207310017886\n",
            "train loss:0.0012684757489121845\n",
            "train loss:0.0009711698329874286\n",
            "train loss:0.0002992921286606809\n",
            "train loss:0.0002515591253772448\n",
            "train loss:0.004532297251187858\n",
            "train loss:0.0012886188348319408\n",
            "train loss:0.0004389173047480986\n",
            "train loss:0.00046881560231565626\n",
            "train loss:0.0029208252245863887\n",
            "train loss:0.0009173355907748004\n",
            "train loss:0.0004988195229924791\n",
            "train loss:0.001148642169541367\n",
            "train loss:0.0009611952521888989\n",
            "train loss:0.0016831374656716463\n",
            "train loss:0.002197323182340978\n",
            "train loss:0.0033577689716648446\n",
            "train loss:0.0015199189789657814\n",
            "train loss:0.005551482611426676\n",
            "train loss:0.00017876334704101091\n",
            "train loss:0.0016661365155240726\n",
            "train loss:0.0017597344198530345\n",
            "train loss:0.001466003412817411\n",
            "train loss:0.00035897331080989877\n",
            "train loss:0.001044937452007044\n",
            "train loss:0.001431075386932744\n",
            "train loss:0.0008288895225108325\n",
            "train loss:0.0007742986852557119\n",
            "train loss:0.00017194905234863206\n",
            "train loss:0.0011676143437120336\n",
            "train loss:0.0007023952558019576\n",
            "train loss:0.002953323693195549\n",
            "train loss:9.207709243866407e-05\n",
            "train loss:0.0003792733861999133\n",
            "train loss:0.0016698927564541138\n",
            "train loss:0.0009475359907280527\n",
            "train loss:0.0010596477488956863\n",
            "train loss:0.015161200426467649\n",
            "train loss:0.0001239816815539293\n",
            "train loss:0.0005612598665354463\n",
            "train loss:0.0013373696352151534\n",
            "train loss:0.0003129489121991501\n",
            "train loss:0.0029251845307843322\n",
            "train loss:0.0012470830078953469\n",
            "train loss:0.0012294705784242522\n",
            "train loss:0.0003299279302379974\n",
            "train loss:0.0015608893943516735\n",
            "train loss:0.014486587371941521\n",
            "train loss:0.0019724845429968142\n",
            "train loss:0.0010270362517891091\n",
            "train loss:0.0004889162567032584\n",
            "train loss:0.005495735045360147\n",
            "train loss:0.0007281971998616806\n",
            "train loss:0.0018508974703007005\n",
            "train loss:0.0004815277058115206\n",
            "train loss:0.001931279745850982\n",
            "train loss:0.012201967952374356\n",
            "train loss:0.002893435204415262\n",
            "train loss:0.0008121397141155062\n",
            "train loss:0.003251188662769908\n",
            "train loss:0.0014232647012190603\n",
            "train loss:0.016680933221200372\n",
            "train loss:0.000953451252837054\n",
            "train loss:0.0002361859205256988\n",
            "train loss:0.000652536464371815\n",
            "train loss:0.0007734625772071973\n",
            "train loss:0.0004665212463204881\n",
            "train loss:0.0005274958794360834\n",
            "train loss:0.00019315859021396372\n",
            "train loss:0.0003181895447134371\n",
            "train loss:0.004332677350647267\n",
            "train loss:0.00319858432050877\n",
            "train loss:0.0019286565627265682\n",
            "train loss:0.00029804394634734403\n",
            "train loss:0.0004153921928304936\n",
            "train loss:0.00444530454128305\n",
            "train loss:0.00021948730492444345\n",
            "train loss:0.0017195429653615612\n",
            "train loss:0.0012123227187870234\n",
            "train loss:0.0025431085568784887\n",
            "train loss:0.0037721175932371796\n",
            "train loss:0.003212574206892127\n",
            "train loss:0.009715174615132525\n",
            "train loss:0.011129585721652187\n",
            "train loss:0.002186450778903556\n",
            "train loss:0.00418301934111462\n",
            "train loss:0.0017940144528286086\n",
            "train loss:0.0008176408283691267\n",
            "train loss:0.000750162582960207\n",
            "train loss:0.001551799505289308\n",
            "train loss:0.0014928434379428196\n",
            "train loss:0.00630695609963609\n",
            "train loss:0.0037219479509668684\n",
            "train loss:0.0005737112997756546\n",
            "train loss:0.00012751825224075307\n",
            "train loss:0.0001492373010990718\n",
            "train loss:0.0027962228628871846\n",
            "train loss:0.0007013088983709869\n",
            "train loss:0.0014419952835212763\n",
            "train loss:0.0004284688713462096\n",
            "train loss:0.004965194541312597\n",
            "train loss:0.00016906351566161327\n",
            "train loss:0.002321936859589126\n",
            "train loss:0.000626708177300838\n",
            "train loss:0.0044169663463439306\n",
            "train loss:0.00040353779672711157\n",
            "train loss:0.0006152114280729056\n",
            "train loss:0.0001319627030023158\n",
            "train loss:0.0016975962759284311\n",
            "train loss:0.001426880696215749\n",
            "train loss:0.00032700307546972533\n",
            "train loss:0.0007264487174985531\n",
            "train loss:0.00038753011989010975\n",
            "train loss:0.0008776552150832127\n",
            "train loss:0.0018763485120122892\n",
            "train loss:0.008681902170928261\n",
            "train loss:0.0009206337514575697\n",
            "train loss:0.0006538188985027249\n",
            "train loss:0.005897788876459623\n",
            "train loss:0.00016800856780433755\n",
            "train loss:0.0018963183407885859\n",
            "train loss:0.0005923966781680969\n",
            "train loss:0.0029440760031813956\n",
            "train loss:0.00019510226049639955\n",
            "train loss:0.0006567274787233307\n",
            "train loss:0.0009842495307523396\n",
            "train loss:0.005467172195830722\n",
            "train loss:0.005860733690938175\n",
            "train loss:0.013760230641952592\n",
            "train loss:0.0010921877394669188\n",
            "train loss:0.0014151568408889295\n",
            "train loss:0.0014836705171270084\n",
            "train loss:0.001322522565261474\n",
            "train loss:0.0022826533783339526\n",
            "train loss:0.0020313106957896856\n",
            "train loss:0.0011776239109063896\n",
            "train loss:0.0004443954457925471\n",
            "train loss:0.045108352006765245\n",
            "train loss:0.008032481882432524\n",
            "train loss:0.003748988086091769\n",
            "train loss:0.002388263179315443\n",
            "train loss:0.0019609609703339723\n",
            "train loss:0.00098834417875884\n",
            "train loss:0.00033428011102600903\n",
            "train loss:0.0008068912556218569\n",
            "train loss:0.0008174518151294052\n",
            "train loss:0.0009510313350273307\n",
            "train loss:0.0002926044468373175\n",
            "train loss:0.00044379339355465544\n",
            "train loss:0.008285112743710438\n",
            "train loss:0.005998086999396646\n",
            "train loss:0.0018781238046316413\n",
            "train loss:0.0026119207981855257\n",
            "train loss:0.0017025874977173044\n",
            "train loss:0.004088555544760425\n",
            "train loss:0.006333910188139422\n",
            "train loss:0.0007151286474688251\n",
            "train loss:0.0006269457448297238\n",
            "train loss:0.009772986376684072\n",
            "train loss:0.0013876357120648417\n",
            "train loss:0.0005205325200168022\n",
            "train loss:0.002807747353380336\n",
            "train loss:0.03105475529459139\n",
            "train loss:0.0005283309786738278\n",
            "train loss:0.0013224401798837728\n",
            "train loss:0.0005451253435134072\n",
            "train loss:0.0010352887845525931\n",
            "train loss:3.811643797079872e-05\n",
            "train loss:0.004584043872087528\n",
            "train loss:0.0012587206513280822\n",
            "train loss:0.0005284198397155945\n",
            "train loss:0.005494095660907352\n",
            "train loss:0.0017609938172353234\n",
            "train loss:0.003082875675566139\n",
            "train loss:0.0006780700228215292\n",
            "train loss:0.00023501448724636502\n",
            "train loss:2.486832787894638e-05\n",
            "train loss:0.0027864518467912696\n",
            "train loss:0.0005588643888590467\n",
            "train loss:0.001879879172069686\n",
            "train loss:0.0002966524495697083\n",
            "train loss:0.0015031046514808605\n",
            "train loss:0.0020561584457063705\n",
            "train loss:0.0017732470824675163\n",
            "train loss:0.0002661819551695058\n",
            "train loss:0.0009645978503073162\n",
            "train loss:0.0021717284192322535\n",
            "train loss:0.0010342839873655372\n",
            "train loss:0.012859504739514773\n",
            "train loss:0.0021987346694574885\n",
            "train loss:0.0008387828967415115\n",
            "train loss:0.0015530844459953688\n",
            "train loss:0.00023761733041659103\n",
            "train loss:0.0019394196056268251\n",
            "train loss:0.007917696826674758\n",
            "train loss:0.0010942653595798506\n",
            "train loss:0.004746582367083494\n",
            "train loss:0.00262917883254419\n",
            "train loss:0.005726698647873578\n",
            "train loss:0.012783213546549564\n",
            "train loss:0.003617090874705163\n",
            "train loss:0.0014261434667989918\n",
            "train loss:0.003168720037431318\n",
            "train loss:0.00023345659385777106\n",
            "train loss:0.002439597476042714\n",
            "train loss:0.0006224918951504455\n",
            "train loss:0.00014439855637548508\n",
            "train loss:0.0024855333241277353\n",
            "train loss:0.0027126039896792848\n",
            "train loss:0.0033566588984562363\n",
            "train loss:0.002459719154610884\n",
            "train loss:0.00021615943371475202\n",
            "train loss:0.00010478777448993497\n",
            "train loss:0.0004233947389012961\n",
            "train loss:0.00038667683780423736\n",
            "train loss:0.0022138226146267035\n",
            "train loss:0.00013118825962090167\n",
            "train loss:0.003077925548561533\n",
            "train loss:0.0015456665656762738\n",
            "train loss:0.0009257326933348582\n",
            "train loss:0.0013041473139156214\n",
            "=== epoch:15, train acc:0.999, test acc:0.987 ===\n",
            "train loss:0.0006107495873996576\n",
            "train loss:0.0020916768942646695\n",
            "train loss:0.04395746547838076\n",
            "train loss:0.000513337407448218\n",
            "train loss:0.0020246027861880287\n",
            "train loss:0.0009963301120596733\n",
            "train loss:0.0017780656135958166\n",
            "train loss:0.005536030020108595\n",
            "train loss:0.0015815897595078843\n",
            "train loss:0.00043612230433804917\n",
            "train loss:0.019259338527863296\n",
            "train loss:0.0034806914078447065\n",
            "train loss:0.003530154341870198\n",
            "train loss:0.0010980779279138476\n",
            "train loss:0.0022982498305642047\n",
            "train loss:0.00020138469470927265\n",
            "train loss:0.0009461757855910454\n",
            "train loss:0.0027966372433865278\n",
            "train loss:0.000996143634232683\n",
            "train loss:1.3717222311934728e-05\n",
            "train loss:0.00042568160057845634\n",
            "train loss:0.0001878979535566289\n",
            "train loss:0.012115456233295407\n",
            "train loss:0.0010643146546302582\n",
            "train loss:0.0008339646345255733\n",
            "train loss:0.017237483904619176\n",
            "train loss:0.0015165687469490716\n",
            "train loss:0.00047885614243477675\n",
            "train loss:0.002840034264953512\n",
            "train loss:0.004909723390967052\n",
            "train loss:0.000569817325929292\n",
            "train loss:0.002306391244149201\n",
            "train loss:0.001808556191873436\n",
            "train loss:0.0022526624908068126\n",
            "train loss:0.0028749811710927138\n",
            "train loss:0.0003599799627807702\n",
            "train loss:0.0003582028897445446\n",
            "train loss:0.001470818256670146\n",
            "train loss:0.0016084317709036855\n",
            "train loss:0.0010580088017910954\n",
            "train loss:0.0033496891848754777\n",
            "train loss:0.000323603862518157\n",
            "train loss:0.0007527481737673392\n",
            "train loss:0.0006711678087045809\n",
            "train loss:0.0032309399245475353\n",
            "train loss:0.002356378258099216\n",
            "train loss:0.004042837329612841\n",
            "train loss:0.0006047013000094349\n",
            "train loss:0.002039025764541521\n",
            "train loss:0.01479750592903076\n",
            "train loss:0.0007535326390164826\n",
            "train loss:0.0069322307111430274\n",
            "train loss:0.0008132993487190387\n",
            "train loss:0.0007502679996162538\n",
            "train loss:0.002395323053970142\n",
            "train loss:0.0002015832433849502\n",
            "train loss:0.00011529715068348921\n",
            "train loss:0.0002527414302248547\n",
            "train loss:0.00266027476994389\n",
            "train loss:0.00040460603004360574\n",
            "train loss:0.00045435938301396667\n",
            "train loss:0.0007065407240403321\n",
            "train loss:0.0018682532331002896\n",
            "train loss:0.0001445399379411806\n",
            "train loss:0.0004526980672454535\n",
            "train loss:0.0020427374818051867\n",
            "train loss:0.0015420493968826346\n",
            "train loss:0.0046802597184483034\n",
            "train loss:0.0019327627160105052\n",
            "train loss:0.0218906314382356\n",
            "train loss:0.0006404343351732102\n",
            "train loss:0.007696434340685538\n",
            "train loss:0.002528859371744074\n",
            "train loss:0.0017172969183346287\n",
            "train loss:0.003794102736682588\n",
            "train loss:0.00027028768575864255\n",
            "train loss:0.0011223984590642334\n",
            "train loss:0.00016445506804018553\n",
            "train loss:0.001843119419867297\n",
            "train loss:0.000186735575929152\n",
            "train loss:0.0019152093601526477\n",
            "train loss:0.001126113203810091\n",
            "train loss:0.0004998271968007287\n",
            "train loss:0.0019849002164051537\n",
            "train loss:0.0012709873466452116\n",
            "train loss:0.056840628368866615\n",
            "train loss:0.0014060293431590238\n",
            "train loss:0.0007922696732865772\n",
            "train loss:0.0014467623741276072\n",
            "train loss:0.006630883959976328\n",
            "train loss:0.00033673423404798514\n",
            "train loss:0.0030264299233892676\n",
            "train loss:0.0004712172041733236\n",
            "train loss:0.0004537466250167627\n",
            "train loss:0.0005407577254702498\n",
            "train loss:0.0012694013940439716\n",
            "train loss:0.001456387664200829\n",
            "train loss:0.006171566977725068\n",
            "train loss:0.000903129084685097\n",
            "train loss:0.002069393074731138\n",
            "train loss:0.009440267321303515\n",
            "train loss:0.011272448242072138\n",
            "train loss:0.0013094453765172764\n",
            "train loss:0.003463911703096686\n",
            "train loss:0.00016260634530885893\n",
            "train loss:0.003968599265686013\n",
            "train loss:0.011949181205987274\n",
            "train loss:4.574195751682334e-05\n",
            "train loss:0.0013697980946233623\n",
            "train loss:0.001237884247337388\n",
            "train loss:0.0006719730201938727\n",
            "train loss:0.0065824447139275055\n",
            "train loss:0.0003452283116823242\n",
            "train loss:0.001048448631995972\n",
            "train loss:0.005977019486320279\n",
            "train loss:0.0009184649158419751\n",
            "train loss:0.0006102765992555395\n",
            "train loss:0.00321851160522907\n",
            "train loss:0.00937446664411777\n",
            "train loss:0.002373912873839255\n",
            "train loss:0.02039537883603301\n",
            "train loss:0.00032476424738138915\n",
            "train loss:0.0007737874055568372\n",
            "train loss:0.0005539525155867784\n",
            "train loss:0.0014480579023539062\n",
            "train loss:0.0028632509466596573\n",
            "train loss:0.0038616266269514863\n",
            "train loss:0.0002445948723161142\n",
            "train loss:0.0014928181224142059\n",
            "train loss:0.0010987970969892553\n",
            "train loss:0.0011292459112911706\n",
            "train loss:0.0017683095340768036\n",
            "train loss:0.0016383625453736798\n",
            "train loss:0.0017907645228022692\n",
            "train loss:0.0006921294973945309\n",
            "train loss:0.005093850095164681\n",
            "train loss:0.018336909356187196\n",
            "train loss:0.0008134995488234652\n",
            "train loss:0.001291478084502651\n",
            "train loss:0.001368491151605188\n",
            "train loss:0.001499725433668313\n",
            "train loss:0.014792457685464862\n",
            "train loss:0.0004666618502673227\n",
            "train loss:0.011650079943615738\n",
            "train loss:0.00040778431114511175\n",
            "train loss:0.0033055188119513732\n",
            "train loss:0.0025781975529369826\n",
            "train loss:0.017314075667562892\n",
            "train loss:0.002850911540979409\n",
            "train loss:0.005623272934742013\n",
            "train loss:0.00326445393576035\n",
            "train loss:0.002409362709860671\n",
            "train loss:0.0004680124851192658\n",
            "train loss:0.0002807030116084554\n",
            "train loss:0.0020510537278691176\n",
            "train loss:0.007993003135885217\n",
            "train loss:0.001326136531544572\n",
            "train loss:0.003137372241260467\n",
            "train loss:0.0042771370892818105\n",
            "train loss:0.0434433393090298\n",
            "train loss:0.003861296515787148\n",
            "train loss:0.000812159144980174\n",
            "train loss:0.002073516963607403\n",
            "train loss:0.0009226166774572601\n",
            "train loss:0.002560173199547978\n",
            "train loss:0.004610292350597041\n",
            "train loss:0.005228582107872024\n",
            "train loss:0.017303835102378543\n",
            "train loss:0.0020751934069482367\n",
            "train loss:0.001990600011052445\n",
            "train loss:0.004485749195742393\n",
            "train loss:0.007652544087194136\n",
            "train loss:0.002522423355406257\n",
            "train loss:0.009286813073468916\n",
            "train loss:0.001126839316385052\n",
            "train loss:0.010029738164009896\n",
            "train loss:0.0024177853171452423\n",
            "train loss:0.015659773493019852\n",
            "train loss:0.0010707650920043445\n",
            "train loss:0.00335685275215572\n",
            "train loss:0.002347697856742053\n",
            "train loss:0.0007596375771283913\n",
            "train loss:0.0009137398279470126\n",
            "train loss:0.006040930288843778\n",
            "train loss:0.0018491821724077662\n",
            "train loss:0.004451257881326001\n",
            "train loss:0.0013947142334280519\n",
            "train loss:0.0030381517019664366\n",
            "train loss:0.001440511338241429\n",
            "train loss:0.0022599328549268057\n",
            "train loss:0.002049112490356794\n",
            "train loss:0.005580110139799595\n",
            "train loss:0.0005048717307437229\n",
            "train loss:0.0019551678899513898\n",
            "train loss:0.0012259359774637383\n",
            "train loss:0.0014315940852857865\n",
            "train loss:0.007887650405230656\n",
            "train loss:0.0005020994632767349\n",
            "train loss:0.00011778651950671042\n",
            "train loss:0.018477107311967812\n",
            "train loss:0.000727781382236466\n",
            "train loss:0.0032312550555839074\n",
            "train loss:0.010941134204695571\n",
            "train loss:0.0029733645799220713\n",
            "train loss:0.0008257104448656913\n",
            "train loss:0.00020574462603630854\n",
            "train loss:0.007497604960842357\n",
            "train loss:0.004859149312752088\n",
            "train loss:0.005178062728822352\n",
            "train loss:0.00258616459667944\n",
            "train loss:0.00931657927075489\n",
            "train loss:0.0023786878700767563\n",
            "train loss:0.0014908627646589038\n",
            "train loss:0.00441860259877597\n",
            "train loss:0.023008293258435605\n",
            "train loss:0.0006060462599272456\n",
            "train loss:0.007191574952055404\n",
            "train loss:0.004223028477093845\n",
            "train loss:0.004299272441464994\n",
            "train loss:0.0006334214479831662\n",
            "train loss:0.0009910380976751723\n",
            "train loss:0.021213668973382303\n",
            "train loss:0.0015061153474301547\n",
            "train loss:0.00518798465459122\n",
            "train loss:0.0017334813335691803\n",
            "train loss:0.0018209314070975293\n",
            "train loss:0.005354858950106151\n",
            "train loss:0.0028718583578844744\n",
            "train loss:0.0005635008139424622\n",
            "train loss:0.0014123599991648459\n",
            "train loss:0.0006113837143615755\n",
            "train loss:0.038688883704668206\n",
            "train loss:0.0005884577313628589\n",
            "train loss:0.0031939086897088105\n",
            "train loss:0.006711268963494401\n",
            "train loss:0.0036581145446719526\n",
            "train loss:0.002444491994566768\n",
            "train loss:0.002470844581387156\n",
            "train loss:0.0008690612876693818\n",
            "train loss:0.008212618436772788\n",
            "train loss:0.008047546172824873\n",
            "train loss:0.010390128339378753\n",
            "train loss:0.002643636859231199\n",
            "train loss:0.00599394556314605\n",
            "train loss:0.0030664308376851406\n",
            "train loss:0.008499299291398772\n",
            "train loss:0.007282132080038785\n",
            "train loss:0.00039725398252843944\n",
            "train loss:0.004054415938766219\n",
            "train loss:0.0027674428989684042\n",
            "train loss:0.010880306393720961\n",
            "train loss:0.001086507701482979\n",
            "train loss:0.005542388545277185\n",
            "train loss:0.0011636267591472876\n",
            "train loss:0.001837078693994491\n",
            "train loss:0.004164277624933964\n",
            "train loss:0.009230882772706152\n",
            "train loss:0.00687060470182846\n",
            "train loss:0.0017475658474953623\n",
            "train loss:0.0031502357428531192\n",
            "train loss:0.003845702295983454\n",
            "train loss:0.004817807692749762\n",
            "train loss:0.0022117832804662303\n",
            "train loss:0.0028898877692288987\n",
            "train loss:0.0014627829596946707\n",
            "train loss:0.001194977302758291\n",
            "train loss:0.019679716979525398\n",
            "train loss:0.002753019036258684\n",
            "train loss:0.01601701409777287\n",
            "train loss:0.007510725472310144\n",
            "train loss:0.0075742347264528866\n",
            "train loss:0.0071858711157841\n",
            "train loss:0.00742278966329166\n",
            "train loss:0.000287849536536406\n",
            "train loss:0.02867006499999277\n",
            "train loss:0.00989714446832094\n",
            "train loss:0.00206278019682008\n",
            "train loss:0.0027917570753990006\n",
            "train loss:0.006640495257339272\n",
            "train loss:0.007213236707779168\n",
            "train loss:0.006389645967813974\n",
            "train loss:0.0019468941821790295\n",
            "train loss:0.0025841564924531366\n",
            "train loss:0.000718832802869005\n",
            "train loss:0.0035923851907118725\n",
            "train loss:0.0011952408475014744\n",
            "train loss:0.0016298410009885128\n",
            "train loss:0.0027257703795878774\n",
            "train loss:0.04553720329743825\n",
            "train loss:0.004840268503775052\n",
            "train loss:0.00038964261828839774\n",
            "train loss:0.0008478280229651149\n",
            "train loss:0.0017910754673338863\n",
            "train loss:0.003382339338314908\n",
            "train loss:0.0003087617535823791\n",
            "train loss:0.005726270433193517\n",
            "train loss:0.0014175057490161385\n",
            "train loss:0.0014247743842580307\n",
            "train loss:0.003516240122632225\n",
            "train loss:0.00012252274754299874\n",
            "train loss:0.009204211919545631\n",
            "train loss:0.004481398208850929\n",
            "train loss:0.003128378436384723\n",
            "train loss:0.0025693543607118994\n",
            "train loss:0.0029227464332257226\n",
            "train loss:0.0020354330207238004\n",
            "train loss:0.0010870988483904277\n",
            "train loss:0.0044195934562617925\n",
            "train loss:0.0031172026864902088\n",
            "train loss:0.007005105733682147\n",
            "train loss:0.0024878229830762725\n",
            "train loss:0.0011514758882097324\n",
            "train loss:0.0009408942105949494\n",
            "train loss:0.0008464653480661849\n",
            "train loss:0.009008255333268824\n",
            "train loss:0.0019323391747601173\n",
            "train loss:0.0003692158712048276\n",
            "train loss:0.00021600641115653244\n",
            "train loss:9.401832055646247e-05\n",
            "train loss:0.000510128459197595\n",
            "train loss:0.0039152386048814615\n",
            "train loss:0.00038161280967208814\n",
            "train loss:0.006154642808468245\n",
            "train loss:0.005719480128384095\n",
            "train loss:0.001448446372247097\n",
            "train loss:0.011443406672740146\n",
            "train loss:0.0007857464155537109\n",
            "train loss:0.0009538101116764084\n",
            "train loss:0.003744060808623168\n",
            "train loss:0.0015852483780457313\n",
            "train loss:0.0038361898205018625\n",
            "train loss:0.002496446915726532\n",
            "train loss:0.0026007561768154512\n",
            "train loss:8.784694791836236e-05\n",
            "train loss:0.0029147852455741192\n",
            "train loss:0.0005418544222388042\n",
            "train loss:0.0012400014421279119\n",
            "train loss:0.0010471227960058339\n",
            "train loss:0.0028089035938858144\n",
            "train loss:0.0014757355022386861\n",
            "train loss:0.001406318040451867\n",
            "train loss:0.0002529941952690409\n",
            "train loss:0.0013536325838614267\n",
            "train loss:0.0044413300828439325\n",
            "train loss:0.002457709567712992\n",
            "train loss:0.0003387211822552\n",
            "train loss:0.0014609557254168402\n",
            "train loss:0.007812821572339604\n",
            "train loss:0.001515036669046176\n",
            "train loss:0.0012906715173261743\n",
            "train loss:0.0008008376283002805\n",
            "train loss:0.00014258197485479053\n",
            "train loss:0.0007988277859246169\n",
            "train loss:4.810766850550707e-05\n",
            "train loss:0.006588060964251347\n",
            "train loss:0.0003497551610124824\n",
            "train loss:0.00210982291399397\n",
            "train loss:0.0021093028366163183\n",
            "train loss:0.0041632062197371955\n",
            "train loss:0.005952687246116489\n",
            "train loss:0.0003822162448286584\n",
            "train loss:0.00029792346782855683\n",
            "train loss:0.00093215536131321\n",
            "train loss:4.26410629095097e-05\n",
            "train loss:0.0007290435963534232\n",
            "train loss:0.007911624932009496\n",
            "train loss:0.0007347589724284316\n",
            "train loss:0.00227306534352634\n",
            "train loss:0.004348689636920571\n",
            "train loss:0.00018004413883137448\n",
            "train loss:0.00268358184141745\n",
            "train loss:0.0071348435913978535\n",
            "train loss:0.0031437321958291413\n",
            "train loss:0.0035127216578385106\n",
            "train loss:0.0014196659036113054\n",
            "train loss:0.0012144244311245052\n",
            "train loss:0.0005106099726544552\n",
            "train loss:7.614104333890701e-05\n",
            "train loss:0.0030692236789199036\n",
            "train loss:0.003264087029149681\n",
            "train loss:0.003208736109363193\n",
            "train loss:0.010445707559980415\n",
            "train loss:0.0008858559617440204\n",
            "train loss:0.0008143702123411207\n",
            "train loss:0.00030163034147636384\n",
            "train loss:0.0005322224848789165\n",
            "train loss:0.00010665422862341599\n",
            "train loss:0.001959956292722495\n",
            "train loss:0.005524608616508514\n",
            "train loss:0.00025483284022155545\n",
            "train loss:0.00026634247959813473\n",
            "train loss:0.0003260864737360325\n",
            "train loss:0.0016567369456746883\n",
            "train loss:0.0011923509123503687\n",
            "train loss:0.00010604476532556168\n",
            "train loss:0.0018225847418928199\n",
            "train loss:0.01801363080863097\n",
            "train loss:0.01849764826127448\n",
            "train loss:0.0009226749273682655\n",
            "train loss:0.00016664190471714515\n",
            "train loss:0.026309491543705977\n",
            "train loss:0.010135418231250349\n",
            "train loss:0.007300095880857432\n",
            "train loss:0.001264611086402961\n",
            "train loss:0.0018146972415450718\n",
            "train loss:0.0004664365582065485\n",
            "train loss:0.00013640839632899444\n",
            "train loss:0.0056926943614251015\n",
            "train loss:0.000914460189367491\n",
            "train loss:0.00324506772853819\n",
            "train loss:0.002725110791928464\n",
            "train loss:0.0038056608391666097\n",
            "train loss:0.0033501348913788794\n",
            "train loss:0.0017465290657493358\n",
            "train loss:0.0006439452345495405\n",
            "train loss:0.0031969510260699734\n",
            "train loss:5.5769734580725947e-05\n",
            "train loss:0.0022857789763821496\n",
            "train loss:0.0029776455098237615\n",
            "train loss:0.003962156111077035\n",
            "train loss:0.003186093429631073\n",
            "train loss:0.0015613476785714276\n",
            "train loss:0.002187944770179909\n",
            "train loss:0.004571327697868912\n",
            "train loss:0.0004166075542449238\n",
            "train loss:0.0013660894221577936\n",
            "train loss:0.00028546621219651785\n",
            "train loss:0.008917576779107811\n",
            "train loss:0.004391465226779998\n",
            "train loss:0.0006683584276444818\n",
            "train loss:0.0006770507191670869\n",
            "train loss:0.0015807891563307597\n",
            "train loss:0.002235526611506636\n",
            "train loss:0.0011270010987143226\n",
            "train loss:0.005095521530708954\n",
            "train loss:0.0013381352150590112\n",
            "train loss:0.00033889479320179554\n",
            "train loss:0.00817230818492699\n",
            "train loss:0.0005493395287804336\n",
            "train loss:0.004383985197237816\n",
            "train loss:0.0034107544997091016\n",
            "train loss:0.0026376763320373336\n",
            "train loss:0.0009970968245811933\n",
            "train loss:0.002178625858067736\n",
            "train loss:0.0065613854754239045\n",
            "train loss:0.00039049667149215544\n",
            "train loss:0.004920092059726441\n",
            "train loss:0.0010573776860083636\n",
            "train loss:0.00605487211611864\n",
            "train loss:0.00263569573086538\n",
            "train loss:0.0028554496151174773\n",
            "train loss:0.0003261512632912025\n",
            "train loss:0.02504539225437235\n",
            "train loss:0.0028218355276710206\n",
            "train loss:0.005412241306065504\n",
            "train loss:0.0022570931731091728\n",
            "train loss:0.0017157362712507686\n",
            "train loss:0.0014681219559980582\n",
            "train loss:0.0030877012966003116\n",
            "train loss:0.001064120625596882\n",
            "train loss:0.001478701747731849\n",
            "train loss:0.0018644901387375663\n",
            "train loss:0.0013776080324299105\n",
            "train loss:0.0006988554676037334\n",
            "train loss:0.004296553861182845\n",
            "train loss:0.0015277727591359149\n",
            "train loss:2.9379761550333582e-05\n",
            "train loss:0.0011179213553165887\n",
            "train loss:0.001983864970460223\n",
            "train loss:0.019726804534813517\n",
            "train loss:0.001578336844608511\n",
            "train loss:0.00661579770096776\n",
            "train loss:0.0006696881151019803\n",
            "train loss:0.0002794324969851588\n",
            "train loss:0.0018963090555961925\n",
            "train loss:0.0023924524086423276\n",
            "train loss:0.004417930995047812\n",
            "train loss:0.0006938464080548977\n",
            "train loss:0.0020946068443484455\n",
            "train loss:0.004991564984898274\n",
            "train loss:0.0010878969207480634\n",
            "train loss:0.0016311071517735524\n",
            "train loss:0.002411441098232398\n",
            "train loss:0.00028217769653467596\n",
            "train loss:0.022159069803355993\n",
            "train loss:0.0019935008126924602\n",
            "train loss:0.004636998519512662\n",
            "train loss:0.0012015965993443802\n",
            "train loss:0.0005102066766219871\n",
            "train loss:7.979846008979403e-05\n",
            "train loss:0.00026506722817140274\n",
            "train loss:0.0020475301302410953\n",
            "train loss:0.0040623088209675575\n",
            "train loss:0.001800796982142441\n",
            "train loss:0.0021210187444153524\n",
            "train loss:0.001718853545420623\n",
            "train loss:0.0015445096181821432\n",
            "train loss:0.000844705426132722\n",
            "train loss:0.0008719829128243342\n",
            "train loss:0.0014586970657239635\n",
            "train loss:0.002074929145392304\n",
            "train loss:0.03612147613940663\n",
            "train loss:0.004686125034142534\n",
            "train loss:0.00015622058903369253\n",
            "train loss:0.0005436001215371178\n",
            "train loss:0.0024579392877131993\n",
            "train loss:0.0053970795177566665\n",
            "train loss:0.0020245334074591582\n",
            "train loss:0.006513610025394376\n",
            "train loss:0.00400956842215716\n",
            "train loss:0.001224309279662919\n",
            "train loss:0.005332341903401833\n",
            "train loss:0.000507017441278274\n",
            "train loss:4.937113417174868e-05\n",
            "train loss:0.0009290643381872985\n",
            "train loss:8.287177388286143e-05\n",
            "train loss:0.000720959930372828\n",
            "train loss:0.000115777034940056\n",
            "train loss:0.0013984680082198048\n",
            "train loss:0.006339579149560423\n",
            "train loss:0.005634195515250153\n",
            "train loss:0.002316774013674251\n",
            "train loss:0.0035378190431306876\n",
            "train loss:0.0011455917594956487\n",
            "train loss:0.003202364196244938\n",
            "train loss:9.453334065793595e-05\n",
            "train loss:0.0017023831899020067\n",
            "train loss:0.001042781393209715\n",
            "train loss:0.00043970522385160645\n",
            "train loss:0.000603292677972363\n",
            "train loss:0.005187452180229028\n",
            "train loss:0.0010320128576107755\n",
            "train loss:0.004589922317907675\n",
            "train loss:0.003812096841417976\n",
            "train loss:0.00846690349177292\n",
            "train loss:0.0024361762697870885\n",
            "train loss:0.004548763702794648\n",
            "train loss:0.000252183748840069\n",
            "train loss:0.001783805506938018\n",
            "train loss:0.0061019785100698935\n",
            "train loss:0.00042916638306938413\n",
            "train loss:0.0026848060659425952\n",
            "train loss:0.0007801403960160065\n",
            "train loss:0.007789862728063448\n",
            "train loss:0.00017287885854312394\n",
            "train loss:0.005341416706667783\n",
            "train loss:0.0014325843837109306\n",
            "train loss:0.0005324515203622795\n",
            "train loss:0.030223917043863943\n",
            "train loss:0.001997377088640601\n",
            "train loss:0.00029905371857663225\n",
            "train loss:0.0012507543092723549\n",
            "train loss:0.0020827788980429455\n",
            "train loss:0.000537865302146372\n",
            "train loss:0.0010095317719476555\n",
            "train loss:0.004862448551723608\n",
            "train loss:0.0019719729344756663\n",
            "train loss:0.00544056471200768\n",
            "train loss:0.00019297564570762356\n",
            "train loss:0.005657182481251935\n",
            "train loss:0.0020494465911828287\n",
            "train loss:0.0005756313081801456\n",
            "train loss:0.0015103059547833155\n",
            "train loss:0.00207930116261145\n",
            "train loss:0.003047245702918927\n",
            "train loss:0.0008582667847320581\n",
            "train loss:0.0017445557839175605\n",
            "train loss:0.0013500210039065785\n",
            "train loss:0.003661867538360159\n",
            "train loss:0.0035157486267212257\n",
            "train loss:0.00363707964701341\n",
            "train loss:0.0009187253827008031\n",
            "train loss:0.0010549398940887399\n",
            "train loss:0.0005009855280074258\n",
            "train loss:0.00907115478540701\n",
            "train loss:0.0015216998490558164\n",
            "train loss:0.0007926858062009435\n",
            "train loss:0.002490623444409086\n",
            "train loss:0.0009131692750252376\n",
            "train loss:0.0005996838980047405\n",
            "train loss:0.0002231214158703938\n",
            "train loss:0.002768798226391941\n",
            "train loss:0.00026809637610928304\n",
            "train loss:0.000461611010631785\n",
            "train loss:0.0006192534620016874\n",
            "train loss:0.0002782164752203145\n",
            "train loss:0.0006272373633511297\n",
            "train loss:0.0003906547666846227\n",
            "train loss:0.0007422178272632513\n",
            "train loss:0.005144511723472395\n",
            "train loss:0.0017047787219956056\n",
            "train loss:0.0004225432483539992\n",
            "train loss:0.005670440732607278\n",
            "train loss:0.0074397545059031265\n",
            "train loss:0.003334079247262149\n",
            "train loss:0.0005212976191227804\n",
            "train loss:0.0020683079489755797\n",
            "train loss:0.001807724643534773\n",
            "train loss:0.02272492547690206\n",
            "train loss:0.0013228048307960647\n",
            "=== epoch:16, train acc:0.997, test acc:0.987 ===\n",
            "train loss:0.0052978363233105695\n",
            "train loss:0.00208938041321027\n",
            "train loss:0.0070626343399276224\n",
            "train loss:0.001460442996330486\n",
            "train loss:0.0005855013063227503\n",
            "train loss:0.002100500636837509\n",
            "train loss:0.0017604840980135574\n",
            "train loss:0.0009776255929776453\n",
            "train loss:0.006479064573004653\n",
            "train loss:0.0003016595707081663\n",
            "train loss:0.0044154086349053\n",
            "train loss:0.0022180079221941025\n",
            "train loss:0.00204753829080984\n",
            "train loss:0.0012853056426869029\n",
            "train loss:0.001131777645963235\n",
            "train loss:0.007474195168169472\n",
            "train loss:0.0008594581522961231\n",
            "train loss:0.0016958458694016883\n",
            "train loss:0.0013930279301163837\n",
            "train loss:0.003102411113274745\n",
            "train loss:0.002470905109517993\n",
            "train loss:0.00032192035502713335\n",
            "train loss:0.002170151260265338\n",
            "train loss:0.0010447272800012837\n",
            "train loss:0.00016742702791579\n",
            "train loss:0.0029045577051104625\n",
            "train loss:0.002258504623923029\n",
            "train loss:0.0005348563271871695\n",
            "train loss:0.002126409716844069\n",
            "train loss:0.0013304673526640443\n",
            "train loss:0.002668857171867579\n",
            "train loss:0.0021929160083778293\n",
            "train loss:0.0016713020023747852\n",
            "train loss:0.007797864787372832\n",
            "train loss:0.003920866198663161\n",
            "train loss:0.0009506395985232691\n",
            "train loss:0.006229356694087518\n",
            "train loss:0.021707540897764357\n",
            "train loss:0.00228176396485962\n",
            "train loss:0.0015384370214701277\n",
            "train loss:0.003136822640544084\n",
            "train loss:0.0005096106252615581\n",
            "train loss:0.0024859760824572333\n",
            "train loss:0.0013448219510847187\n",
            "train loss:0.008704550936114311\n",
            "train loss:0.00011172650982849982\n",
            "train loss:0.0041975073969787585\n",
            "train loss:0.011357885428805501\n",
            "train loss:0.00260532992521008\n",
            "train loss:0.0243712244191009\n",
            "train loss:0.0005888240528794933\n",
            "train loss:0.001693637936428634\n",
            "train loss:5.9693775055344996e-05\n",
            "train loss:0.008742448597921244\n",
            "train loss:0.016534613523022542\n",
            "train loss:0.034683185918624584\n",
            "train loss:0.0011978779792538904\n",
            "train loss:0.00045708359969918457\n",
            "train loss:0.00034343814605992793\n",
            "train loss:0.004301352346953978\n",
            "train loss:0.036732576743091695\n",
            "train loss:0.0021584660182999245\n",
            "train loss:0.0005965275332846635\n",
            "train loss:0.001017996913822087\n",
            "train loss:0.001397658775898628\n",
            "train loss:0.002355031388537236\n",
            "train loss:0.002004102900174427\n",
            "train loss:0.0005319411083791526\n",
            "train loss:0.002943602680884328\n",
            "train loss:0.024244737515337903\n",
            "train loss:6.912250725539252e-05\n",
            "train loss:0.001983609288988157\n",
            "train loss:0.01942257593167831\n",
            "train loss:0.004100851791097781\n",
            "train loss:0.05632150980381479\n",
            "train loss:0.0020864278763399855\n",
            "train loss:0.001186966329242692\n",
            "train loss:0.003332495610835796\n",
            "train loss:0.001110913481444408\n",
            "train loss:3.4619013370221964e-05\n",
            "train loss:0.0020543234594471967\n",
            "train loss:0.0005692156451127147\n",
            "train loss:0.014559182890920529\n",
            "train loss:0.006956411502523265\n",
            "train loss:0.00177433366768551\n",
            "train loss:0.0059440426117221045\n",
            "train loss:0.0008410353589385783\n",
            "train loss:0.005910045525113383\n",
            "train loss:0.003415397434228463\n",
            "train loss:0.003661247722248959\n",
            "train loss:0.0007498265914029564\n",
            "train loss:0.018049032281609253\n",
            "train loss:0.02184080878207495\n",
            "train loss:0.005801571882987765\n",
            "train loss:0.0025530095062092028\n",
            "train loss:0.0005031716240951353\n",
            "train loss:0.009964026664587938\n",
            "train loss:0.0003469370374562143\n",
            "train loss:0.0005125068582279091\n",
            "train loss:0.0027508049267956094\n",
            "train loss:0.00048600078979640427\n",
            "train loss:0.0036061906803336436\n",
            "train loss:0.004494384810733698\n",
            "train loss:0.0004358985954500793\n",
            "train loss:5.972134723156533e-05\n",
            "train loss:0.0015984216012860414\n",
            "train loss:0.001125794582574455\n",
            "train loss:0.00045763891326462686\n",
            "train loss:0.0006712429528207323\n",
            "train loss:0.0015647608268276108\n",
            "train loss:9.178547526225917e-05\n",
            "train loss:0.004664569250217602\n",
            "train loss:0.0014642009883657743\n",
            "train loss:0.0005649937753739961\n",
            "train loss:0.0009617755894457739\n",
            "train loss:0.00013321013867515196\n",
            "train loss:0.004632113556363243\n",
            "train loss:0.0022120155205422294\n",
            "train loss:0.011394435892768675\n",
            "train loss:0.004984641989050159\n",
            "train loss:0.00021165979473233138\n",
            "train loss:0.004316581000502049\n",
            "train loss:0.001528958123448193\n",
            "train loss:0.0007515645302890887\n",
            "train loss:0.0012066247121415392\n",
            "train loss:0.013006950673683213\n",
            "train loss:0.0006243428180937622\n",
            "train loss:0.0009104568957757993\n",
            "train loss:0.03052834760772943\n",
            "train loss:0.0006480919121981612\n",
            "train loss:0.003595053442046733\n",
            "train loss:0.0029019586091713146\n",
            "train loss:0.010726706245924861\n",
            "train loss:0.0019093465142232626\n",
            "train loss:0.005930622489395128\n",
            "train loss:0.0005113210425378716\n",
            "train loss:0.005412661584107371\n",
            "train loss:0.002439705134253945\n",
            "train loss:0.001578293038107331\n",
            "train loss:0.0030908638071180635\n",
            "train loss:0.0022165072348224886\n",
            "train loss:0.0038090377677791128\n",
            "train loss:0.0002994170911692293\n",
            "train loss:0.006754093078840017\n",
            "train loss:0.0038478897950265634\n",
            "train loss:0.008916474531595832\n",
            "train loss:0.0036079506360972256\n",
            "train loss:0.0001215203957497083\n",
            "train loss:0.0013721694395418154\n",
            "train loss:0.0011442089168533735\n",
            "train loss:0.0006813753093279179\n",
            "train loss:0.0008538089415115911\n",
            "train loss:0.0009941593522822313\n",
            "train loss:0.0003074880079897283\n",
            "train loss:0.0033143803480566313\n",
            "train loss:0.0016136803480568923\n",
            "train loss:0.00022725936984238426\n",
            "train loss:0.002919746369501057\n",
            "train loss:0.00043499147386887537\n",
            "train loss:0.0011957482935129678\n",
            "train loss:0.000964889355441817\n",
            "train loss:0.0014759327573826194\n",
            "train loss:0.002639368075928304\n",
            "train loss:0.0015810338165630565\n",
            "train loss:0.0019782040799094062\n",
            "train loss:0.0058065885186994225\n",
            "train loss:0.0009615940086902376\n",
            "train loss:0.003423641776747215\n",
            "train loss:0.000858839033614531\n",
            "train loss:0.00040105773017178807\n",
            "train loss:0.0008834407546880639\n",
            "train loss:0.0016194824713814601\n",
            "train loss:0.000463978390475472\n",
            "train loss:0.0005062054603360156\n",
            "train loss:0.00012134664018246054\n",
            "train loss:0.00020644782771521544\n",
            "train loss:0.00354005861557737\n",
            "train loss:0.001853052810528881\n",
            "train loss:0.002100402336637532\n",
            "train loss:0.003811205400028211\n",
            "train loss:0.054260236102937516\n",
            "train loss:0.002739938805772989\n",
            "train loss:0.0007155928780990295\n",
            "train loss:0.00030379486123654885\n",
            "train loss:0.003225036660150145\n",
            "train loss:0.00018186403751871416\n",
            "train loss:0.0028769564013032085\n",
            "train loss:0.012227076758224084\n",
            "train loss:0.0004644188135139372\n",
            "train loss:0.006115189669883848\n",
            "train loss:0.003181883326941452\n",
            "train loss:0.0012890728753064404\n",
            "train loss:0.0008364683231600214\n",
            "train loss:0.005720937884448602\n",
            "train loss:0.003561616524055477\n",
            "train loss:0.0005292243159233127\n",
            "train loss:0.011291597479574119\n",
            "train loss:0.000281415785919712\n",
            "train loss:0.0010383855083839135\n",
            "train loss:0.0034592341728182092\n",
            "train loss:0.0048958849103342755\n",
            "train loss:0.00353487391646336\n",
            "train loss:0.0013030210014594987\n",
            "train loss:0.0015466006602785334\n",
            "train loss:0.003999251875963933\n",
            "train loss:0.014225612279429454\n",
            "train loss:0.00032356244946941015\n",
            "train loss:0.0014381021831498648\n",
            "train loss:0.005943191732885238\n",
            "train loss:0.004270562292017343\n",
            "train loss:0.0027642855256381593\n",
            "train loss:0.0038734310815242033\n",
            "train loss:0.0031180215284520073\n",
            "train loss:0.001697783318379608\n",
            "train loss:0.0009189501300679352\n",
            "train loss:0.002614666006920272\n",
            "train loss:0.006198824540201614\n",
            "train loss:0.0020867404117183345\n",
            "train loss:0.004377869329684715\n",
            "train loss:0.000146923262524941\n",
            "train loss:0.00043839220852862613\n",
            "train loss:0.005790209130148403\n",
            "train loss:0.0013369887318236662\n",
            "train loss:0.0010635842441158718\n",
            "train loss:0.0003252618119800559\n",
            "train loss:0.0025688153388605233\n",
            "train loss:0.001494760174642878\n",
            "train loss:0.004970277094592737\n",
            "train loss:0.0037224482404907256\n",
            "train loss:0.010904407641324892\n",
            "train loss:0.0023939039738964506\n",
            "train loss:0.00035345592663318487\n",
            "train loss:0.0042773972574370635\n",
            "train loss:0.0008271164226282596\n",
            "train loss:0.0039303263189980914\n",
            "train loss:0.0025880307495058765\n",
            "train loss:0.003672524045807321\n",
            "train loss:0.0038414193271626156\n",
            "train loss:0.0006136081818779305\n",
            "train loss:0.004892231715357735\n",
            "train loss:0.0031131029861308644\n",
            "train loss:0.0012062421138822029\n",
            "train loss:0.0017432751156826096\n",
            "train loss:0.01813613511514118\n",
            "train loss:0.000164822990949889\n",
            "train loss:0.0016808822273012291\n",
            "train loss:0.007133184967786567\n",
            "train loss:0.0005669956659057006\n",
            "train loss:0.00400679150818539\n",
            "train loss:0.002344312794196515\n",
            "train loss:0.001942795733593356\n",
            "train loss:0.0029180810157478925\n",
            "train loss:0.0062944940843979134\n",
            "train loss:0.0008509638344851502\n",
            "train loss:0.0019022763548126231\n",
            "train loss:0.006397562079800262\n",
            "train loss:0.0009130378197056258\n",
            "train loss:0.00015852521482301234\n",
            "train loss:0.003914014217817019\n",
            "train loss:0.0009015103417615889\n",
            "train loss:0.0016559884889995923\n",
            "train loss:0.0005791352727280969\n",
            "train loss:0.0013695137841006689\n",
            "train loss:0.0013104765239170416\n",
            "train loss:0.0005964790224164718\n",
            "train loss:0.0014756969269248508\n",
            "train loss:0.003980079235614911\n",
            "train loss:0.01675762565099398\n",
            "train loss:0.0003230960689574135\n",
            "train loss:0.012848228141962474\n",
            "train loss:0.04752684625119943\n",
            "train loss:0.00011492914854944917\n",
            "train loss:0.004254807824604404\n",
            "train loss:0.0008878815520273561\n",
            "train loss:0.004887542008403806\n",
            "train loss:0.0033497128782338215\n",
            "train loss:0.001773750193466127\n",
            "train loss:0.008808491433929522\n",
            "train loss:0.005539789857153841\n",
            "train loss:0.0017828247674070133\n",
            "train loss:0.00019703251456961558\n",
            "train loss:0.0006021026803815094\n",
            "train loss:0.001560924640659971\n",
            "train loss:0.0013093849033937264\n",
            "train loss:0.0015933991948194172\n",
            "train loss:0.0007215865830575919\n",
            "train loss:0.0014406116546780015\n",
            "train loss:0.00045679185444707333\n",
            "train loss:0.0016535341799913813\n",
            "train loss:7.945154151713931e-05\n",
            "train loss:0.00016238361822005958\n",
            "train loss:0.002935727209876195\n",
            "train loss:0.002252884708457862\n",
            "train loss:0.002514566050674173\n",
            "train loss:0.0006423027116472849\n",
            "train loss:0.0008133240795340142\n",
            "train loss:0.007729776753055314\n",
            "train loss:0.0005440695728012288\n",
            "train loss:0.0021381117832115983\n",
            "train loss:0.005910627755564121\n",
            "train loss:0.0010523227661587793\n",
            "train loss:0.0003340477498701573\n",
            "train loss:0.0002491260167304328\n",
            "train loss:0.00489389512639475\n",
            "train loss:0.0006191114614926734\n",
            "train loss:0.0009229641017223258\n",
            "train loss:0.0014788391872456164\n",
            "train loss:0.002977926886785091\n",
            "train loss:0.0006577072353563232\n",
            "train loss:0.0009620100464997137\n",
            "train loss:0.0006641291713176807\n",
            "train loss:0.001832734279062155\n",
            "train loss:0.003535774493543115\n",
            "train loss:0.002626469990904055\n",
            "train loss:0.0010448191585111745\n",
            "train loss:0.001651985857225552\n",
            "train loss:0.0005799674222744092\n",
            "train loss:0.0030887888493743125\n",
            "train loss:0.002431724424479676\n",
            "train loss:0.0010458226078760417\n",
            "train loss:0.00024835023051994633\n",
            "train loss:0.001625378837729452\n",
            "train loss:0.00927397372061418\n",
            "train loss:0.0005884586822599224\n",
            "train loss:0.0059518652191232115\n",
            "train loss:0.0050231717854642595\n",
            "train loss:0.0023666677186353607\n",
            "train loss:0.0064059561599139085\n",
            "train loss:0.0024359337279088785\n",
            "train loss:0.0013318020026846123\n",
            "train loss:0.008353223134812628\n",
            "train loss:0.00011261132177728598\n",
            "train loss:0.0004517140656173657\n",
            "train loss:0.00027805431067380587\n",
            "train loss:0.0034294032612689446\n",
            "train loss:0.0021522893946809784\n",
            "train loss:0.0013930253412764062\n",
            "train loss:0.00011243688553182147\n",
            "train loss:0.003736747665617149\n",
            "train loss:0.0011378784230669657\n",
            "train loss:8.252792246782958e-05\n",
            "train loss:0.001181150605222359\n",
            "train loss:0.0007153518545480019\n",
            "train loss:0.001205546618336008\n",
            "train loss:0.0002044017863096218\n",
            "train loss:0.003404778544326879\n",
            "train loss:0.0005891821522884044\n",
            "train loss:0.0021187482503300906\n",
            "train loss:0.0011503752377511076\n",
            "train loss:0.0024862263395092465\n",
            "train loss:4.7787744415395735e-05\n",
            "train loss:0.0006321622463004929\n",
            "train loss:9.96548396564448e-05\n",
            "train loss:0.00021060899382328947\n",
            "train loss:0.00015642029565214973\n",
            "train loss:0.00698280606890981\n",
            "train loss:0.0006147055146669532\n",
            "train loss:0.0004093406942830914\n",
            "train loss:0.002786353454215359\n",
            "train loss:0.002758927018120848\n",
            "train loss:0.0007753711984067684\n",
            "train loss:0.004061836868353719\n",
            "train loss:0.00022417677371021808\n",
            "train loss:0.00480749162647035\n",
            "train loss:0.0009273561721957173\n",
            "train loss:0.007723685019991022\n",
            "train loss:0.0016317437359051688\n",
            "train loss:0.001731133118531396\n",
            "train loss:0.0011200434905456913\n",
            "train loss:0.0017318748334539946\n",
            "train loss:0.0024361689858604014\n",
            "train loss:0.0005490207119970462\n",
            "train loss:0.0008163933898972543\n",
            "train loss:0.00037406865547976885\n",
            "train loss:0.00036934696987627976\n",
            "train loss:0.00018259567017112402\n",
            "train loss:0.012083278453874826\n",
            "train loss:0.0018275435929303125\n",
            "train loss:0.0023452445724789594\n",
            "train loss:0.004504783998574097\n",
            "train loss:0.002761057509945575\n",
            "train loss:0.0027558948990921593\n",
            "train loss:0.001965165989428937\n",
            "train loss:9.731194794969602e-05\n",
            "train loss:0.0007394532714193983\n",
            "train loss:0.0009898913996849253\n",
            "train loss:0.0002711839494931458\n",
            "train loss:0.00010287028950108376\n",
            "train loss:0.0009496073115139285\n",
            "train loss:0.000416350765551808\n",
            "train loss:0.00043109744389192384\n",
            "train loss:0.00040787589655280393\n",
            "train loss:0.0021808789796988544\n",
            "train loss:0.0002220527923347832\n",
            "train loss:0.0008460438913408907\n",
            "train loss:0.012080871261694642\n",
            "train loss:0.00015603162131856994\n",
            "train loss:0.0013062657253344553\n",
            "train loss:0.0025762311413948897\n",
            "train loss:0.0019171435240460724\n",
            "train loss:0.0009548066731885813\n",
            "train loss:0.00038736385745527\n",
            "train loss:0.0015022569242648476\n",
            "train loss:0.0001781672932338411\n",
            "train loss:0.0007263614998332286\n",
            "train loss:0.00046709511094867375\n",
            "train loss:0.0030422180755284283\n",
            "train loss:0.00018887425090711375\n",
            "train loss:0.0005804986198910255\n",
            "train loss:0.00111168602243277\n",
            "train loss:0.0028148189923141442\n",
            "train loss:0.00014312549678391397\n",
            "train loss:0.00016994444562315163\n",
            "train loss:0.0136286676507047\n",
            "train loss:0.0019868802013087625\n",
            "train loss:0.001398462006376889\n",
            "train loss:0.0016817982904606319\n",
            "train loss:0.0003183365434747738\n",
            "train loss:0.0003090034258410105\n",
            "train loss:0.0004769348193412993\n",
            "train loss:0.0017244281431455502\n",
            "train loss:0.002126841899248344\n",
            "train loss:0.0001850937884342571\n",
            "train loss:0.000944908941835535\n",
            "train loss:0.004795123773658895\n",
            "train loss:0.000253467183588959\n",
            "train loss:0.0007432892858157556\n",
            "train loss:0.001276668859574186\n",
            "train loss:0.011453812148382567\n",
            "train loss:0.0004711669467340094\n",
            "train loss:0.000523640471417316\n",
            "train loss:0.00016533502393217532\n",
            "train loss:0.0015236502708372718\n",
            "train loss:0.00037572837864950854\n",
            "train loss:0.0002562096558768089\n",
            "train loss:0.0015021983111746517\n",
            "train loss:0.0007476923780428918\n",
            "train loss:0.0005400300080615756\n",
            "train loss:0.002035335590897565\n",
            "train loss:0.00072585644384107\n",
            "train loss:0.0001018999320205074\n",
            "train loss:0.0001047272362905218\n",
            "train loss:0.001815271791625859\n",
            "train loss:0.00025141068849023697\n",
            "train loss:0.002703195634986734\n",
            "train loss:0.0007612061386967955\n",
            "train loss:0.002949577023257345\n",
            "train loss:0.0010459302781256868\n",
            "train loss:0.018457568280647665\n",
            "train loss:0.00029022911917873803\n",
            "train loss:0.0004818884259508032\n",
            "train loss:0.0007096360455528729\n",
            "train loss:0.00032070993163078876\n",
            "train loss:0.0012514755828977795\n",
            "train loss:0.019379029545380407\n",
            "train loss:0.003820020538938356\n",
            "train loss:0.00132547604634462\n",
            "train loss:0.000673967314218318\n",
            "train loss:0.003176977363550287\n",
            "train loss:0.00526105866980154\n",
            "train loss:0.0001681072866932845\n",
            "train loss:0.005556511719029565\n",
            "train loss:0.002773757859734171\n",
            "train loss:0.0005401425970556294\n",
            "train loss:0.01315298109381633\n",
            "train loss:0.002029739733751182\n",
            "train loss:0.0004078039687211093\n",
            "train loss:0.001874062325304244\n",
            "train loss:0.0009277815271086245\n",
            "train loss:0.00042665632173546886\n",
            "train loss:0.00075592892044248\n",
            "train loss:0.0006173080722542344\n",
            "train loss:0.0004014389414528643\n",
            "train loss:0.0005774632677942839\n",
            "train loss:0.0020038615555439208\n",
            "train loss:0.001141923890429471\n",
            "train loss:0.0003123965262066899\n",
            "train loss:0.0006267736988966799\n",
            "train loss:0.006962488834633331\n",
            "train loss:0.0006424993347040191\n",
            "train loss:0.0030689348494340627\n",
            "train loss:0.002155200729229545\n",
            "train loss:0.0051061154462017895\n",
            "train loss:0.00026014410700809105\n",
            "train loss:0.0009596291377770702\n",
            "train loss:0.0073526105228571255\n",
            "train loss:0.001233081703566538\n",
            "train loss:0.0005475878036913732\n",
            "train loss:0.0003789292637208724\n",
            "train loss:0.0003539253940753304\n",
            "train loss:0.00023430216398543806\n",
            "train loss:0.000983162837765064\n",
            "train loss:0.001102969688173273\n",
            "train loss:0.0009452049380189445\n",
            "train loss:0.0020171008293628726\n",
            "train loss:8.221800397437102e-05\n",
            "train loss:0.004303378161244386\n",
            "train loss:0.004522655172802535\n",
            "train loss:0.0011051760286060958\n",
            "train loss:0.0010332890624095699\n",
            "train loss:0.00046949377504111156\n",
            "train loss:0.00014904628884708453\n",
            "train loss:0.0020108472867419297\n",
            "train loss:0.019800706801325597\n",
            "train loss:0.0011209343342287496\n",
            "train loss:0.00011426789634690147\n",
            "train loss:0.0016484069942103586\n",
            "train loss:0.0014009132469709029\n",
            "train loss:0.0011509713284015182\n",
            "train loss:0.0003177186093302551\n",
            "train loss:0.0004390463087431172\n",
            "train loss:0.004744790672721804\n",
            "train loss:0.0004655653060535837\n",
            "train loss:0.015697465948364873\n",
            "train loss:0.003210788570997703\n",
            "train loss:0.0006069728876176991\n",
            "train loss:0.0014539989807689558\n",
            "train loss:0.0019201056174458927\n",
            "train loss:0.0010250394827641975\n",
            "train loss:0.005683244318698352\n",
            "train loss:0.00013121767850558016\n",
            "train loss:0.008563745242452927\n",
            "train loss:0.0006010684350771341\n",
            "train loss:0.004841038925406789\n",
            "train loss:9.657628640804667e-05\n",
            "train loss:0.0056044805122798325\n",
            "train loss:0.0014859802183108304\n",
            "train loss:0.00019075540467377967\n",
            "train loss:7.958051847278987e-05\n",
            "train loss:0.003112999050267891\n",
            "train loss:0.0003664823975803048\n",
            "train loss:0.0006541846636000598\n",
            "train loss:0.0006000268603127165\n",
            "train loss:0.0001454302042264647\n",
            "train loss:0.018702772695599022\n",
            "train loss:0.014911572758622031\n",
            "train loss:0.0028651075814038578\n",
            "train loss:0.0015895660415967908\n",
            "train loss:0.0002126626996228217\n",
            "train loss:0.0002805451085046242\n",
            "train loss:0.0007497350978537952\n",
            "train loss:0.00015547433625685245\n",
            "train loss:0.0011129055892610763\n",
            "train loss:0.0007687731105539383\n",
            "train loss:0.0007563799436036794\n",
            "train loss:0.0023297018701231103\n",
            "train loss:0.0009758351694268463\n",
            "train loss:0.0014833793788260837\n",
            "train loss:0.001046162774451284\n",
            "train loss:0.00016540054881624327\n",
            "train loss:0.00039628498106933927\n",
            "train loss:0.006559430835603488\n",
            "train loss:0.0011596626396339711\n",
            "train loss:0.0009839600245330157\n",
            "train loss:0.002298508319823418\n",
            "train loss:0.0010367773243863134\n",
            "train loss:0.0028321808065153954\n",
            "train loss:0.00017631381925375125\n",
            "train loss:0.00021946419079683756\n",
            "train loss:0.00027642104670456724\n",
            "train loss:0.0003476531648947035\n",
            "train loss:0.0013084655917219362\n",
            "train loss:0.021886207355908974\n",
            "train loss:0.0010360870724105456\n",
            "train loss:0.0011910952090373867\n",
            "train loss:0.0004829896078123586\n",
            "train loss:0.001587635119525212\n",
            "train loss:0.0012651488275323883\n",
            "train loss:0.0005349106307361531\n",
            "train loss:9.970247754570097e-05\n",
            "train loss:0.0023671619247910116\n",
            "train loss:3.2505567981994026e-05\n",
            "train loss:0.0004977291319932165\n",
            "train loss:0.0009177201426812519\n",
            "train loss:0.002790049736943943\n",
            "train loss:0.005446442862609064\n",
            "train loss:0.0001893866161333543\n",
            "train loss:0.008951650380176736\n",
            "train loss:0.00031920849958272303\n",
            "train loss:0.00012560350437050858\n",
            "train loss:0.0011440251832912454\n",
            "train loss:0.002310021620346891\n",
            "train loss:0.001975195493168309\n",
            "train loss:0.00023091259175639904\n",
            "train loss:0.0020602664258281005\n",
            "train loss:0.001778212461939812\n",
            "train loss:0.0018146050377213945\n",
            "train loss:0.001344809189197798\n",
            "train loss:0.0010593523281422796\n",
            "train loss:0.0002889817568318022\n",
            "train loss:0.0011462726268266403\n",
            "train loss:0.0023759801381895702\n",
            "train loss:0.012618877233459056\n",
            "train loss:0.0017716328781201415\n",
            "train loss:9.9953178117964e-05\n",
            "train loss:0.0005743641716117589\n",
            "train loss:0.0003576110470077806\n",
            "train loss:0.0002820647886918738\n",
            "train loss:0.0003333698742817841\n",
            "train loss:0.0035085462433591215\n",
            "=== epoch:17, train acc:0.996, test acc:0.985 ===\n",
            "train loss:0.0009195597347289065\n",
            "train loss:0.0002987780929995581\n",
            "train loss:0.0003983470081593716\n",
            "train loss:0.0024056513776692045\n",
            "train loss:0.0007272426433048102\n",
            "train loss:0.0013617551637014349\n",
            "train loss:0.0002739023363437281\n",
            "train loss:0.0025241209221430554\n",
            "train loss:0.0007832164261730509\n",
            "train loss:0.004341206310906057\n",
            "train loss:0.0036942185732978445\n",
            "train loss:0.007207400357026779\n",
            "train loss:0.0009037011011961038\n",
            "train loss:0.0002583988261645918\n",
            "train loss:0.00027556392917179684\n",
            "train loss:0.0004036005892772265\n",
            "train loss:0.00015882283209471014\n",
            "train loss:0.001079379636304961\n",
            "train loss:0.0005130579517110466\n",
            "train loss:0.0013366086193795906\n",
            "train loss:0.0021858654832007858\n",
            "train loss:0.00934704794740258\n",
            "train loss:0.00017834944702157333\n",
            "train loss:3.805314152476774e-05\n",
            "train loss:0.0005401215390136365\n",
            "train loss:0.0009465585049108454\n",
            "train loss:0.0006374856473456438\n",
            "train loss:0.00011222583736150302\n",
            "train loss:0.0002108435513446204\n",
            "train loss:0.0002547936447500958\n",
            "train loss:0.0031555876585172317\n",
            "train loss:0.000230004822167066\n",
            "train loss:0.0002390520932911911\n",
            "train loss:0.0006050053082043827\n",
            "train loss:0.0006680041476298917\n",
            "train loss:0.0012471748851006644\n",
            "train loss:0.0009094769688264369\n",
            "train loss:0.00205936187953127\n",
            "train loss:0.000128772372511836\n",
            "train loss:0.0007524549341258989\n",
            "train loss:0.0004533381829700127\n",
            "train loss:0.000499480330072571\n",
            "train loss:0.0011665610042847039\n",
            "train loss:3.5389467778271965e-05\n",
            "train loss:0.003286449870163623\n",
            "train loss:0.003139475029701323\n",
            "train loss:3.596851917240031e-05\n",
            "train loss:0.00032067620533424396\n",
            "train loss:0.0018345489733754454\n",
            "train loss:0.0007928988385076466\n",
            "train loss:0.0005498035865001953\n",
            "train loss:0.0022665523527990216\n",
            "train loss:0.0003480823004135429\n",
            "train loss:0.00011090029478691105\n",
            "train loss:0.0007936034518291143\n",
            "train loss:0.0022476083562836817\n",
            "train loss:0.0014969269208231944\n",
            "train loss:0.00017938256994283125\n",
            "train loss:0.001168438247468511\n",
            "train loss:0.00025062101018574377\n",
            "train loss:0.0014446689328390832\n",
            "train loss:0.0005881390621777224\n",
            "train loss:0.000490140131292154\n",
            "train loss:0.00016989500910971966\n",
            "train loss:0.0004852130520943955\n",
            "train loss:0.0027504032226199136\n",
            "train loss:0.0010794722165989807\n",
            "train loss:0.0013692330476501708\n",
            "train loss:0.0003463217534511463\n",
            "train loss:0.0002005216672256888\n",
            "train loss:0.005671237302306048\n",
            "train loss:0.0025143022524394937\n",
            "train loss:0.0031427951243861696\n",
            "train loss:0.002790800015518057\n",
            "train loss:0.001015697814931544\n",
            "train loss:0.0009989999431259254\n",
            "train loss:0.0002545403518396381\n",
            "train loss:0.005797024216526312\n",
            "train loss:0.00156365990616265\n",
            "train loss:0.010278997499686365\n",
            "train loss:0.0005458088691818329\n",
            "train loss:0.00032112930481672174\n",
            "train loss:0.0003994084730043555\n",
            "train loss:0.007704657261131327\n",
            "train loss:0.0006696052904028232\n",
            "train loss:0.001815440441006562\n",
            "train loss:0.0026997660852208973\n",
            "train loss:2.493641283176917e-05\n",
            "train loss:0.00046161507178843107\n",
            "train loss:0.0016222032235645974\n",
            "train loss:0.0009139310500514129\n",
            "train loss:0.00019358211772127194\n",
            "train loss:4.203494680812798e-05\n",
            "train loss:0.0004154418432817733\n",
            "train loss:0.0006822580981042148\n",
            "train loss:0.0014936465610568548\n",
            "train loss:4.2056332505773395e-05\n",
            "train loss:0.00023846048109538534\n",
            "train loss:0.002127324111021675\n",
            "train loss:0.00016000239146679866\n",
            "train loss:0.00038119218955265865\n",
            "train loss:0.0003977201063580312\n",
            "train loss:0.00035704837236098493\n",
            "train loss:0.002145325163607592\n",
            "train loss:0.004050559921561298\n",
            "train loss:0.0014374786739392107\n",
            "train loss:0.0013841337867028358\n",
            "train loss:0.0024782413749637406\n",
            "train loss:7.378350990668928e-05\n",
            "train loss:0.0001009317153304352\n",
            "train loss:2.2267284736030305e-05\n",
            "train loss:0.0006817019576647667\n",
            "train loss:0.0004415615841532585\n",
            "train loss:0.0018253347052388654\n",
            "train loss:0.0013419418394334318\n",
            "train loss:0.0001271995825281291\n",
            "train loss:0.004002481252935898\n",
            "train loss:0.0008767370020667296\n",
            "train loss:0.00018799939865023815\n",
            "train loss:0.00014862704188653832\n",
            "train loss:2.322132148792702e-05\n",
            "train loss:0.015087190847204512\n",
            "train loss:0.0006220457794555647\n",
            "train loss:0.00026278482049642363\n",
            "train loss:0.0013405017844280217\n",
            "train loss:3.460736042816416e-05\n",
            "train loss:0.0020405309623764277\n",
            "train loss:0.00021321542601146831\n",
            "train loss:0.000897510314648447\n",
            "train loss:0.004333723582034325\n",
            "train loss:0.00018760576926170021\n",
            "train loss:0.010023114169420705\n",
            "train loss:0.0009303872790528088\n",
            "train loss:0.000682172448182134\n",
            "train loss:0.0019477745400795132\n",
            "train loss:0.00023464862498573098\n",
            "train loss:0.0005169824859292513\n",
            "train loss:4.9850532095718146e-05\n",
            "train loss:0.0020164433685268086\n",
            "train loss:0.0016130247171507314\n",
            "train loss:0.0002922164094735227\n",
            "train loss:0.0057885706470858275\n",
            "train loss:0.0025604234450224495\n",
            "train loss:0.0046802873733549424\n",
            "train loss:0.003527440525839981\n",
            "train loss:6.319644597057968e-05\n",
            "train loss:0.002913017451702895\n",
            "train loss:0.005652059407329027\n",
            "train loss:0.0004183778203450496\n",
            "train loss:0.00028429592017888963\n",
            "train loss:0.00369109065601456\n",
            "train loss:0.00012096417899242363\n",
            "train loss:0.004137565651518407\n",
            "train loss:0.001399720963481196\n",
            "train loss:0.0012523382721321874\n",
            "train loss:0.0034630731637698376\n",
            "train loss:0.002869078375866048\n",
            "train loss:0.0005337562700600132\n",
            "train loss:0.0006231623741492464\n",
            "train loss:0.007225808009867758\n",
            "train loss:0.009379642783035248\n",
            "train loss:0.0016819555916611356\n",
            "train loss:0.042408321418505694\n",
            "train loss:3.086699081117746e-05\n",
            "train loss:0.00027809407292878524\n",
            "train loss:0.0033918380072214204\n",
            "train loss:0.0017268520292189613\n",
            "train loss:0.0013670153285032086\n",
            "train loss:0.003078432404088633\n",
            "train loss:0.0009079838262242865\n",
            "train loss:0.000705031089112562\n",
            "train loss:0.00027035881080437563\n",
            "train loss:0.004330006620046847\n",
            "train loss:0.0001793889984129005\n",
            "train loss:0.0035915291769823255\n",
            "train loss:0.001249798735113713\n",
            "train loss:0.007069759598083603\n",
            "train loss:0.0020835392617244188\n",
            "train loss:0.004261354485823685\n",
            "train loss:0.002626442751336811\n",
            "train loss:0.0030170759569916176\n",
            "train loss:0.001086209482057267\n",
            "train loss:0.0011175513466777532\n",
            "train loss:0.003871971058723907\n",
            "train loss:0.0009359694871597485\n",
            "train loss:0.0016812018020724217\n",
            "train loss:0.001573047456034181\n",
            "train loss:0.0006818335779100842\n",
            "train loss:0.0007397181454217103\n",
            "train loss:0.0005837073389874216\n",
            "train loss:0.00015474513385048234\n",
            "train loss:0.0002303807994187196\n",
            "train loss:7.502916594165591e-05\n",
            "train loss:0.0022619644774800494\n",
            "train loss:0.0016131535979251582\n",
            "train loss:0.0250241214756444\n",
            "train loss:0.0015927954671678415\n",
            "train loss:0.0018348958664000166\n",
            "train loss:0.0009603582835216877\n",
            "train loss:0.014622883204302045\n",
            "train loss:0.0007919731303720475\n",
            "train loss:0.0015028874486656845\n",
            "train loss:0.00022843850208233575\n",
            "train loss:0.0003067849724078016\n",
            "train loss:0.002235865994875381\n",
            "train loss:0.004984525935344802\n",
            "train loss:0.0016677004168174458\n",
            "train loss:0.0005984189830316021\n",
            "train loss:0.000341920521804091\n",
            "train loss:0.006727178511544305\n",
            "train loss:0.0002345361916617577\n",
            "train loss:0.00023370582608915052\n",
            "train loss:0.0009900088460087573\n",
            "train loss:0.0014141694032919734\n",
            "train loss:0.0007386904129218569\n",
            "train loss:0.0010787031312225522\n",
            "train loss:0.0008010324103367167\n",
            "train loss:0.0017729531817435573\n",
            "train loss:0.0020225747308823773\n",
            "train loss:0.0002046042675635956\n",
            "train loss:0.0007443119521308444\n",
            "train loss:0.003638882789110444\n",
            "train loss:0.004917444949747517\n",
            "train loss:0.00011107831518825953\n",
            "train loss:0.0022481319081570396\n",
            "train loss:0.00048212593173275136\n",
            "train loss:0.0017422372658578821\n",
            "train loss:0.01178355980008441\n",
            "train loss:0.0014435391350487731\n",
            "train loss:0.0023574097283454547\n",
            "train loss:0.004064312014260073\n",
            "train loss:0.00047044844598476284\n",
            "train loss:0.0015770451110438397\n",
            "train loss:0.0005138596687238642\n",
            "train loss:0.0009169077437818973\n",
            "train loss:0.00048322352496605263\n",
            "train loss:9.614020746867102e-05\n",
            "train loss:0.0007948065646151269\n",
            "train loss:0.0012024571076601329\n",
            "train loss:0.00023989634840855507\n",
            "train loss:0.0004868173150245546\n",
            "train loss:0.0028864576773890115\n",
            "train loss:0.0004150159564740482\n",
            "train loss:0.00042829419795527795\n",
            "train loss:0.0046204015333915546\n",
            "train loss:0.0003038720705694016\n",
            "train loss:0.0003307045538649733\n",
            "train loss:0.0011951837741008213\n",
            "train loss:0.0008642488457512685\n",
            "train loss:0.0020915005949114743\n",
            "train loss:0.0009869412258605805\n",
            "train loss:0.014188419364047015\n",
            "train loss:0.000622851096679396\n",
            "train loss:0.0005694224529376655\n",
            "train loss:0.000949947838789172\n",
            "train loss:0.0004208228779823052\n",
            "train loss:0.001564869988646795\n",
            "train loss:3.1090868568179566e-06\n",
            "train loss:0.00013223426523245892\n",
            "train loss:4.560323719550597e-05\n",
            "train loss:0.0003640922086229124\n",
            "train loss:0.002270225058427031\n",
            "train loss:0.0025524114533151615\n",
            "train loss:0.0002299009830963117\n",
            "train loss:0.0008977989608267799\n",
            "train loss:0.0004160517418106167\n",
            "train loss:0.0005279855833947506\n",
            "train loss:0.002079755397672509\n",
            "train loss:0.00029051265262698917\n",
            "train loss:2.451414058287763e-05\n",
            "train loss:0.010824978099695608\n",
            "train loss:0.0009937190635713794\n",
            "train loss:0.0009366943560676307\n",
            "train loss:0.0002251276675354699\n",
            "train loss:6.221823400444358e-05\n",
            "train loss:0.00047903798909784847\n",
            "train loss:0.0013578336995604728\n",
            "train loss:0.0010278346840525785\n",
            "train loss:0.0006185998724666332\n",
            "train loss:0.0019690476502664212\n",
            "train loss:0.002764288318377949\n",
            "train loss:0.0012553153823846265\n",
            "train loss:0.0007449356384321754\n",
            "train loss:0.0010044108609836286\n",
            "train loss:0.0018989826924412979\n",
            "train loss:0.0007811455799887791\n",
            "train loss:0.0038710871520167556\n",
            "train loss:0.0010134121473367504\n",
            "train loss:0.000990629759213337\n",
            "train loss:0.00037010396208629463\n",
            "train loss:0.0016352123066195523\n",
            "train loss:0.0010648152845278557\n",
            "train loss:0.00041385978164069676\n",
            "train loss:0.001671277188914741\n",
            "train loss:0.000562577048264984\n",
            "train loss:0.0008238092056663618\n",
            "train loss:0.0008924949106454596\n",
            "train loss:0.0006559077351890155\n",
            "train loss:7.668758161565605e-05\n",
            "train loss:0.0002294542167017328\n",
            "train loss:0.0007020989976407685\n",
            "train loss:0.0028742426036732736\n",
            "train loss:0.0002877144174120767\n",
            "train loss:0.0014518939426467778\n",
            "train loss:0.00043099717058679764\n",
            "train loss:0.019858787059925227\n",
            "train loss:0.0007266406067601295\n",
            "train loss:0.00039137220224662523\n",
            "train loss:0.0006070631936062507\n",
            "train loss:0.0032638207017051412\n",
            "train loss:0.0010229884867860506\n",
            "train loss:0.0006792496069094243\n",
            "train loss:0.000665109842232829\n",
            "train loss:3.6709876314947246e-05\n",
            "train loss:0.003051087946959987\n",
            "train loss:0.011081628872662628\n",
            "train loss:9.669355605879594e-05\n",
            "train loss:9.17708867235578e-05\n",
            "train loss:0.0018764627683084054\n",
            "train loss:0.00026010154616069327\n",
            "train loss:6.593530586394014e-05\n",
            "train loss:0.00013128038552878446\n",
            "train loss:7.572313146671273e-05\n",
            "train loss:0.0011355941302020466\n",
            "train loss:0.00014018263939318722\n",
            "train loss:0.0004229388576337827\n",
            "train loss:0.00022923506481284977\n",
            "train loss:0.0004014727666366896\n",
            "train loss:0.002140655821281121\n",
            "train loss:0.0008485052695672107\n",
            "train loss:0.000656408613129737\n",
            "train loss:0.001642556783025631\n",
            "train loss:0.00032622922558761156\n",
            "train loss:0.0008695209072415459\n",
            "train loss:0.0006007025018679595\n",
            "train loss:0.0003231762094941955\n",
            "train loss:0.00033760265976896185\n",
            "train loss:0.0022627483744132003\n",
            "train loss:1.3086327227069337e-05\n",
            "train loss:0.0002937486105911621\n",
            "train loss:0.0020611558843775116\n",
            "train loss:0.001108210670917325\n",
            "train loss:0.003970021931366767\n",
            "train loss:0.0013970198092146334\n",
            "train loss:0.0016107935614315266\n",
            "train loss:0.0009192189789791972\n",
            "train loss:0.0034976368342052614\n",
            "train loss:0.0002582088374348943\n",
            "train loss:0.005980781940387637\n",
            "train loss:0.002026800699233665\n",
            "train loss:6.735035146045741e-05\n",
            "train loss:0.001237420047146983\n",
            "train loss:0.0009542264287198003\n",
            "train loss:0.0006137382472741666\n",
            "train loss:0.0007064316438240549\n",
            "train loss:0.0014475993728234895\n",
            "train loss:0.0013438750733667255\n",
            "train loss:0.0012793167495653828\n",
            "train loss:0.0031702506134987886\n",
            "train loss:0.0009412445790613528\n",
            "train loss:0.00048529498872617064\n",
            "train loss:8.667429303640648e-05\n",
            "train loss:9.730146347066906e-06\n",
            "train loss:0.000552492203133259\n",
            "train loss:0.00015233693411926936\n",
            "train loss:0.0016234781157570047\n",
            "train loss:0.004638270129655569\n",
            "train loss:0.001381070258405456\n",
            "train loss:0.0020105343854899074\n",
            "train loss:0.00040702431136052463\n",
            "train loss:0.0005396764680582275\n",
            "train loss:0.0014939891042861175\n",
            "train loss:0.001457348313870049\n",
            "train loss:0.00022983255177968031\n",
            "train loss:6.65367673944783e-05\n",
            "train loss:0.001219541309843206\n",
            "train loss:0.0002935889810542422\n",
            "train loss:0.0005971022297235035\n",
            "train loss:0.005340339881828704\n",
            "train loss:0.0010311795344070136\n",
            "train loss:0.00022013451616376394\n",
            "train loss:0.004399131434575863\n",
            "train loss:0.0005488612749929046\n",
            "train loss:0.004040804786321728\n",
            "train loss:0.002283501841581458\n",
            "train loss:0.003476686432520391\n",
            "train loss:0.00958157272740036\n",
            "train loss:0.0012344871853018598\n",
            "train loss:0.0002640441708218282\n",
            "train loss:5.377892082863973e-05\n",
            "train loss:0.0035129166352808024\n",
            "train loss:0.0026307488982882515\n",
            "train loss:0.05060545523587475\n",
            "train loss:0.0042856752657899275\n",
            "train loss:0.00010280961638056914\n",
            "train loss:0.0038043781018217266\n",
            "train loss:0.0022174817480010447\n",
            "train loss:0.001103706180516366\n",
            "train loss:0.0013413612222309167\n",
            "train loss:0.007612753345155831\n",
            "train loss:0.0011324156741500543\n",
            "train loss:0.0008139759211740516\n",
            "train loss:8.256323215625179e-05\n",
            "train loss:0.003962068960618601\n",
            "train loss:1.647688308654109e-05\n",
            "train loss:0.001242193839772159\n",
            "train loss:0.0010729318589313274\n",
            "train loss:0.003081864488119655\n",
            "train loss:0.005535996195002823\n",
            "train loss:0.005219642164158362\n",
            "train loss:0.00042025380904803017\n",
            "train loss:0.000520633348708585\n",
            "train loss:0.002546975568531035\n",
            "train loss:0.004485968185720462\n",
            "train loss:0.0004037952242262389\n",
            "train loss:0.0017290927885248442\n",
            "train loss:0.0003481866622173077\n",
            "train loss:0.002948085409003528\n",
            "train loss:0.001645331399293301\n",
            "train loss:0.0005578479056544039\n",
            "train loss:0.010200121989969455\n",
            "train loss:0.0018408955396080626\n",
            "train loss:0.000945867004833545\n",
            "train loss:0.018982966059731193\n",
            "train loss:0.0018553849597753038\n",
            "train loss:0.00014026276877104475\n",
            "train loss:0.00061057586658497\n",
            "train loss:0.008584648272452932\n",
            "train loss:0.0021834642655799547\n",
            "train loss:0.0025277322916314434\n",
            "train loss:0.002473587203896566\n",
            "train loss:0.00253626705637033\n",
            "train loss:0.003959295204412628\n",
            "train loss:0.0005324830609760947\n",
            "train loss:0.0010957950219613608\n",
            "train loss:0.0020139657877573554\n",
            "train loss:0.0010639741022531507\n",
            "train loss:0.0035262790666007343\n",
            "train loss:0.00875461301078542\n",
            "train loss:0.0039718881651968525\n",
            "train loss:0.015439914140852044\n",
            "train loss:0.000840567567414405\n",
            "train loss:0.0017690655820428703\n",
            "train loss:0.0034888035493160652\n",
            "train loss:0.00019249598136607035\n",
            "train loss:0.012287854203382019\n",
            "train loss:0.0034143114909236626\n",
            "train loss:0.0017845106529661317\n",
            "train loss:0.016046044233768406\n",
            "train loss:0.003483230115978111\n",
            "train loss:0.0011612142123305226\n",
            "train loss:0.0008332065132908984\n",
            "train loss:0.0008745483375952422\n",
            "train loss:0.007353271762523028\n",
            "train loss:0.002858481282827992\n",
            "train loss:0.0006671941805874819\n",
            "train loss:0.0029363270272532395\n",
            "train loss:0.004511063656971218\n",
            "train loss:0.00044865422911918347\n",
            "train loss:0.0035218110637746597\n",
            "train loss:0.0008123221463593252\n",
            "train loss:0.002306154647656885\n",
            "train loss:0.0006232702459556289\n",
            "train loss:0.004889827350382979\n",
            "train loss:0.0008186700681304527\n",
            "train loss:0.002583025930269106\n",
            "train loss:0.0011478249409503398\n",
            "train loss:7.785324946208001e-05\n",
            "train loss:0.005310492199145363\n",
            "train loss:0.0002232416643032925\n",
            "train loss:0.00025450593988823606\n",
            "train loss:0.0014964659281804424\n",
            "train loss:0.002276225739805255\n",
            "train loss:0.006490046072990033\n",
            "train loss:0.0043419687407422565\n",
            "train loss:0.014840465887247687\n",
            "train loss:0.001004557024656347\n",
            "train loss:7.933833573080154e-05\n",
            "train loss:0.0011939206918094294\n",
            "train loss:0.0001235008964959089\n",
            "train loss:0.002736769907759346\n",
            "train loss:0.0014653763555545702\n",
            "train loss:0.004193789672718757\n",
            "train loss:0.00907350145850625\n",
            "train loss:0.00018620079980564573\n",
            "train loss:0.0016880837623792608\n",
            "train loss:0.0002599738247788776\n",
            "train loss:0.00044802228950493355\n",
            "train loss:0.00022890339106128034\n",
            "train loss:0.0005347248824188986\n",
            "train loss:0.011293065279459898\n",
            "train loss:0.0020931651271452498\n",
            "train loss:0.010352910257064158\n",
            "train loss:0.0004904461026515434\n",
            "train loss:0.0030525540587764952\n",
            "train loss:0.002060876627861641\n",
            "train loss:0.009309120203749027\n",
            "train loss:7.727192853926002e-05\n",
            "train loss:3.712134723105284e-05\n",
            "train loss:0.0021220177853159467\n",
            "train loss:0.012150636729139339\n",
            "train loss:8.331858503589667e-05\n",
            "train loss:0.001010410332290213\n",
            "train loss:0.0020853781002146514\n",
            "train loss:5.7136208712600916e-05\n",
            "train loss:0.0005003661214167596\n",
            "train loss:0.0015616684818257364\n",
            "train loss:3.10435478493942e-05\n",
            "train loss:0.0008174628247703874\n",
            "train loss:0.0008348165330623389\n",
            "train loss:0.0006424000088081049\n",
            "train loss:0.001470767759367374\n",
            "train loss:0.00040067121215125556\n",
            "train loss:5.1275629901220206e-05\n",
            "train loss:0.003543144811212453\n",
            "train loss:3.4925238231686496e-05\n",
            "train loss:0.001214013980343059\n",
            "train loss:0.0007147298589010973\n",
            "train loss:0.000826737116192688\n",
            "train loss:4.937332223070361e-05\n",
            "train loss:0.002567174811258736\n",
            "train loss:0.0018494916646710424\n",
            "train loss:0.00028973273084718123\n",
            "train loss:0.00022295525059777663\n",
            "train loss:0.00025275252333107864\n",
            "train loss:0.0013601599461668342\n",
            "train loss:0.0006974602884398339\n",
            "train loss:0.00033130927767332493\n",
            "train loss:0.003763111804815559\n",
            "train loss:0.0031111581459005483\n",
            "train loss:0.0006419008878135843\n",
            "train loss:0.002864973195467355\n",
            "train loss:0.00021693859196058923\n",
            "train loss:0.0006673740166123821\n",
            "train loss:0.0002996576048301268\n",
            "train loss:0.0004241071260926605\n",
            "train loss:0.00020071138772467876\n",
            "train loss:0.0003966894670052965\n",
            "train loss:0.000787496449661455\n",
            "train loss:0.0017016864698376913\n",
            "train loss:0.000821694875513263\n",
            "train loss:0.006928613197668275\n",
            "train loss:0.00031642372147632457\n",
            "train loss:0.0012591782038047766\n",
            "train loss:0.006075910321590205\n",
            "train loss:0.00024619566375677247\n",
            "train loss:0.000535159093784277\n",
            "train loss:0.0004984721911240234\n",
            "train loss:0.00017949952074556707\n",
            "train loss:0.0005842287888943037\n",
            "train loss:0.000950819302208648\n",
            "train loss:0.00036233252450806034\n",
            "train loss:0.0012655577787993972\n",
            "train loss:0.0020236329846517933\n",
            "train loss:0.000762584417322186\n",
            "train loss:1.4165813853128625e-05\n",
            "train loss:0.00029134085619375333\n",
            "train loss:0.000723216622168487\n",
            "train loss:0.002665934594210889\n",
            "train loss:0.0002519420489141569\n",
            "train loss:0.003201398661994162\n",
            "train loss:0.0018865313339305827\n",
            "train loss:0.0005231373885149463\n",
            "train loss:0.0010570103861540382\n",
            "train loss:4.625924878770089e-05\n",
            "train loss:4.230046905039768e-05\n",
            "train loss:0.005424114784694698\n",
            "train loss:0.005184037117510818\n",
            "train loss:0.0005889420561749196\n",
            "train loss:0.06224961671845362\n",
            "train loss:0.00638044529832282\n",
            "train loss:0.00023064427427234652\n",
            "train loss:0.001590640623035971\n",
            "train loss:0.0021068568645514936\n",
            "train loss:0.0011672392284403502\n",
            "train loss:0.004058847971521004\n",
            "train loss:0.005708424974676943\n",
            "train loss:0.010664879675500482\n",
            "train loss:0.0026170936514912395\n",
            "train loss:0.0013114904855078934\n",
            "train loss:0.0004634439612949203\n",
            "train loss:0.0012688371668711496\n",
            "train loss:0.0013691916339946047\n",
            "train loss:0.0004860438696177736\n",
            "train loss:0.003053852224743997\n",
            "train loss:0.0008589077992359977\n",
            "train loss:0.004882993418405885\n",
            "train loss:0.0012956895161467638\n",
            "train loss:0.000265229106119503\n",
            "train loss:0.0006064612079594143\n",
            "train loss:0.004150297604985756\n",
            "train loss:0.001158920559041737\n",
            "train loss:0.0011402035010979184\n",
            "train loss:0.0009522185165834205\n",
            "train loss:0.0004160948939449489\n",
            "train loss:2.248643978032373e-05\n",
            "train loss:0.0001442366698120517\n",
            "train loss:0.003112641789760976\n",
            "train loss:0.0006209245558985227\n",
            "train loss:0.0021854914822410586\n",
            "=== epoch:18, train acc:0.998, test acc:0.983 ===\n",
            "train loss:0.002506191169182707\n",
            "train loss:0.0010618513101574986\n",
            "train loss:0.00044119478307926376\n",
            "train loss:0.0007868649264697429\n",
            "train loss:0.022867745464445796\n",
            "train loss:0.0027291995077244817\n",
            "train loss:0.00031870136124751753\n",
            "train loss:0.0012306533132516576\n",
            "train loss:0.0006663112753218812\n",
            "train loss:0.0022083114340610815\n",
            "train loss:0.00024982987632924633\n",
            "train loss:0.0003983731740091696\n",
            "train loss:0.0033598605050065154\n",
            "train loss:0.0030584003013367227\n",
            "train loss:0.0023013659892967835\n",
            "train loss:0.00523978448129399\n",
            "train loss:0.002471107073337789\n",
            "train loss:0.0010651065670139086\n",
            "train loss:0.0034599092359310956\n",
            "train loss:0.0015519656467521679\n",
            "train loss:0.0030982582777970423\n",
            "train loss:0.0011266529447827459\n",
            "train loss:0.001438556207931121\n",
            "train loss:0.004520636008272903\n",
            "train loss:0.00043737867786550406\n",
            "train loss:0.0011478568571963138\n",
            "train loss:0.0013689273147449413\n",
            "train loss:0.0004545723763592123\n",
            "train loss:0.00020154823070477483\n",
            "train loss:0.009648136176905438\n",
            "train loss:0.016244770713264186\n",
            "train loss:0.0032130522316323406\n",
            "train loss:0.04767357915190089\n",
            "train loss:0.00047444395964625673\n",
            "train loss:0.00016436986224852612\n",
            "train loss:0.005308845433717612\n",
            "train loss:0.003705901946400886\n",
            "train loss:0.009547490420665663\n",
            "train loss:0.00029715765872315153\n",
            "train loss:0.00019560201023083426\n",
            "train loss:0.0003489188320016842\n",
            "train loss:0.00044745577703706856\n",
            "train loss:0.0023765370475871393\n",
            "train loss:0.0002661644952090507\n",
            "train loss:0.0018026683881526414\n",
            "train loss:0.00046188369187026486\n",
            "train loss:0.0008013687796735207\n",
            "train loss:0.0006942330241017328\n",
            "train loss:0.032105231262757224\n",
            "train loss:0.00015682223375410574\n",
            "train loss:0.00010132948679402114\n",
            "train loss:0.002801193656566855\n",
            "train loss:0.002396919992907654\n",
            "train loss:0.0020279870405105604\n",
            "train loss:0.0024340453158745697\n",
            "train loss:0.00020704293813870653\n",
            "train loss:8.247967323950106e-05\n",
            "train loss:0.0018171734964664557\n",
            "train loss:0.0035601152402739106\n",
            "train loss:0.003559068798158578\n",
            "train loss:0.00858313361786073\n",
            "train loss:0.004377217940675539\n",
            "train loss:0.0005632735272875318\n",
            "train loss:0.0035194789294640365\n",
            "train loss:0.00011744034647071287\n",
            "train loss:0.004547755265356607\n",
            "train loss:0.002296018730058982\n",
            "train loss:0.0004683250121910276\n",
            "train loss:0.0034501174326960773\n",
            "train loss:0.00038699710830878483\n",
            "train loss:0.0002644695494085686\n",
            "train loss:0.002293143610506294\n",
            "train loss:0.0024345386871139686\n",
            "train loss:0.0012260560164495593\n",
            "train loss:0.0062699214524946836\n",
            "train loss:0.0024178426098360613\n",
            "train loss:0.0002582211675601714\n",
            "train loss:0.0003703125122996274\n",
            "train loss:0.0003291519547735805\n",
            "train loss:0.005915838991119412\n",
            "train loss:0.012472402741051079\n",
            "train loss:0.0004858829606873742\n",
            "train loss:0.0019843131065003542\n",
            "train loss:0.0003445159051614311\n",
            "train loss:0.007855786375030897\n",
            "train loss:0.000838807593015471\n",
            "train loss:0.006352176905855442\n",
            "train loss:0.0010231675687925447\n",
            "train loss:0.004350544645854763\n",
            "train loss:0.00235602326454361\n",
            "train loss:0.0015525744356686732\n",
            "train loss:0.0011717524604480949\n",
            "train loss:0.0005899517682510465\n",
            "train loss:0.00013001856487181588\n",
            "train loss:0.00046633941687820575\n",
            "train loss:0.0036712188352497526\n",
            "train loss:0.0035277256652453746\n",
            "train loss:0.0011406022338425739\n",
            "train loss:0.0021601769023007755\n",
            "train loss:0.00037115341503501286\n",
            "train loss:0.0003283251705864399\n",
            "train loss:0.013704092365813818\n",
            "train loss:0.0011418541635531635\n",
            "train loss:0.010481588019144828\n",
            "train loss:0.017483866536779268\n",
            "train loss:0.0014603407175868494\n",
            "train loss:0.0003156603476893577\n",
            "train loss:0.00019521236274437962\n",
            "train loss:0.006914601389239311\n",
            "train loss:0.004730278717245778\n",
            "train loss:0.0011378043493665646\n",
            "train loss:0.0020221916239784553\n",
            "train loss:0.0011319303802312826\n",
            "train loss:0.002162036393844169\n",
            "train loss:0.023701069277403643\n",
            "train loss:0.008595974754680082\n",
            "train loss:0.006638701512502831\n",
            "train loss:0.0004931811642407732\n",
            "train loss:0.00018489439778541624\n",
            "train loss:0.005251002236045544\n",
            "train loss:0.002509882116943716\n",
            "train loss:0.0024387349835420986\n",
            "train loss:0.00018426630248473733\n",
            "train loss:0.019783481444574442\n",
            "train loss:0.0011754319245562295\n",
            "train loss:0.002306441480714224\n",
            "train loss:0.008630548792396359\n",
            "train loss:0.0036063147951307024\n",
            "train loss:0.0026526712803159403\n",
            "train loss:0.002544911646832904\n",
            "train loss:0.0028395066776715143\n",
            "train loss:0.0006576107838887229\n",
            "train loss:0.0004689722718875664\n",
            "train loss:0.0006854908841517248\n",
            "train loss:0.001474022081881381\n",
            "train loss:0.0010683082177863179\n",
            "train loss:0.002623235119661912\n",
            "train loss:0.004645334394329025\n",
            "train loss:0.006376682299825721\n",
            "train loss:0.00039985122011209815\n",
            "train loss:0.0009709459440840958\n",
            "train loss:0.000639034947400884\n",
            "train loss:0.0006322970772384473\n",
            "train loss:0.00032293654899651943\n",
            "train loss:0.003376861452860894\n",
            "train loss:0.005209905456826322\n",
            "train loss:0.0027694350797016236\n",
            "train loss:0.001767865842846875\n",
            "train loss:0.0033146515331996316\n",
            "train loss:0.00010127793144949963\n",
            "train loss:0.005820078220627896\n",
            "train loss:0.009934156498293944\n",
            "train loss:0.004639030158467497\n",
            "train loss:0.004555855120863578\n",
            "train loss:0.001007928388032848\n",
            "train loss:0.0005558791446791706\n",
            "train loss:0.0002756318935829395\n",
            "train loss:0.002435870638241204\n",
            "train loss:0.0014274314303382401\n",
            "train loss:0.0033104366055826183\n",
            "train loss:0.002099813333392977\n",
            "train loss:0.0024948217475182404\n",
            "train loss:0.0003091757204189905\n",
            "train loss:0.003480375064822205\n",
            "train loss:0.0009080471769876257\n",
            "train loss:0.0029754996502998846\n",
            "train loss:0.00013904056909898012\n",
            "train loss:4.95472973079136e-05\n",
            "train loss:0.0027432443090193133\n",
            "train loss:0.00045104365021726755\n",
            "train loss:0.0008242750992131935\n",
            "train loss:0.004368605127732511\n",
            "train loss:0.00022412951763092485\n",
            "train loss:0.0031442318719693964\n",
            "train loss:0.002117387519929049\n",
            "train loss:0.00013992763619557442\n",
            "train loss:0.0003923080404787379\n",
            "train loss:0.0058536291742932465\n",
            "train loss:0.0004035576469081036\n",
            "train loss:0.0005479954660623667\n",
            "train loss:0.0037368084558801096\n",
            "train loss:0.0001334372311601197\n",
            "train loss:0.0003883366477732735\n",
            "train loss:0.005639435544147304\n",
            "train loss:0.0004583102958009662\n",
            "train loss:0.000592206547655736\n",
            "train loss:0.0020761576919197783\n",
            "train loss:0.0026402311036225995\n",
            "train loss:0.0009529049056383137\n",
            "train loss:0.0022467028459015183\n",
            "train loss:0.0018522771825062552\n",
            "train loss:0.0058151182675055466\n",
            "train loss:0.005874662768173199\n",
            "train loss:0.003020382134892817\n",
            "train loss:0.00041756661924144655\n",
            "train loss:0.0012847132140085783\n",
            "train loss:0.001128868725394713\n",
            "train loss:0.0002744091075627819\n",
            "train loss:0.0012766429966650404\n",
            "train loss:0.00032450399281774687\n",
            "train loss:0.0004288521475486784\n",
            "train loss:0.0007648794555801356\n",
            "train loss:0.001576994891022607\n",
            "train loss:0.0041979789048953295\n",
            "train loss:0.0013053812681471121\n",
            "train loss:0.0004854724149536192\n",
            "train loss:0.0021073587208175655\n",
            "train loss:0.0005122705901347655\n",
            "train loss:0.0005174228446589134\n",
            "train loss:0.0012729258827709913\n",
            "train loss:0.0029212167556876283\n",
            "train loss:0.00030455061598122333\n",
            "train loss:0.0024210158714965567\n",
            "train loss:0.0005634369712207383\n",
            "train loss:0.00013787736844534287\n",
            "train loss:0.0014403276281111487\n",
            "train loss:0.0005265866164425723\n",
            "train loss:0.00010285943125690802\n",
            "train loss:0.0014707947366044904\n",
            "train loss:0.0021452069373748034\n",
            "train loss:0.00010497052937192346\n",
            "train loss:0.0011162307307433998\n",
            "train loss:0.005085740015711322\n",
            "train loss:0.0006814734086380207\n",
            "train loss:0.008634792748809796\n",
            "train loss:0.0007537001038062111\n",
            "train loss:0.006760029211597013\n",
            "train loss:7.831714586958365e-05\n",
            "train loss:0.00011342355204944633\n",
            "train loss:0.00015192507415912634\n",
            "train loss:0.005062021146123679\n",
            "train loss:0.0005983168655821935\n",
            "train loss:0.0010853660114471329\n",
            "train loss:0.0024284375505944705\n",
            "train loss:0.0003512043002283279\n",
            "train loss:0.00031766542310427586\n",
            "train loss:0.0022932571164300817\n",
            "train loss:0.0015676244797466338\n",
            "train loss:0.00040954229145896297\n",
            "train loss:0.0031042322467170792\n",
            "train loss:0.00036539244960147733\n",
            "train loss:0.0005621195943876263\n",
            "train loss:0.0023440616407281916\n",
            "train loss:0.00024356622351084052\n",
            "train loss:0.0007156813162878741\n",
            "train loss:9.638083140614633e-05\n",
            "train loss:0.0011181607413918103\n",
            "train loss:0.00032593786491525047\n",
            "train loss:0.00017787165716706316\n",
            "train loss:0.0002824373523098046\n",
            "train loss:0.0007359535339356001\n",
            "train loss:0.002577473218598775\n",
            "train loss:0.009011750846837016\n",
            "train loss:0.0011779252494880327\n",
            "train loss:0.000506826209028312\n",
            "train loss:0.0007601169903111327\n",
            "train loss:0.004262339011178373\n",
            "train loss:0.00024964440969628605\n",
            "train loss:0.0004134125915948569\n",
            "train loss:0.0022918549970065093\n",
            "train loss:0.00033315537283303985\n",
            "train loss:0.0006395661392365892\n",
            "train loss:0.000474579635964477\n",
            "train loss:0.00035131166073392285\n",
            "train loss:0.0002253197590467898\n",
            "train loss:0.0023653134692410097\n",
            "train loss:0.0009693426619542423\n",
            "train loss:0.00028174615940666195\n",
            "train loss:0.00020366825560223467\n",
            "train loss:0.0022660824241212286\n",
            "train loss:0.0006682985583037364\n",
            "train loss:7.815113652451635e-05\n",
            "train loss:9.560488963551458e-05\n",
            "train loss:0.0009234439438092009\n",
            "train loss:0.00032762251455977734\n",
            "train loss:0.0011360433851073646\n",
            "train loss:0.003155093093992223\n",
            "train loss:0.01837845921124071\n",
            "train loss:6.225584950331484e-05\n",
            "train loss:0.00011936105281515448\n",
            "train loss:0.0028010651240238417\n",
            "train loss:0.0007342254948928598\n",
            "train loss:0.0002970085142075512\n",
            "train loss:0.004204800675012641\n",
            "train loss:0.00022331193639096113\n",
            "train loss:0.007269304536991671\n",
            "train loss:0.0025633782188353572\n",
            "train loss:8.726894554480068e-05\n",
            "train loss:0.00018855788286871464\n",
            "train loss:0.004662484175867633\n",
            "train loss:0.00029009325815733503\n",
            "train loss:0.0025361354910825875\n",
            "train loss:0.0005880733769726506\n",
            "train loss:0.0003660138130057089\n",
            "train loss:0.0018058167587753273\n",
            "train loss:0.0015864971686761651\n",
            "train loss:0.0005853930822599142\n",
            "train loss:0.00013694454657853328\n",
            "train loss:0.004485092517069609\n",
            "train loss:0.0012554097022561626\n",
            "train loss:0.0023328029334230625\n",
            "train loss:0.0022838752572466687\n",
            "train loss:0.0004265930946693464\n",
            "train loss:0.0037908519387609913\n",
            "train loss:0.0019945453061743375\n",
            "train loss:0.00021189673826856119\n",
            "train loss:0.002722805012587638\n",
            "train loss:0.0023750727327909887\n",
            "train loss:0.0011120405194528617\n",
            "train loss:0.00025865046371903433\n",
            "train loss:0.00044570415184036363\n",
            "train loss:0.0026504770337282026\n",
            "train loss:0.00022155061867558563\n",
            "train loss:0.001074968741781445\n",
            "train loss:0.0007371183516507326\n",
            "train loss:0.00023073068005005211\n",
            "train loss:0.0010778822901028476\n",
            "train loss:0.0004945359798736829\n",
            "train loss:0.0025254450927821464\n",
            "train loss:0.0020774557823938574\n",
            "train loss:0.0018244482299579713\n",
            "train loss:0.0002871248932107414\n",
            "train loss:0.0011667264692747923\n",
            "train loss:0.0017306828269937334\n",
            "train loss:0.0015878824322951937\n",
            "train loss:0.0014485659477305676\n",
            "train loss:0.0018534103331445762\n",
            "train loss:0.00010267258095938975\n",
            "train loss:0.0011222972006814027\n",
            "train loss:0.0005436899456601524\n",
            "train loss:0.0014925114141034297\n",
            "train loss:0.004206295943415845\n",
            "train loss:0.0008857919088743556\n",
            "train loss:0.000310409519102318\n",
            "train loss:0.000827209504714311\n",
            "train loss:0.00014464378114856295\n",
            "train loss:0.0004867872500225534\n",
            "train loss:0.0038492373694879946\n",
            "train loss:0.00016933871440809426\n",
            "train loss:0.00018151036171102172\n",
            "train loss:0.001421311942316941\n",
            "train loss:0.0005951282563136764\n",
            "train loss:0.007133202577546888\n",
            "train loss:0.002410257114396237\n",
            "train loss:0.0005997888293341911\n",
            "train loss:0.0005983653644763596\n",
            "train loss:0.0023542100290611327\n",
            "train loss:0.00012180500345941845\n",
            "train loss:0.000444011187217476\n",
            "train loss:0.0024510680582224558\n",
            "train loss:0.0023558265931947152\n",
            "train loss:0.0005905294341643716\n",
            "train loss:0.0005859468970858632\n",
            "train loss:0.0005109556479176462\n",
            "train loss:0.004992319472221986\n",
            "train loss:0.004514358440723848\n",
            "train loss:0.0010275126534893095\n",
            "train loss:0.0006721842899676398\n",
            "train loss:0.00047743159316640673\n",
            "train loss:0.0038673544724842058\n",
            "train loss:0.006657191328780356\n",
            "train loss:0.0012568132115058285\n",
            "train loss:0.0001318961142461599\n",
            "train loss:0.010266456797304027\n",
            "train loss:0.001960387937966442\n",
            "train loss:0.002387827541670793\n",
            "train loss:0.0010048753639508741\n",
            "train loss:0.0009429559814888954\n",
            "train loss:0.01182364811237799\n",
            "train loss:0.0017957513562424682\n",
            "train loss:0.00013591199770958444\n",
            "train loss:0.0006155857367309208\n",
            "train loss:0.0010368546474552793\n",
            "train loss:0.00013949887800103657\n",
            "train loss:0.0002301462404736665\n",
            "train loss:0.0005639538478273052\n",
            "train loss:0.0008340523233861366\n",
            "train loss:0.0030943398618793762\n",
            "train loss:0.0011817048431236115\n",
            "train loss:0.003419512353755046\n",
            "train loss:0.0017954193040757615\n",
            "train loss:0.0032598549423247165\n",
            "train loss:0.00016651225177359783\n",
            "train loss:0.00020230562016392692\n",
            "train loss:0.001354189225543474\n",
            "train loss:0.0042886427885732974\n",
            "train loss:0.0012126246637892348\n",
            "train loss:0.0013018650113552734\n",
            "train loss:0.0006764860971340213\n",
            "train loss:0.0008506864178024074\n",
            "train loss:0.0014346350047222888\n",
            "train loss:0.0010242840398148715\n",
            "train loss:0.006006917138168168\n",
            "train loss:0.0028908468417150023\n",
            "train loss:0.0012008197564802496\n",
            "train loss:0.00015180656859678586\n",
            "train loss:3.825333848173938e-05\n",
            "train loss:0.00024762145306758075\n",
            "train loss:0.0003096641125857162\n",
            "train loss:0.0017486012867279574\n",
            "train loss:0.001985826904236726\n",
            "train loss:0.002297517929720416\n",
            "train loss:0.0007895777293378091\n",
            "train loss:0.0002894430586194689\n",
            "train loss:0.0015889219915745996\n",
            "train loss:0.0034620952397636535\n",
            "train loss:7.609635390345553e-06\n",
            "train loss:0.0032847535071074026\n",
            "train loss:0.0002584085965995781\n",
            "train loss:0.0003017498341976734\n",
            "train loss:0.002178382430685458\n",
            "train loss:0.0002305644420509492\n",
            "train loss:0.002696207846568337\n",
            "train loss:0.0009132107521053716\n",
            "train loss:0.002220458365753278\n",
            "train loss:0.0013060081470565139\n",
            "train loss:0.0043403983457901306\n",
            "train loss:0.000214011536021876\n",
            "train loss:0.00015117964869492575\n",
            "train loss:6.598725973411322e-05\n",
            "train loss:0.0030823440067164164\n",
            "train loss:0.0002034831625319053\n",
            "train loss:0.002270644130236574\n",
            "train loss:0.0017418467300788526\n",
            "train loss:0.00042902444221465543\n",
            "train loss:0.0002733518751871923\n",
            "train loss:0.002612262349874861\n",
            "train loss:0.0026799335367921778\n",
            "train loss:0.005922853601996314\n",
            "train loss:6.758253814171548e-05\n",
            "train loss:0.00021369928861313329\n",
            "train loss:0.00041280992775770935\n",
            "train loss:0.0010929049756845909\n",
            "train loss:0.001442913442920473\n",
            "train loss:0.0031972133288657983\n",
            "train loss:0.00048460619049424955\n",
            "train loss:0.001057210152631522\n",
            "train loss:0.00014783545342840398\n",
            "train loss:0.0010365868728249225\n",
            "train loss:0.0011313282354712395\n",
            "train loss:0.002464582462540287\n",
            "train loss:0.00017246776893046437\n",
            "train loss:0.00041752312253249266\n",
            "train loss:0.0010484798501758372\n",
            "train loss:0.0010910671343749801\n",
            "train loss:0.00255340060911605\n",
            "train loss:0.002578094461280047\n",
            "train loss:0.0015733513298731444\n",
            "train loss:0.00010510391353821688\n",
            "train loss:6.864335088102715e-05\n",
            "train loss:0.000469574021726493\n",
            "train loss:0.0005616659519612617\n",
            "train loss:0.0022909566977222483\n",
            "train loss:0.0007664249621894344\n",
            "train loss:0.004060691129390608\n",
            "train loss:0.00020239404566874618\n",
            "train loss:0.00033445357709948394\n",
            "train loss:0.0018802071093244712\n",
            "train loss:0.021122775215106807\n",
            "train loss:2.7607717166019955e-05\n",
            "train loss:0.004238006395259214\n",
            "train loss:0.001957302073464212\n",
            "train loss:4.14370810983578e-05\n",
            "train loss:0.0006280072477843222\n",
            "train loss:0.001124964634154817\n",
            "train loss:0.0012307976271305442\n",
            "train loss:0.001536872669491072\n",
            "train loss:0.0006665886103954212\n",
            "train loss:0.0006542422680085632\n",
            "train loss:0.0029389732092548445\n",
            "train loss:0.0030909914714330306\n",
            "train loss:0.0009294014348717738\n",
            "train loss:0.002154499623798152\n",
            "train loss:0.00015115279939696008\n",
            "train loss:0.0003883398027250205\n",
            "train loss:0.0026881871274589825\n",
            "train loss:0.002896166481384912\n",
            "train loss:0.0036229371857961446\n",
            "train loss:7.450442294140002e-05\n",
            "train loss:0.0007292782228438803\n",
            "train loss:0.00014704580987227962\n",
            "train loss:0.0002704148595990509\n",
            "train loss:0.00012257843661733927\n",
            "train loss:0.0037687868083031663\n",
            "train loss:0.00010364945991745497\n",
            "train loss:0.0005103211712562217\n",
            "train loss:0.002520010336214665\n",
            "train loss:0.00026045048375258905\n",
            "train loss:0.0020421591743868873\n",
            "train loss:0.002555874041565343\n",
            "train loss:0.0009434743446497182\n",
            "train loss:0.0011079701846003656\n",
            "train loss:0.0008379212585100164\n",
            "train loss:0.002758018654696027\n",
            "train loss:0.00022682900947644917\n",
            "train loss:0.0018639056749775549\n",
            "train loss:0.0004886785971776977\n",
            "train loss:0.0007899524373128254\n",
            "train loss:0.0005574614634579825\n",
            "train loss:0.0002128545483824915\n",
            "train loss:0.0004938856685047223\n",
            "train loss:0.002446205502232954\n",
            "train loss:0.00015668946484705126\n",
            "train loss:0.0001269953438551073\n",
            "train loss:0.0009342782760752838\n",
            "train loss:0.002307282792020035\n",
            "train loss:0.0032184440998752036\n",
            "train loss:4.461966567487825e-05\n",
            "train loss:0.0007966454520899454\n",
            "train loss:0.0005178539719578218\n",
            "train loss:0.0001611337430740649\n",
            "train loss:0.0004211420767666904\n",
            "train loss:2.8520292712968547e-05\n",
            "train loss:0.0016755878518162808\n",
            "train loss:0.004243038747177558\n",
            "train loss:0.00018422159280058554\n",
            "train loss:7.825639806361334e-05\n",
            "train loss:0.002504090477058009\n",
            "train loss:0.00025454847079459325\n",
            "train loss:0.0006326215799936602\n",
            "train loss:1.3282093608582444e-05\n",
            "train loss:0.0014311046988311432\n",
            "train loss:4.45075424980728e-05\n",
            "train loss:0.001659059204382979\n",
            "train loss:0.00035792540511721623\n",
            "train loss:4.7206303157359535e-05\n",
            "train loss:0.0009207696635720983\n",
            "train loss:0.002964492072149869\n",
            "train loss:0.0011286470245437355\n",
            "train loss:0.00021224022093023435\n",
            "train loss:5.865540921339757e-05\n",
            "train loss:0.0006203406853793308\n",
            "train loss:0.0017959429992031673\n",
            "train loss:0.00012237839754254188\n",
            "train loss:0.0007183897999194365\n",
            "train loss:5.6229885312986366e-05\n",
            "train loss:0.0007724013446931255\n",
            "train loss:0.0011558380843242784\n",
            "train loss:0.00017313187381382705\n",
            "train loss:0.0004773959229206415\n",
            "train loss:0.00021262081473204578\n",
            "train loss:0.0008289626971789773\n",
            "train loss:0.0004833827580536419\n",
            "train loss:0.0019823445243580735\n",
            "train loss:0.0003871012956412873\n",
            "train loss:0.0010805219363730805\n",
            "train loss:0.00010148355676281888\n",
            "train loss:0.0017256191626210172\n",
            "train loss:0.0007020179691907815\n",
            "train loss:0.0005785677014879527\n",
            "train loss:0.00018383080255493847\n",
            "train loss:0.00022299083639370213\n",
            "train loss:0.0016597452170480226\n",
            "train loss:0.0014760927163602177\n",
            "train loss:0.0001736914023044223\n",
            "train loss:0.00041276160488086554\n",
            "train loss:0.0006038223855886998\n",
            "train loss:0.0007694652725410657\n",
            "train loss:7.315685706396995e-05\n",
            "train loss:4.633910887708423e-05\n",
            "train loss:0.0015168403167533048\n",
            "train loss:0.0006212625431221397\n",
            "train loss:0.0005382932011243031\n",
            "train loss:0.0005247440921473448\n",
            "train loss:0.0012953995051428644\n",
            "train loss:0.00013483614058161001\n",
            "train loss:0.0006918895391251119\n",
            "train loss:0.00027813635635432426\n",
            "train loss:0.0011830419912045322\n",
            "train loss:0.0009562844237474876\n",
            "train loss:0.00028991245761739995\n",
            "train loss:3.331170963653366e-05\n",
            "train loss:0.000302957143544378\n",
            "train loss:0.0018597281210259592\n",
            "train loss:4.351452787515224e-05\n",
            "train loss:0.00013519449739441645\n",
            "train loss:0.00027858318100931973\n",
            "train loss:7.761177989675458e-05\n",
            "train loss:0.0006327338253962076\n",
            "train loss:0.00037116767728977054\n",
            "train loss:4.796895653532179e-05\n",
            "train loss:0.00023619908331692834\n",
            "train loss:0.0032525661229126403\n",
            "train loss:5.4960047131718425e-05\n",
            "train loss:0.00016541177078884827\n",
            "train loss:0.0003766298049666631\n",
            "train loss:0.0005447101511459908\n",
            "train loss:0.0012390650323733509\n",
            "train loss:0.00012327190762295102\n",
            "train loss:0.00045708627236560565\n",
            "train loss:0.002026523593211969\n",
            "train loss:8.768033568066927e-05\n",
            "train loss:0.0016093407013150645\n",
            "train loss:0.001279018257264605\n",
            "train loss:0.0006707116379114672\n",
            "train loss:0.0009064743020692656\n",
            "train loss:0.0004974137360787218\n",
            "train loss:0.0001460048488613993\n",
            "train loss:0.0003373986490268977\n",
            "train loss:0.00023430029961076928\n",
            "=== epoch:19, train acc:0.999, test acc:0.987 ===\n",
            "train loss:0.0003427369703190231\n",
            "train loss:0.00041574919215550546\n",
            "train loss:0.0011296653889902846\n",
            "train loss:2.4273428359814775e-05\n",
            "train loss:0.0023861308660299537\n",
            "train loss:5.1309175794273465e-05\n",
            "train loss:7.407279403997427e-06\n",
            "train loss:0.002649642330690793\n",
            "train loss:0.00010571283556601065\n",
            "train loss:0.00022705867099841122\n",
            "train loss:0.00434412094666747\n",
            "train loss:0.0009447703826026958\n",
            "train loss:0.005006305842846293\n",
            "train loss:9.684364268813978e-05\n",
            "train loss:9.611036311515677e-05\n",
            "train loss:1.894046081333698e-05\n",
            "train loss:0.004981546035304593\n",
            "train loss:0.0009221254584548713\n",
            "train loss:0.00014907637575082924\n",
            "train loss:0.00012108966245147394\n",
            "train loss:0.001895545165037438\n",
            "train loss:6.820904434281854e-05\n",
            "train loss:0.00021473038737432148\n",
            "train loss:0.0007302363357235027\n",
            "train loss:0.0013956769562531862\n",
            "train loss:0.00029089959913765703\n",
            "train loss:0.0008906875498188863\n",
            "train loss:0.0005702941601295568\n",
            "train loss:0.0003368986349646398\n",
            "train loss:0.0015583179261532043\n",
            "train loss:0.0001848489371920144\n",
            "train loss:0.0001931767452367177\n",
            "train loss:0.006174686707192023\n",
            "train loss:0.0025509064748441457\n",
            "train loss:8.556053890785222e-05\n",
            "train loss:0.00023648096496385222\n",
            "train loss:0.00027466790083328777\n",
            "train loss:0.0004159262981913868\n",
            "train loss:0.002036623086824794\n",
            "train loss:0.0007097975866291587\n",
            "train loss:0.0005248458271443576\n",
            "train loss:0.0005511446972035954\n",
            "train loss:0.0012157955275640489\n",
            "train loss:0.000604945490258906\n",
            "train loss:5.1159488583637296e-05\n",
            "train loss:0.0001121650117941088\n",
            "train loss:0.0011689605743200621\n",
            "train loss:0.0005222001206441136\n",
            "train loss:0.0009988236691569894\n",
            "train loss:0.0025534762909004455\n",
            "train loss:1.6172628055694044e-05\n",
            "train loss:5.158513201365405e-05\n",
            "train loss:0.00013091018325442575\n",
            "train loss:0.0003200468510301407\n",
            "train loss:2.6979303106523693e-05\n",
            "train loss:0.0021053350070307083\n",
            "train loss:0.0025314112228984986\n",
            "train loss:0.0015049366462300156\n",
            "train loss:0.00021241711114890254\n",
            "train loss:0.00017354811009853626\n",
            "train loss:0.00030166866242454854\n",
            "train loss:0.0007034350879538335\n",
            "train loss:3.9816384793073916e-05\n",
            "train loss:0.00027360736779607624\n",
            "train loss:0.00031839301798538384\n",
            "train loss:0.0002422145869207698\n",
            "train loss:0.00016644729974959124\n",
            "train loss:6.256679886061641e-05\n",
            "train loss:0.0001514546027876219\n",
            "train loss:0.0024721489626269642\n",
            "train loss:0.00042485272546043606\n",
            "train loss:0.004776958189128602\n",
            "train loss:0.0010229241618602337\n",
            "train loss:0.0006565484293408798\n",
            "train loss:6.76116654525778e-05\n",
            "train loss:4.870917313054928e-05\n",
            "train loss:0.0026264252022732202\n",
            "train loss:4.4365754986068495e-05\n",
            "train loss:0.00053333817491715\n",
            "train loss:4.4398149683414634e-05\n",
            "train loss:0.0015462082099670343\n",
            "train loss:0.0007629317363856383\n",
            "train loss:0.002008019960259393\n",
            "train loss:0.0017895556859436066\n",
            "train loss:0.0006015167726141321\n",
            "train loss:0.0012823788611287447\n",
            "train loss:0.001396046959619518\n",
            "train loss:0.0005373795471710142\n",
            "train loss:0.0007670495217906051\n",
            "train loss:0.00019956248000732742\n",
            "train loss:0.0008542362973515023\n",
            "train loss:0.00022897104606340536\n",
            "train loss:0.0002094506455491281\n",
            "train loss:0.0015184678523161464\n",
            "train loss:0.0043321219896830234\n",
            "train loss:0.0013822726900948367\n",
            "train loss:0.00021503243279580138\n",
            "train loss:0.00048013294007820916\n",
            "train loss:0.0034331959425852997\n",
            "train loss:0.0009468572770556347\n",
            "train loss:0.0013179011253251494\n",
            "train loss:0.00013399034663210855\n",
            "train loss:0.0001594758984836733\n",
            "train loss:0.00031076722395637443\n",
            "train loss:0.0009084151450449858\n",
            "train loss:0.0006483202640264786\n",
            "train loss:0.00019417920429378604\n",
            "train loss:9.180664561786732e-05\n",
            "train loss:0.00010568808779538139\n",
            "train loss:0.0012496186225263125\n",
            "train loss:0.00017362094829523474\n",
            "train loss:0.00036903942601883185\n",
            "train loss:0.0009486390910618783\n",
            "train loss:0.0002799537561802855\n",
            "train loss:0.0004829914293820599\n",
            "train loss:1.0399876460828512e-05\n",
            "train loss:0.0014609641657417148\n",
            "train loss:0.0017847658566668865\n",
            "train loss:0.0003263635156857698\n",
            "train loss:0.001356840150615391\n",
            "train loss:9.29981697933243e-05\n",
            "train loss:0.00013687840626658515\n",
            "train loss:0.000800853936275826\n",
            "train loss:4.53198003083975e-05\n",
            "train loss:0.00012258934513225208\n",
            "train loss:0.0007655812432956266\n",
            "train loss:0.027213761985110753\n",
            "train loss:0.00023422408607352133\n",
            "train loss:0.0023297404479683034\n",
            "train loss:0.002880063033174941\n",
            "train loss:0.0034786480260488783\n",
            "train loss:3.866930844937991e-05\n",
            "train loss:7.524004993049525e-05\n",
            "train loss:1.4726957353249636e-05\n",
            "train loss:0.015178688486650781\n",
            "train loss:0.0010513176038443222\n",
            "train loss:0.0021588668297697843\n",
            "train loss:0.007605213574487569\n",
            "train loss:0.0004027549573973685\n",
            "train loss:0.00011657555295751951\n",
            "train loss:0.0028427647752972155\n",
            "train loss:0.0009366426456550171\n",
            "train loss:0.0002387962056224367\n",
            "train loss:0.0009867262424967985\n",
            "train loss:0.00018938990793106425\n",
            "train loss:0.00012149333113578967\n",
            "train loss:0.00022342131928584225\n",
            "train loss:0.00047803085884188043\n",
            "train loss:0.0035516547551984823\n",
            "train loss:0.00022080627970474348\n",
            "train loss:0.0005664981821250305\n",
            "train loss:0.0005107538161350969\n",
            "train loss:7.927217949361926e-05\n",
            "train loss:0.00029176128029031345\n",
            "train loss:0.000795825362906546\n",
            "train loss:0.00028267336250016356\n",
            "train loss:0.001075767979555997\n",
            "train loss:0.0007476586566453777\n",
            "train loss:0.0038073291511426816\n",
            "train loss:0.03299338466277292\n",
            "train loss:0.0003436318221416134\n",
            "train loss:0.0001978172010433888\n",
            "train loss:0.00031637940706369876\n",
            "train loss:8.973396143580154e-05\n",
            "train loss:4.8378522674127256e-05\n",
            "train loss:0.0001859739882006734\n",
            "train loss:0.0008516701932422267\n",
            "train loss:0.0006597072566557272\n",
            "train loss:0.0007018523558541484\n",
            "train loss:0.0024516464592755118\n",
            "train loss:0.0004154865476083219\n",
            "train loss:0.00020246841349503143\n",
            "train loss:0.0001316265619832169\n",
            "train loss:0.0021736780946691177\n",
            "train loss:0.0005565624412395663\n",
            "train loss:0.0016917383950659655\n",
            "train loss:5.0818792501591024e-05\n",
            "train loss:0.0003464067554939075\n",
            "train loss:2.3342067485100293e-05\n",
            "train loss:0.0012299700753511953\n",
            "train loss:0.0002140907011378581\n",
            "train loss:4.623238734858436e-05\n",
            "train loss:2.899759316037732e-05\n",
            "train loss:0.000231515760594171\n",
            "train loss:0.000945873766060682\n",
            "train loss:0.0006001168460106317\n",
            "train loss:0.0003395977414372219\n",
            "train loss:0.0007501353034420877\n",
            "train loss:0.0006932799558021211\n",
            "train loss:0.0026123013981880997\n",
            "train loss:9.490271582063521e-05\n",
            "train loss:6.000786621248108e-05\n",
            "train loss:0.0010656844327086986\n",
            "train loss:0.0004753450764098095\n",
            "train loss:0.0011597714637007768\n",
            "train loss:0.0005930296938043621\n",
            "train loss:0.002006583703687447\n",
            "train loss:0.0043736199970146345\n",
            "train loss:0.001282270206673141\n",
            "train loss:0.0016809337637133939\n",
            "train loss:0.0007438681741468791\n",
            "train loss:0.0006891874445864071\n",
            "train loss:0.0016051930603711738\n",
            "train loss:0.00014739945450853973\n",
            "train loss:0.0006353099787524568\n",
            "train loss:5.211003445475214e-05\n",
            "train loss:0.000456574129946143\n",
            "train loss:0.00033095108905799824\n",
            "train loss:0.0016689689237446201\n",
            "train loss:0.0007453757149053701\n",
            "train loss:0.0013398514303987442\n",
            "train loss:4.661311142233464e-05\n",
            "train loss:0.0011448381324318043\n",
            "train loss:0.000937703841587737\n",
            "train loss:0.00043813202111803596\n",
            "train loss:0.0001123203106850375\n",
            "train loss:0.001601473077302546\n",
            "train loss:6.704587466508601e-05\n",
            "train loss:0.001430837482920789\n",
            "train loss:0.0007333685768835012\n",
            "train loss:0.0017353790164757548\n",
            "train loss:0.00018202227956432907\n",
            "train loss:0.00019362420386659273\n",
            "train loss:0.0010832390074889658\n",
            "train loss:0.0003574822739930981\n",
            "train loss:0.002413462828955644\n",
            "train loss:0.003516420090126218\n",
            "train loss:0.00045865250234223263\n",
            "train loss:0.0008060710735330764\n",
            "train loss:0.00548199883222881\n",
            "train loss:5.5091237401654526e-05\n",
            "train loss:0.0006342373074227929\n",
            "train loss:0.006583460769774949\n",
            "train loss:0.002186907034933233\n",
            "train loss:0.0003926442153821377\n",
            "train loss:0.0006435273397639048\n",
            "train loss:0.00025723199246212297\n",
            "train loss:0.0005712438878858086\n",
            "train loss:0.0002377308654167918\n",
            "train loss:0.0010371115244113096\n",
            "train loss:0.0002829557978070846\n",
            "train loss:0.0026716590820777203\n",
            "train loss:0.009744247477706813\n",
            "train loss:0.005792594905189792\n",
            "train loss:0.000574845862619381\n",
            "train loss:0.0007955576002517625\n",
            "train loss:0.0010555060348906757\n",
            "train loss:0.00040543005568749663\n",
            "train loss:0.0025756798635059204\n",
            "train loss:0.00427467107998618\n",
            "train loss:0.005087721926584153\n",
            "train loss:0.014913002925061975\n",
            "train loss:0.0014121703619651036\n",
            "train loss:7.69831913613687e-05\n",
            "train loss:0.001436916406411386\n",
            "train loss:0.0018084483529741048\n",
            "train loss:0.0023294544202552927\n",
            "train loss:0.002589691569148714\n",
            "train loss:0.0019413181519187855\n",
            "train loss:0.0009780641924584904\n",
            "train loss:0.0026936892825154208\n",
            "train loss:0.0017082018850060469\n",
            "train loss:0.001251253389406323\n",
            "train loss:0.0005909797490523096\n",
            "train loss:0.023290432746318847\n",
            "train loss:0.0012944190064935897\n",
            "train loss:0.0016686695167099856\n",
            "train loss:0.0005297126533323944\n",
            "train loss:0.0005440818990811347\n",
            "train loss:0.007614676991241535\n",
            "train loss:0.002829256537993743\n",
            "train loss:0.005988198218244888\n",
            "train loss:0.0017250821025075471\n",
            "train loss:0.00043029438750858925\n",
            "train loss:0.0008921065179803623\n",
            "train loss:0.0010804978744280158\n",
            "train loss:0.001224532279407951\n",
            "train loss:0.0005494411201411915\n",
            "train loss:0.008944667586669997\n",
            "train loss:0.0029538727285017743\n",
            "train loss:0.0008379790611519437\n",
            "train loss:0.0009400253708252995\n",
            "train loss:0.0008762657460511555\n",
            "train loss:0.0005303129755833139\n",
            "train loss:0.00016446877481671374\n",
            "train loss:0.0019248910859064438\n",
            "train loss:0.002230259018031605\n",
            "train loss:0.0008485641011228297\n",
            "train loss:0.0010231944881018157\n",
            "train loss:0.0012101350488125714\n",
            "train loss:4.168627569267384e-05\n",
            "train loss:0.0023220834866253485\n",
            "train loss:0.002243192429922433\n",
            "train loss:0.002582877342268958\n",
            "train loss:0.00012390039756379623\n",
            "train loss:0.0012094368105980365\n",
            "train loss:0.0002561202547125484\n",
            "train loss:0.0009706067303143459\n",
            "train loss:0.0015853359355914995\n",
            "train loss:0.0025743879078909994\n",
            "train loss:0.0013030461718755797\n",
            "train loss:0.01415723643117037\n",
            "train loss:0.000492487860464784\n",
            "train loss:0.01931470118330524\n",
            "train loss:0.0009456784788862541\n",
            "train loss:7.807793211525929e-05\n",
            "train loss:0.0007836632623872638\n",
            "train loss:0.003055767108430017\n",
            "train loss:3.120727640892003e-05\n",
            "train loss:0.00018653397074770651\n",
            "train loss:0.001688993058960041\n",
            "train loss:0.0001787336186173237\n",
            "train loss:0.0022711730060097964\n",
            "train loss:0.0019243707809713387\n",
            "train loss:0.00014001612454263938\n",
            "train loss:0.0002764940412825366\n",
            "train loss:0.00698061241110991\n",
            "train loss:0.00010041013341385671\n",
            "train loss:0.004139353442751715\n",
            "train loss:0.0028106955184839925\n",
            "train loss:0.000704881118650292\n",
            "train loss:0.004159820933372385\n",
            "train loss:0.0012766750282909181\n",
            "train loss:0.0001974337409652357\n",
            "train loss:0.0007743756200594788\n",
            "train loss:0.00032470332618194633\n",
            "train loss:0.0025488022933227443\n",
            "train loss:0.0020580416253839855\n",
            "train loss:0.0005147835913721443\n",
            "train loss:0.00024619487154168407\n",
            "train loss:0.00299955130131474\n",
            "train loss:0.006086356031807751\n",
            "train loss:6.814229967084704e-05\n",
            "train loss:0.0001706951796431018\n",
            "train loss:0.0006412128563626411\n",
            "train loss:3.097635193490669e-05\n",
            "train loss:0.00197490863457915\n",
            "train loss:0.005995646760863555\n",
            "train loss:0.0005259964469679235\n",
            "train loss:0.0019945800946284064\n",
            "train loss:0.001515595399030106\n",
            "train loss:0.0003943626680765465\n",
            "train loss:0.000355163948354089\n",
            "train loss:7.962607085685597e-05\n",
            "train loss:0.006482926952117005\n",
            "train loss:0.0013642629719700975\n",
            "train loss:0.0003829784204014468\n",
            "train loss:0.0022499958969533483\n",
            "train loss:0.00029132979165671664\n",
            "train loss:0.0021670445981379987\n",
            "train loss:0.0013949716239349877\n",
            "train loss:0.0008443020247162019\n",
            "train loss:0.00017884774892594593\n",
            "train loss:0.0006450358567963002\n",
            "train loss:0.006854204398710257\n",
            "train loss:0.0008153297675159081\n",
            "train loss:0.002866293298649104\n",
            "train loss:0.00030670535248869334\n",
            "train loss:0.001092893032105281\n",
            "train loss:0.0009722217631756867\n",
            "train loss:0.00026284830986034093\n",
            "train loss:0.0004137470189829383\n",
            "train loss:0.00011300147440502177\n",
            "train loss:0.0007461523241225619\n",
            "train loss:0.00389114512769108\n",
            "train loss:0.000987781513164182\n",
            "train loss:0.0009165861839355163\n",
            "train loss:0.00428681723922455\n",
            "train loss:4.650066407912182e-05\n",
            "train loss:0.0244198101420185\n",
            "train loss:0.0015644738188421165\n",
            "train loss:0.0007103783487834352\n",
            "train loss:0.004222251952695085\n",
            "train loss:0.00038987803254870464\n",
            "train loss:0.0018442088549118383\n",
            "train loss:0.0005865995241100059\n",
            "train loss:0.001220661433188575\n",
            "train loss:0.0013642804997764113\n",
            "train loss:0.0009768094181433673\n",
            "train loss:0.0002099615362556337\n",
            "train loss:0.00037541845373143657\n",
            "train loss:0.0017638814068343604\n",
            "train loss:0.04152270710354311\n",
            "train loss:0.0001544112337488417\n",
            "train loss:0.0007440026537612098\n",
            "train loss:0.00013092630591763978\n",
            "train loss:0.0005559667095844402\n",
            "train loss:0.0004757965761865348\n",
            "train loss:0.00024400496706120523\n",
            "train loss:0.0013077427649427081\n",
            "train loss:0.00659547293533346\n",
            "train loss:0.000155147959823209\n",
            "train loss:0.0003119151123939876\n",
            "train loss:0.00010803240645681566\n",
            "train loss:0.0020582844108740905\n",
            "train loss:0.0001084752868827476\n",
            "train loss:0.000588638379896043\n",
            "train loss:0.015738966536128617\n",
            "train loss:0.0007473803038344957\n",
            "train loss:0.0009715510999585321\n",
            "train loss:0.0001802435206237949\n",
            "train loss:0.0005243827820574976\n",
            "train loss:0.009505445676217961\n",
            "train loss:0.00011310360301676438\n",
            "train loss:0.0004135552271115974\n",
            "train loss:0.0008607764448744458\n",
            "train loss:9.309246522430088e-05\n",
            "train loss:0.002950765689705584\n",
            "train loss:1.1232822728063294e-05\n",
            "train loss:0.0002025370259138038\n",
            "train loss:0.001006991549686997\n",
            "train loss:0.00012135866549810283\n",
            "train loss:0.0006097615472807308\n",
            "train loss:0.0034345533401783836\n",
            "train loss:0.008591979202380872\n",
            "train loss:0.00015436269525127337\n",
            "train loss:0.028295707139241736\n",
            "train loss:0.00019852206373267657\n",
            "train loss:0.001702138945494355\n",
            "train loss:0.0032446367021869028\n",
            "train loss:0.014281669205801942\n",
            "train loss:0.00025576202290845734\n",
            "train loss:0.0014893505607590342\n",
            "train loss:0.0012526063956810008\n",
            "train loss:0.002154212926872454\n",
            "train loss:0.0064933364379483795\n",
            "train loss:0.004019654673389904\n",
            "train loss:0.0022613592094189294\n",
            "train loss:0.00025426614960490334\n",
            "train loss:0.0007244957209075243\n",
            "train loss:0.0002348999074196379\n",
            "train loss:0.00027884148733562064\n",
            "train loss:0.0032604618988915507\n",
            "train loss:0.0007199200993032972\n",
            "train loss:4.797344140507442e-05\n",
            "train loss:0.0069494950287262965\n",
            "train loss:0.0023654044105804748\n",
            "train loss:0.0008406124591872755\n",
            "train loss:0.004919861675387514\n",
            "train loss:0.002380815863431275\n",
            "train loss:0.0021985377118414707\n",
            "train loss:0.0030704535551965557\n",
            "train loss:0.001456401278926709\n",
            "train loss:0.0001752969064159797\n",
            "train loss:5.586301995240304e-05\n",
            "train loss:0.0018365911568509366\n",
            "train loss:0.0010290771108453815\n",
            "train loss:0.0005171851956642631\n",
            "train loss:0.0036083206126739154\n",
            "train loss:0.004898173520486772\n",
            "train loss:0.0010848596432655676\n",
            "train loss:0.0005724652245090745\n",
            "train loss:0.002584145444541808\n",
            "train loss:0.0003731444353019216\n",
            "train loss:0.006000923345425859\n",
            "train loss:0.0021381023355492223\n",
            "train loss:0.007074851625674942\n",
            "train loss:0.00021020840766934013\n",
            "train loss:0.00043683784287176596\n",
            "train loss:0.002052881093942699\n",
            "train loss:0.0012503343835546898\n",
            "train loss:0.002863767061765757\n",
            "train loss:0.0013429831112599456\n",
            "train loss:0.006562715065333244\n",
            "train loss:0.0016273046980952493\n",
            "train loss:0.0016401889518954215\n",
            "train loss:0.0005313078354463908\n",
            "train loss:0.0033191020894528483\n",
            "train loss:0.000788650605221665\n",
            "train loss:0.0002660997311127472\n",
            "train loss:0.00020051913797863825\n",
            "train loss:0.0013639249478640913\n",
            "train loss:0.002598653881758674\n",
            "train loss:0.0005714729066420182\n",
            "train loss:0.00044228868579508656\n",
            "train loss:0.003199310935391735\n",
            "train loss:0.0009949086055453563\n",
            "train loss:0.0023183813536515694\n",
            "train loss:0.0021195829629390957\n",
            "train loss:0.006826083885291915\n",
            "train loss:0.0005343736038823916\n",
            "train loss:0.001482111945693593\n",
            "train loss:0.0011821081243374783\n",
            "train loss:0.00420036152566625\n",
            "train loss:0.0001230942695594761\n",
            "train loss:0.00015406007821283947\n",
            "train loss:0.00341514877970523\n",
            "train loss:0.0033104831270019163\n",
            "train loss:0.0011262984123869594\n",
            "train loss:0.0008143544668651536\n",
            "train loss:0.0013485519261654862\n",
            "train loss:0.001191470637101842\n",
            "train loss:0.0008739475142528933\n",
            "train loss:0.0005970781010205608\n",
            "train loss:0.00010682366733038163\n",
            "train loss:0.00023243841912026092\n",
            "train loss:0.0015728414794401921\n",
            "train loss:0.0015620616111386649\n",
            "train loss:0.00013618445549751692\n",
            "train loss:0.001024523451900998\n",
            "train loss:0.002401661132685652\n",
            "train loss:0.0016398506879928673\n",
            "train loss:0.0002687886598755931\n",
            "train loss:0.001818715573104184\n",
            "train loss:0.0033955031852992345\n",
            "train loss:0.0014333530675477307\n",
            "train loss:0.001152019867056018\n",
            "train loss:0.0009025281572576951\n",
            "train loss:0.01284296165406134\n",
            "train loss:0.002146762079778148\n",
            "train loss:0.001225724607461361\n",
            "train loss:0.000492817986555934\n",
            "train loss:0.00042310929412964617\n",
            "train loss:0.002275615803732428\n",
            "train loss:0.0009897128245104018\n",
            "train loss:0.0008970921361004848\n",
            "train loss:0.0026405068289749173\n",
            "train loss:0.005972038682557602\n",
            "train loss:0.004389129737385323\n",
            "train loss:0.0005778526228810702\n",
            "train loss:0.00013745122767952662\n",
            "train loss:0.00035553685391487007\n",
            "train loss:0.0008284161952773781\n",
            "train loss:0.00024094174684189832\n",
            "train loss:0.002809388538192456\n",
            "train loss:0.0011359218301405822\n",
            "train loss:0.00025232336552363115\n",
            "train loss:0.0002512978740106006\n",
            "train loss:0.016551351182763418\n",
            "train loss:0.005459045227609095\n",
            "train loss:0.003920095941386123\n",
            "train loss:0.0004486910558610896\n",
            "train loss:0.00016606214260700885\n",
            "train loss:0.0008429215007205823\n",
            "train loss:0.0010908405084012114\n",
            "train loss:0.001824707919481547\n",
            "train loss:0.001407373275283699\n",
            "train loss:0.002517765431917532\n",
            "train loss:0.0009804840375715856\n",
            "train loss:0.0007067371281850883\n",
            "train loss:0.00019575994672847797\n",
            "train loss:0.0013123071360078293\n",
            "train loss:0.001346872351096656\n",
            "train loss:0.0019016871009377705\n",
            "train loss:0.0020391764993882714\n",
            "train loss:0.05363599809363402\n",
            "train loss:0.001883764335317945\n",
            "train loss:0.0015382349204575143\n",
            "train loss:0.00038096731218050016\n",
            "train loss:0.0018768052254748314\n",
            "train loss:0.0011071031980057736\n",
            "train loss:0.0020629133254218093\n",
            "train loss:0.0004937671734185704\n",
            "train loss:4.0924455441821595e-05\n",
            "train loss:0.0016650442120462283\n",
            "train loss:0.003373538063913159\n",
            "train loss:0.00011248305672472228\n",
            "train loss:0.003408774690925884\n",
            "train loss:0.0004541314158907869\n",
            "train loss:0.0013089999388190513\n",
            "train loss:0.0008266266146398224\n",
            "train loss:0.00011508245453927297\n",
            "train loss:0.0010275074431443506\n",
            "train loss:0.018417150875025448\n",
            "train loss:0.005074107612570409\n",
            "train loss:1.988489411463438e-05\n",
            "train loss:0.0003907156667594607\n",
            "train loss:0.0017966919495881182\n",
            "train loss:0.0015433918723583556\n",
            "train loss:0.005437250944019103\n",
            "train loss:0.0012710523310700311\n",
            "train loss:0.005296927674118765\n",
            "train loss:5.551423969588386e-05\n",
            "train loss:0.0002124979236116849\n",
            "train loss:0.0008044582976400146\n",
            "train loss:9.299700929156935e-05\n",
            "train loss:0.0002964334712548124\n",
            "train loss:0.0017304975629382623\n",
            "train loss:0.0010187280041599545\n",
            "train loss:0.0026075478624893113\n",
            "train loss:0.003125272449186225\n",
            "train loss:0.00024998986956425106\n",
            "train loss:0.008402296943378176\n",
            "train loss:0.003047782530697163\n",
            "train loss:0.0009037445865907222\n",
            "train loss:0.0008944548604152682\n",
            "train loss:0.00012899327612567695\n",
            "train loss:0.0002676063684626675\n",
            "train loss:0.0007376699612913263\n",
            "train loss:0.00021785874962742784\n",
            "train loss:0.00026656127934132705\n",
            "train loss:0.0015462139394826855\n",
            "train loss:0.006406599716935216\n",
            "train loss:0.0023771308287298207\n",
            "train loss:0.0001332155981104956\n",
            "train loss:0.00017676783356077998\n",
            "train loss:0.0012835609761036035\n",
            "train loss:0.0004505404144931615\n",
            "train loss:0.0016550427154413247\n",
            "train loss:0.0008082649772257146\n",
            "=== epoch:20, train acc:0.998, test acc:0.986 ===\n",
            "train loss:0.0006807196059335516\n",
            "train loss:0.00038317499697701506\n",
            "train loss:0.0019013158933231377\n",
            "train loss:0.0018651588429953378\n",
            "train loss:0.00018270641814068355\n",
            "train loss:0.00037224778200491333\n",
            "train loss:0.0013905069468455999\n",
            "train loss:0.00023931892268889038\n",
            "train loss:0.0025893654125665228\n",
            "train loss:0.0037405795632408177\n",
            "train loss:9.455753546015019e-05\n",
            "train loss:0.00017750191436943647\n",
            "train loss:0.000537684389680183\n",
            "train loss:0.00017274627140331555\n",
            "train loss:0.0004870738107611835\n",
            "train loss:0.001198173029884999\n",
            "train loss:0.0011646837125185937\n",
            "train loss:0.0010033173348159168\n",
            "train loss:0.00020308422968031524\n",
            "train loss:0.0015688621366128721\n",
            "train loss:0.00021301195150731653\n",
            "train loss:0.00019109939176475025\n",
            "train loss:0.0020855178763823983\n",
            "train loss:0.0007086557216053737\n",
            "train loss:0.0003679056046177194\n",
            "train loss:0.0023376415479765766\n",
            "train loss:0.0002839758281916583\n",
            "train loss:0.006081304550879779\n",
            "train loss:0.00014728402786663372\n",
            "train loss:0.001759654129306776\n",
            "train loss:0.0009897766280945726\n",
            "train loss:0.0014890772114762208\n",
            "train loss:0.00020731395793961144\n",
            "train loss:0.0002035396014277089\n",
            "train loss:0.0027088872298095708\n",
            "train loss:0.0009630851665304198\n",
            "train loss:0.0005840858803655059\n",
            "train loss:0.00104237445483894\n",
            "train loss:0.0002622807107211729\n",
            "train loss:0.0006704529257034885\n",
            "train loss:0.0015037161066780418\n",
            "train loss:0.0008368244609777808\n",
            "train loss:0.001487949670139103\n",
            "train loss:0.00018128742933375036\n",
            "train loss:0.0005781716523281991\n",
            "train loss:0.0013541446870805884\n",
            "train loss:0.0021977416033827813\n",
            "train loss:0.0008258258122663684\n",
            "train loss:0.0020310103281058456\n",
            "train loss:5.579328853869323e-05\n",
            "train loss:0.00022625527705326688\n",
            "train loss:0.007737839854297712\n",
            "train loss:0.0019058561769458332\n",
            "train loss:0.0022764763270553805\n",
            "train loss:0.002700805144620921\n",
            "train loss:0.005278220640659526\n",
            "train loss:0.0028224632775521495\n",
            "train loss:0.00297992761088343\n",
            "train loss:0.0004075762648042137\n",
            "train loss:0.0034889903448592657\n",
            "train loss:0.003493455313372279\n",
            "train loss:0.00035062854383086025\n",
            "train loss:4.2660290716195944e-05\n",
            "train loss:0.0016414285089621664\n",
            "train loss:0.0008645689046015281\n",
            "train loss:0.0009780077688311165\n",
            "train loss:0.001542028765424133\n",
            "train loss:0.0013650793968518935\n",
            "train loss:0.0005941514158679916\n",
            "train loss:0.0004862872984535471\n",
            "train loss:0.0004075379678095211\n",
            "train loss:0.0012246146116028209\n",
            "train loss:0.0015118175093411896\n",
            "train loss:0.0022482482851494613\n",
            "train loss:0.0018980098134568754\n",
            "train loss:0.0002513359442785721\n",
            "train loss:0.0010866399357033845\n",
            "train loss:0.00036404248029841776\n",
            "train loss:0.000977488500130875\n",
            "train loss:0.0004413093298512102\n",
            "train loss:0.0012725677227906898\n",
            "train loss:0.0013332013213280237\n",
            "train loss:0.00037433248990380796\n",
            "train loss:0.003884463493579465\n",
            "train loss:2.6374012151231483e-05\n",
            "train loss:0.003776358973665903\n",
            "train loss:0.0007103682255204183\n",
            "train loss:0.0004165167822872302\n",
            "train loss:0.000493531242242833\n",
            "train loss:0.0008937035222151904\n",
            "train loss:0.0004726944870916474\n",
            "train loss:0.0016459657976719438\n",
            "train loss:0.0004345780036135507\n",
            "train loss:0.00011965389338245763\n",
            "train loss:0.004358925477022002\n",
            "train loss:0.002878462323130683\n",
            "train loss:0.000261117793036429\n",
            "train loss:0.00037230147898360876\n",
            "train loss:0.0007541559192117909\n",
            "train loss:0.0038199970542423843\n",
            "train loss:1.5461282759992626e-05\n",
            "train loss:0.0030314870327951588\n",
            "train loss:0.00041146481836869443\n",
            "train loss:0.002652925011984212\n",
            "train loss:7.878038996971151e-05\n",
            "train loss:3.730820345221478e-05\n",
            "train loss:0.00010965983507394161\n",
            "train loss:0.00048396722958503997\n",
            "train loss:0.001166223786586081\n",
            "train loss:0.003087569319139777\n",
            "train loss:0.001748759585424167\n",
            "train loss:0.0017774215508078086\n",
            "train loss:0.0001018336229325031\n",
            "train loss:0.014066306343849698\n",
            "train loss:0.00247428122003128\n",
            "train loss:0.00024005351232452157\n",
            "train loss:0.002330440846451718\n",
            "train loss:6.868116006169159e-05\n",
            "train loss:0.009174514015120185\n",
            "train loss:0.0011438028353260173\n",
            "train loss:0.0004272235163807312\n",
            "train loss:0.002611581657139194\n",
            "train loss:0.01359287620685447\n",
            "train loss:0.0184082814523806\n",
            "train loss:0.0018284657861008604\n",
            "train loss:0.001539086567360641\n",
            "train loss:0.0003044941985709676\n",
            "train loss:0.0008929077232617123\n",
            "train loss:0.00025585957218997166\n",
            "train loss:0.002185776326449197\n",
            "train loss:0.003599410150803352\n",
            "train loss:0.00010388375422046528\n",
            "train loss:5.225479039958193e-05\n",
            "train loss:0.00011153165554865988\n",
            "train loss:8.885515481427367e-05\n",
            "train loss:0.00028161424606780054\n",
            "train loss:0.012572519982091322\n",
            "train loss:0.0009871156402358567\n",
            "train loss:0.0008679011572628276\n",
            "train loss:8.18961178203613e-05\n",
            "train loss:0.018093208915115445\n",
            "train loss:0.0023951110502875616\n",
            "train loss:0.0009178491316479214\n",
            "train loss:0.0012846361683530672\n",
            "train loss:0.0008098986312246126\n",
            "train loss:0.0015452106979112138\n",
            "train loss:0.003733235154722807\n",
            "train loss:0.00011965490759783474\n",
            "train loss:0.010817705117140182\n",
            "train loss:0.0012096379901170818\n",
            "train loss:0.0003119458947207055\n",
            "train loss:0.0006914859347688238\n",
            "train loss:0.00032541773112720293\n",
            "train loss:0.00014538789907848492\n",
            "train loss:0.0018420591977556014\n",
            "train loss:0.0013910062703118506\n",
            "train loss:0.00042386240500185703\n",
            "train loss:2.2031814127273547e-05\n",
            "train loss:0.00010262258963710869\n",
            "train loss:0.00018102071310088528\n",
            "train loss:0.0012314963797546215\n",
            "train loss:0.0006745830740448747\n",
            "train loss:0.0006627326632938525\n",
            "train loss:0.0009973105409584997\n",
            "train loss:0.002340444114479104\n",
            "train loss:0.016927225521214336\n",
            "train loss:0.004720152086439717\n",
            "train loss:7.831113371191974e-05\n",
            "train loss:8.067178055078122e-05\n",
            "train loss:0.00020372606624527\n",
            "train loss:0.0003964196644380847\n",
            "train loss:0.0011820186675297203\n",
            "train loss:0.008847181981635198\n",
            "train loss:0.004455337633802795\n",
            "train loss:0.0002524223231959055\n",
            "train loss:0.003057795179532076\n",
            "train loss:0.00017540620871671104\n",
            "train loss:0.0008189238619761102\n",
            "train loss:0.0007804239498419338\n",
            "train loss:0.005629481966486711\n",
            "train loss:0.000838449965264089\n",
            "train loss:0.00336344059976037\n",
            "train loss:0.000431317914751991\n",
            "train loss:0.00189342898000425\n",
            "train loss:0.0013996901265280243\n",
            "train loss:0.0007910212864402092\n",
            "train loss:0.00030328061143559856\n",
            "train loss:0.00022440451399564988\n",
            "train loss:0.0008430634499379278\n",
            "train loss:9.317367424109329e-05\n",
            "train loss:0.00018904570935366174\n",
            "train loss:0.002807964824794397\n",
            "train loss:0.000542437406429161\n",
            "train loss:0.0006482795551587684\n",
            "train loss:7.537545877092499e-05\n",
            "train loss:0.001502112556922032\n",
            "train loss:0.002139035498324461\n",
            "train loss:0.0007062249226680877\n",
            "train loss:0.0019589379293363986\n",
            "train loss:0.00010698637957828801\n",
            "train loss:1.392257590127235e-05\n",
            "train loss:0.00019313921450002502\n",
            "train loss:2.5766525568982605e-05\n",
            "train loss:0.0005028159111874994\n",
            "train loss:0.001587178282207469\n",
            "train loss:0.0038163421738647858\n",
            "train loss:0.0010690374741472268\n",
            "train loss:0.0008753113599247679\n",
            "train loss:0.0005513332860317389\n",
            "train loss:0.0014420347045872382\n",
            "train loss:0.002491315593948884\n",
            "train loss:6.285347456186176e-05\n",
            "train loss:0.0013874016757461666\n",
            "train loss:0.0013989452396682672\n",
            "train loss:0.001935502162447462\n",
            "train loss:0.0007523474432991275\n",
            "train loss:0.0004154317630674514\n",
            "train loss:0.002112846630678109\n",
            "train loss:0.00010185806374636958\n",
            "train loss:0.00014664965204800627\n",
            "train loss:0.0006103487120928297\n",
            "train loss:3.170859454533158e-05\n",
            "train loss:0.0004487564008034409\n",
            "train loss:0.001487409105173058\n",
            "train loss:0.00017758573729374018\n",
            "train loss:4.766009202134919e-05\n",
            "train loss:0.001550678714042645\n",
            "train loss:0.0004361733547562434\n",
            "train loss:0.0009407588102372823\n",
            "train loss:0.0006772683035029978\n",
            "train loss:0.0002687696665967552\n",
            "train loss:0.0072557493011158\n",
            "train loss:0.0001634065053346349\n",
            "train loss:0.0016211179046452225\n",
            "train loss:0.0010553744000272134\n",
            "train loss:0.0008372241341963542\n",
            "train loss:0.0033124497734635726\n",
            "train loss:0.00019368613609654833\n",
            "train loss:0.00010300909966310014\n",
            "train loss:0.0022974205594989976\n",
            "train loss:0.00020087419799171366\n",
            "train loss:0.003903636448548868\n",
            "train loss:0.00039742992522180095\n",
            "train loss:0.0005063031627788555\n",
            "train loss:0.0008313965207224547\n",
            "train loss:0.0010632099892405938\n",
            "train loss:0.019092836271262733\n",
            "train loss:0.010926524685008447\n",
            "train loss:0.0020825099229468914\n",
            "train loss:0.0008679720213360266\n",
            "train loss:0.0007308079697148939\n",
            "train loss:0.012450115827150417\n",
            "train loss:0.0002759705767245004\n",
            "train loss:0.0010424708215575832\n",
            "train loss:0.00029520677377855605\n",
            "train loss:0.0002395098422515217\n",
            "train loss:0.00023341385642044985\n",
            "train loss:0.0007134335243395589\n",
            "train loss:0.00012485146423115635\n",
            "train loss:0.000542577137962471\n",
            "train loss:0.0008426535082558624\n",
            "train loss:0.0002939230064304138\n",
            "train loss:5.917739357286792e-05\n",
            "train loss:0.0028260959760312105\n",
            "train loss:0.001582257816432926\n",
            "train loss:0.0013218219981232316\n",
            "train loss:0.00019661554351366656\n",
            "train loss:0.001724806207808119\n",
            "train loss:0.00012213434468792887\n",
            "train loss:0.0010512915553635225\n",
            "train loss:5.4863580452963486e-05\n",
            "train loss:0.00024930501336450564\n",
            "train loss:0.00027408926096430115\n",
            "train loss:6.422338773912794e-05\n",
            "train loss:0.01194802797914065\n",
            "train loss:0.000362580219975557\n",
            "train loss:0.00034842104929863225\n",
            "train loss:0.0002961932891651799\n",
            "train loss:0.00013033025404540552\n",
            "train loss:0.00020026646271092838\n",
            "train loss:0.0008405980718530269\n",
            "train loss:8.87323955933011e-05\n",
            "train loss:0.00019501128402051265\n",
            "train loss:0.00013487760587210419\n",
            "train loss:0.0003249164720687994\n",
            "train loss:2.8991995733293084e-05\n",
            "train loss:0.0002677505872512676\n",
            "train loss:0.0006542749686528964\n",
            "train loss:0.00021344547270725513\n",
            "train loss:1.1887115721297904e-05\n",
            "train loss:0.0003013882405980176\n",
            "train loss:0.000988537188828356\n",
            "train loss:2.980673447743832e-05\n",
            "train loss:0.00017212140920138967\n",
            "train loss:0.002583687214060033\n",
            "train loss:0.0006464490427679013\n",
            "train loss:4.9131192610055375e-05\n",
            "train loss:0.00020083247825798887\n",
            "train loss:0.00017997565890082414\n",
            "train loss:0.0006802214534858279\n",
            "train loss:0.00022767285945639505\n",
            "train loss:0.0007724736045885867\n",
            "train loss:0.0020232083973303326\n",
            "train loss:0.0004197384919046466\n",
            "train loss:0.001667501772115167\n",
            "train loss:0.0017584191852467123\n",
            "train loss:0.0018784972629441196\n",
            "train loss:0.0007383694542779993\n",
            "train loss:0.0006835391736619696\n",
            "train loss:0.0003653238462986219\n",
            "train loss:0.0006683425489521782\n",
            "train loss:0.001646305634946799\n",
            "train loss:0.0038447097881333593\n",
            "train loss:0.0005193710595492815\n",
            "train loss:0.0010400121706649356\n",
            "train loss:0.00011482967787249568\n",
            "train loss:3.3192734462107706e-05\n",
            "train loss:3.026629733023169e-05\n",
            "train loss:4.5903538113004443e-05\n",
            "train loss:5.7785934119664395e-05\n",
            "train loss:3.919311000059549e-05\n",
            "train loss:0.0003792615935658559\n",
            "train loss:0.0007787912425990473\n",
            "train loss:0.00012998733630724857\n",
            "train loss:8.112025734172228e-05\n",
            "train loss:0.0016290405244341682\n",
            "train loss:0.0045047242339093215\n",
            "train loss:0.00019642269230920565\n",
            "train loss:0.00040617935319767944\n",
            "train loss:0.002367388134652878\n",
            "train loss:0.0011005355159118602\n",
            "train loss:8.548291111673336e-05\n",
            "train loss:0.0011320495602231589\n",
            "train loss:0.0008480367678310313\n",
            "train loss:0.0003388604538875214\n",
            "train loss:0.0015898433876492257\n",
            "train loss:7.281631840105334e-05\n",
            "train loss:0.00045047880292297987\n",
            "train loss:0.00018188883816842887\n",
            "train loss:2.687161097519004e-05\n",
            "train loss:0.0022367463584047178\n",
            "train loss:0.010416453207343327\n",
            "train loss:0.00011285678460565911\n",
            "train loss:0.004406944507262007\n",
            "train loss:0.0012734497449974388\n",
            "train loss:0.0003877505225758253\n",
            "train loss:0.0018054616976628166\n",
            "train loss:5.355192567124167e-05\n",
            "train loss:0.008108644397121306\n",
            "train loss:0.0002012698114694547\n",
            "train loss:2.3011779913250894e-05\n",
            "train loss:0.0006593124409434929\n",
            "train loss:0.00025552083704820735\n",
            "train loss:0.00010550587447317974\n",
            "train loss:0.002000661437341282\n",
            "train loss:0.00011381209923618173\n",
            "train loss:0.0025685576057037744\n",
            "train loss:0.0004322708031501713\n",
            "train loss:0.002229998262433367\n",
            "train loss:0.0030027751500047166\n",
            "train loss:0.00012060924558677162\n",
            "train loss:0.001062199577550899\n",
            "train loss:0.0029807588191519013\n",
            "train loss:0.00103113488664538\n",
            "train loss:0.00578520295541158\n",
            "train loss:0.00015169233606830102\n",
            "train loss:0.002175804817780235\n",
            "train loss:4.649528203367141e-05\n",
            "train loss:0.0009919016675477713\n",
            "train loss:0.0018896129554703495\n",
            "train loss:0.00024097782433171754\n",
            "train loss:8.925204206170089e-05\n",
            "train loss:0.0003305554168060301\n",
            "train loss:9.538411814429064e-05\n",
            "train loss:0.002493998436584764\n",
            "train loss:0.0036399070897036212\n",
            "train loss:0.0016639784710009745\n",
            "train loss:0.0002895575923818139\n",
            "train loss:0.0002758061927689562\n",
            "train loss:5.373967381007544e-05\n",
            "train loss:0.01916599333584382\n",
            "train loss:0.0006556558966074313\n",
            "train loss:0.00010264194674132835\n",
            "train loss:0.0010480422282483103\n",
            "train loss:8.705831408070215e-05\n",
            "train loss:0.0014446018917533832\n",
            "train loss:0.0007591201636768429\n",
            "train loss:0.001738429006242864\n",
            "train loss:0.0004623707667027338\n",
            "train loss:0.0004992659724201977\n",
            "train loss:2.1077954174611568e-05\n",
            "train loss:0.0021904295495915804\n",
            "train loss:0.0019039279386848088\n",
            "train loss:0.0010299367881463739\n",
            "train loss:0.0005020095826850953\n",
            "train loss:0.002991021518494651\n",
            "train loss:0.0017425720664255358\n",
            "train loss:0.0004431658477439168\n",
            "train loss:0.00014809659127286635\n",
            "train loss:4.2464643022702995e-05\n",
            "train loss:0.0006931249556975827\n",
            "train loss:0.0007823938228420354\n",
            "train loss:0.0007568901516876371\n",
            "train loss:0.002243129761233193\n",
            "train loss:0.00011462974430890519\n",
            "train loss:0.0019568880314919405\n",
            "train loss:0.0008772440615004691\n",
            "train loss:0.0011125341928462202\n",
            "train loss:0.001190902754269091\n",
            "train loss:0.0006978507497824712\n",
            "train loss:0.0012616811950351451\n",
            "train loss:0.00031945357208253866\n",
            "train loss:2.200858608628525e-05\n",
            "train loss:0.0013597839101478345\n",
            "train loss:8.330110538441374e-05\n",
            "train loss:0.0019316156948843044\n",
            "train loss:0.0003558903087699567\n",
            "train loss:0.0005018485170045536\n",
            "train loss:0.0028293473309253738\n",
            "train loss:0.0006234870602666705\n",
            "train loss:0.0013277869969448499\n",
            "train loss:0.00023204102895787215\n",
            "train loss:0.00012048190065549665\n",
            "train loss:0.0012252012154613191\n",
            "train loss:0.0036653827059400885\n",
            "train loss:0.003337986231013042\n",
            "train loss:0.007212908622586496\n",
            "train loss:0.0011038544753084487\n",
            "train loss:0.0011963484857548154\n",
            "train loss:0.001564067959104358\n",
            "train loss:0.0004659155183954842\n",
            "train loss:7.453420409441712e-05\n",
            "train loss:0.0012148590838681878\n",
            "train loss:0.00674357635789521\n",
            "train loss:0.005055674015345759\n",
            "train loss:0.00013065152194473962\n",
            "train loss:0.00010191846235138212\n",
            "train loss:9.5884698491404e-05\n",
            "train loss:0.00024006134243905592\n",
            "train loss:0.0014132120934977654\n",
            "train loss:7.588621840686667e-05\n",
            "train loss:0.0024483159067798664\n",
            "train loss:0.006445719267093969\n",
            "train loss:0.0007660644066679501\n",
            "train loss:0.0017111743164255427\n",
            "train loss:0.0013221357987334298\n",
            "train loss:0.0009566673313504216\n",
            "train loss:0.003354128237723798\n",
            "train loss:0.0026466534417424483\n",
            "train loss:0.000272343721538452\n",
            "train loss:0.00046926665461062577\n",
            "train loss:0.0024740484485049822\n",
            "train loss:0.000755953011262382\n",
            "train loss:0.00123879593940218\n",
            "train loss:0.0014927606934742087\n",
            "train loss:0.0026712610568027724\n",
            "train loss:0.003403515972180513\n",
            "train loss:0.002137606661914348\n",
            "train loss:0.0014504720986907233\n",
            "train loss:0.0007682067840730342\n",
            "train loss:0.0007148134576480225\n",
            "train loss:0.0059823737427072395\n",
            "train loss:0.0008138753886056491\n",
            "train loss:0.001083099231950827\n",
            "train loss:0.002873665202026052\n",
            "train loss:0.0028416541350175274\n",
            "train loss:6.559766605604496e-05\n",
            "train loss:0.00144374484550696\n",
            "train loss:0.0011649403778934754\n",
            "train loss:0.0005949032728475852\n",
            "train loss:3.278058362841305e-05\n",
            "train loss:0.001218576360794254\n",
            "train loss:0.0010510862943566008\n",
            "train loss:0.0015182725393441884\n",
            "train loss:0.0004913207292941298\n",
            "train loss:0.0002412778857942909\n",
            "train loss:0.000736315447832094\n",
            "train loss:4.777464625284581e-05\n",
            "train loss:0.0010997445397561282\n",
            "train loss:0.0005672713745793225\n",
            "train loss:2.5861990615785306e-05\n",
            "train loss:0.001083985461103537\n",
            "train loss:7.589586050547799e-06\n",
            "train loss:0.0004572922569323311\n",
            "train loss:0.0013860967309356348\n",
            "train loss:0.0020474521492258478\n",
            "train loss:0.00010364648353349512\n",
            "train loss:8.007114157695619e-05\n",
            "train loss:0.0003676240603521823\n",
            "train loss:0.0002142104365711514\n",
            "train loss:0.00012027490272391422\n",
            "train loss:0.0023561555507503034\n",
            "train loss:0.0004430863721926083\n",
            "train loss:0.0013370093701620526\n",
            "train loss:0.000460723080245893\n",
            "train loss:0.0004099363305977948\n",
            "train loss:0.0002843335951409016\n",
            "train loss:0.0004204192784525451\n",
            "train loss:0.0011623151141283668\n",
            "train loss:0.00022289379547543045\n",
            "train loss:6.720809683437451e-05\n",
            "train loss:0.0009533234540645355\n",
            "train loss:0.0013918747668725138\n",
            "train loss:0.00010307851658966015\n",
            "train loss:2.7724995314520958e-05\n",
            "train loss:0.0005389008203088072\n",
            "train loss:0.0018842458280283352\n",
            "train loss:0.00033399208116228465\n",
            "train loss:0.00013909056206270389\n",
            "train loss:4.3108775588646354e-05\n",
            "train loss:0.00010371882317190034\n",
            "train loss:0.004049400288730541\n",
            "train loss:0.001011832055935772\n",
            "train loss:0.0003840115217398651\n",
            "train loss:0.00024860651136757704\n",
            "train loss:0.001978356437933976\n",
            "train loss:2.356671979471788e-05\n",
            "train loss:0.00036020505424642967\n",
            "train loss:0.00027495870465815296\n",
            "train loss:0.00015300623974542338\n",
            "train loss:0.00121619482453499\n",
            "train loss:0.0004547101482093318\n",
            "train loss:0.00023458993889558625\n",
            "train loss:0.0012273186157111533\n",
            "train loss:0.00025573077933175746\n",
            "train loss:0.001971176967562045\n",
            "train loss:0.0011886758325027598\n",
            "train loss:0.00046547995441463297\n",
            "train loss:0.000930100855145901\n",
            "train loss:0.0002880273922250628\n",
            "train loss:0.0011146033007218175\n",
            "train loss:0.00030260168937003854\n",
            "train loss:9.893281296678025e-05\n",
            "train loss:8.453234482009698e-05\n",
            "train loss:0.00035508891214028777\n",
            "train loss:0.0010708933893686754\n",
            "train loss:0.00042242296103712904\n",
            "train loss:0.00041001478362022665\n",
            "train loss:0.007275740735098238\n",
            "train loss:0.00025402777776905997\n",
            "train loss:6.444439938041636e-05\n",
            "train loss:0.00037377678599216673\n",
            "train loss:0.001236423008048367\n",
            "train loss:0.001507962628558\n",
            "train loss:0.0005862264634993291\n",
            "train loss:0.0018715743127989228\n",
            "train loss:0.0027118634578548375\n",
            "train loss:0.0015997104302397775\n",
            "train loss:0.002110354963415313\n",
            "train loss:0.0013350804329264935\n",
            "train loss:4.254478957605531e-05\n",
            "train loss:2.162207669593593e-05\n",
            "train loss:0.002571131874897255\n",
            "train loss:0.0003355708339141664\n",
            "train loss:0.00016910539624735764\n",
            "train loss:0.00011024025328809288\n",
            "train loss:0.00047217193380429725\n",
            "train loss:0.000369176174933143\n",
            "train loss:0.00022061195567881934\n",
            "train loss:7.788272852529343e-05\n",
            "train loss:0.0003435410896880764\n",
            "train loss:0.03761160899932706\n",
            "train loss:2.604622830711411e-06\n",
            "train loss:5.2240054663340384e-05\n",
            "train loss:0.001294152455342472\n",
            "train loss:8.428811844103223e-05\n",
            "train loss:0.0017968230353648657\n",
            "train loss:0.0008335595031744237\n",
            "train loss:0.0003360530038399425\n",
            "train loss:0.001443877950197055\n",
            "train loss:7.05961201429156e-05\n",
            "train loss:5.2202747846029805e-05\n",
            "train loss:0.0008051206005992738\n",
            "train loss:0.0001858896632870384\n",
            "train loss:0.0007629171909585538\n",
            "train loss:0.0012503610658700492\n",
            "train loss:4.180091421989312e-05\n",
            "train loss:0.00023606095673046274\n",
            "train loss:0.0005156778864809531\n",
            "train loss:0.0006279375643496196\n",
            "train loss:0.0005200300840118788\n",
            "train loss:0.0014071650404877537\n",
            "train loss:0.0001283720137338788\n",
            "train loss:0.0002324175042417047\n",
            "train loss:0.00018353113136356455\n",
            "train loss:0.003028887629186636\n",
            "train loss:0.0005374761283859747\n",
            "train loss:0.0004382056163344941\n",
            "train loss:0.00045510780426274446\n",
            "train loss:0.0013738554610288473\n",
            "train loss:0.00028762268760634585\n",
            "train loss:0.00041770481235642467\n",
            "train loss:0.0023637074637751503\n",
            "train loss:8.347077434469573e-05\n",
            "train loss:0.0005577240949302891\n",
            "train loss:0.0005620212969709292\n",
            "train loss:0.00011725490140309713\n",
            "train loss:0.0004641996807858584\n",
            "train loss:0.002848595044516109\n",
            "=============== Final Test Accuracy ===============\n",
            "test acc:0.9893\n",
            "Saved Network Parameters!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cnn시각화하기\n",
        "# 합성곱 계층은 입력으로 받은 이미지 데이터에서 무엇을 보고있는지 확인\n",
        "# 1번째 층의 가중치 시각화하기\n",
        "# 1번쨰 층의 합성곱 계층의 가중치는 그 형상이 (30,1,5,5) : 필터30개, 채널 1개, 5x5, 회색조1개채널\n",
        "# visualize_filter.py확인\n",
        "# 학습전 필터는 무작위로 초기화되고있어 흑백의정도에 규칙성이 없음\n",
        "# 학습을 다 마친 필터는 규칙성있는 이미지가 됨\n",
        "# 규치성있는 필터 : 에지(색상이 바뀐 경계선)와 블롭(국소적으로 덩어리진 영역)\n",
        "# 합성곱 계층의 필터는 에지나 블롭등의 원시적인 정보를 추출\n"
      ],
      "metadata": {
        "id": "pbMBOK0WD7Ek"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from simple_convnet import SimpleConvNet\n",
        "\n",
        "def filter_show(filters, nx=8, margin=3, scale=10):\n",
        "    \"\"\"\n",
        "    c.f. https://gist.github.com/aidiary/07d530d5e08011832b12#file-draw_weight-py\n",
        "    \"\"\"\n",
        "    FN, C, FH, FW = filters.shape\n",
        "    ny = int(np.ceil(FN / nx))\n",
        "\n",
        "    fig = plt.figure()\n",
        "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
        "\n",
        "    for i in range(FN):\n",
        "        ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[])\n",
        "        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "network = SimpleConvNet()\n",
        "# 무작위(랜덤) 초기화 후의 가중치\n",
        "filter_show(network.params['W1'])\n",
        "\n",
        "# 학습된 가중치\n",
        "network.load_params(\"params.pkl\")\n",
        "filter_show(network.params['W1'])\n",
        "\n"
      ],
      "metadata": {
        "id": "shVjg_rIOcje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 층 깊이에 따른 추출정보 변화\n",
        "# 계층이 깊어질수록 추출되는 정보는 더 추상화 됨\n",
        "# Ex) AlexNet : 합성곱 게층과 풀링 계층을 여러겹 쌓고 마지막으로 완전연결 계층을 거쳐 결과를 출력하는 구조\n",
        "# 단순한 모양에서 고급정보로 변환됨, 사물의 의미를 이해하도록 변화함"
      ],
      "metadata": {
        "id": "ToQU7Gx5OZNz"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CzivlIeu9ueN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ch8. 딥러닝 "
      ],
      "metadata": {
        "id": "bhmeQ1WncHqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VGG NET\n",
        "# 가중치 초깃값으로 HE초깃값을 사용\n",
        "# 가중치 매개변수 갱신에는 Adam을 사용\n",
        "# 3x3의 작은 필터를 사용한 합성곱 계층\n",
        "# 활성화 함수는 relu\n",
        "# ch8. deep_convnet.py / ch8. train_deepnet.py 참고\n",
        "# 학습된 가중치 매개변수를 deep_conv_net_params.pkl에 저장함"
      ],
      "metadata": {
        "id": "MxRAfC1eZ1DZ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
        "import pickle\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "from common.layers import *\n",
        "\n",
        "\n",
        "class DeepConvNet:\n",
        "    \"\"\"정확도 99% 이상의 고정밀 합성곱 신경망\n",
        "\n",
        "    네트워크 구성은 아래와 같음\n",
        "        conv - relu - conv- relu - pool -\n",
        "        conv - relu - conv- relu - pool -\n",
        "        conv - relu - conv- relu - pool -\n",
        "        affine - relu - dropout - affine - dropout - softmax\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=(1, 28, 28),\n",
        "                 conv_param_1 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n",
        "                 conv_param_2 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n",
        "                 conv_param_3 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\n",
        "                 conv_param_4 = {'filter_num':32, 'filter_size':3, 'pad':2, 'stride':1},\n",
        "                 conv_param_5 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
        "                 conv_param_6 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
        "                 hidden_size=50, output_size=10):\n",
        "        # 가중치 초기화===========\n",
        "        # 각 층의 뉴런 하나당 앞 층의 몇 개 뉴런과 연결되는가（TODO: 자동 계산되게 바꿀 것）\n",
        "        pre_node_nums = np.array([1*3*3, 16*3*3, 16*3*3, 32*3*3, 32*3*3, 64*3*3, 64*4*4, hidden_size])\n",
        "        wight_init_scales = np.sqrt(2.0 / pre_node_nums)  # ReLU를 사용할 때의 권장 초깃값\n",
        "        \n",
        "        self.params = {}\n",
        "        pre_channel_num = input_dim[0]\n",
        "        for idx, conv_param in enumerate([conv_param_1, conv_param_2, conv_param_3, conv_param_4, conv_param_5, conv_param_6]):\n",
        "            self.params['W' + str(idx+1)] = wight_init_scales[idx] * np.random.randn(conv_param['filter_num'], pre_channel_num, conv_param['filter_size'], conv_param['filter_size'])\n",
        "            self.params['b' + str(idx+1)] = np.zeros(conv_param['filter_num'])\n",
        "            pre_channel_num = conv_param['filter_num']\n",
        "        self.params['W7'] = wight_init_scales[6] * np.random.randn(64*4*4, hidden_size)\n",
        "        self.params['b7'] = np.zeros(hidden_size)\n",
        "        self.params['W8'] = wight_init_scales[7] * np.random.randn(hidden_size, output_size)\n",
        "        self.params['b8'] = np.zeros(output_size)\n",
        "\n",
        "        # 계층 생성===========\n",
        "        self.layers = []\n",
        "        self.layers.append(Convolution(self.params['W1'], self.params['b1'], \n",
        "                           conv_param_1['stride'], conv_param_1['pad']))\n",
        "        self.layers.append(Relu())\n",
        "        self.layers.append(Convolution(self.params['W2'], self.params['b2'], \n",
        "                           conv_param_2['stride'], conv_param_2['pad']))\n",
        "        self.layers.append(Relu())\n",
        "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
        "        self.layers.append(Convolution(self.params['W3'], self.params['b3'], \n",
        "                           conv_param_3['stride'], conv_param_3['pad']))\n",
        "        self.layers.append(Relu())\n",
        "        self.layers.append(Convolution(self.params['W4'], self.params['b4'],\n",
        "                           conv_param_4['stride'], conv_param_4['pad']))\n",
        "        self.layers.append(Relu())\n",
        "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
        "        self.layers.append(Convolution(self.params['W5'], self.params['b5'],\n",
        "                           conv_param_5['stride'], conv_param_5['pad']))\n",
        "        self.layers.append(Relu())\n",
        "        self.layers.append(Convolution(self.params['W6'], self.params['b6'],\n",
        "                           conv_param_6['stride'], conv_param_6['pad']))\n",
        "        self.layers.append(Relu())\n",
        "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
        "        self.layers.append(Affine(self.params['W7'], self.params['b7']))\n",
        "        self.layers.append(Relu())\n",
        "        self.layers.append(Dropout(0.5))\n",
        "        self.layers.append(Affine(self.params['W8'], self.params['b8']))\n",
        "        self.layers.append(Dropout(0.5))\n",
        "        \n",
        "        self.last_layer = SoftmaxWithLoss()\n",
        "\n",
        "    def predict(self, x, train_flg=False):\n",
        "        for layer in self.layers:\n",
        "            if isinstance(layer, Dropout):\n",
        "                x = layer.forward(x, train_flg)\n",
        "            else:\n",
        "                x = layer.forward(x)\n",
        "        return x\n",
        "\n",
        "    def loss(self, x, t):\n",
        "        y = self.predict(x, train_flg=True)\n",
        "        return self.last_layer.forward(y, t)\n",
        "\n",
        "    def accuracy(self, x, t, batch_size=100):\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "\n",
        "        acc = 0.0\n",
        "\n",
        "        for i in range(int(x.shape[0] / batch_size)):\n",
        "            tx = x[i*batch_size:(i+1)*batch_size]\n",
        "            tt = t[i*batch_size:(i+1)*batch_size]\n",
        "            y = self.predict(tx, train_flg=False)\n",
        "            y = np.argmax(y, axis=1)\n",
        "            acc += np.sum(y == tt)\n",
        "\n",
        "        return acc / x.shape[0]\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.last_layer.backward(dout)\n",
        "\n",
        "        tmp_layers = self.layers.copy()\n",
        "        tmp_layers.reverse()\n",
        "        for layer in tmp_layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 결과 저장\n",
        "        grads = {}\n",
        "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
        "            grads['W' + str(i+1)] = self.layers[layer_idx].dW\n",
        "            grads['b' + str(i+1)] = self.layers[layer_idx].db\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def save_params(self, file_name=\"params.pkl\"):\n",
        "        params = {}\n",
        "        for key, val in self.params.items():\n",
        "            params[key] = val\n",
        "        with open(file_name, 'wb') as f:\n",
        "            pickle.dump(params, f)\n",
        "\n",
        "    def load_params(self, file_name=\"params.pkl\"):\n",
        "        with open(file_name, 'rb') as f:\n",
        "            params = pickle.load(f)\n",
        "        for key, val in params.items():\n",
        "            self.params[key] = val\n",
        "\n",
        "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
        "            self.layers[layer_idx].W = self.params['W' + str(i+1)]\n",
        "            self.layers[layer_idx].b = self.params['b' + str(i+1)]\n"
      ],
      "metadata": {
        "id": "1z1tVYi1axA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# what is the class of this image ? \n",
        "# mnist : 대부분은 cnn을 기초로한 기법들\n",
        "# 현재 1위 99.79% : 합성곱 계층 2개, 완전연결 게층 2개정도\n",
        "# 비교적 단순한 과제라서 신경망의 표현력을 극한까지 높일 필요가 없기때문임\n",
        "# 앙상블학습, 학습률감소, 데이터 확장등을 통해 정확도 향상에 공헌"
      ],
      "metadata": {
        "id": "V1FURb70aqBE"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 확장(Data Augmentation)\n",
        "# 회전 + 이동 + crop + flip "
      ],
      "metadata": {
        "id": "yLqvUQwfdYTB"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 신경망의 층을 깊게 할때의 이점\n",
        "# - 매개변수 수가 줄어든다 (ex. 3x3합성곱 연산을 3회 반복하면 27개 매개변수, 같은 크기의 영역을 1회의 합성곱 연산으로 보기위해서는 49개 매개변수필요)\n",
        "# - 학습의 효율성 증가 : 층을 깊게하여 학습데이터의 양을 줄여 학습을 고속으로 수행할수있음"
      ],
      "metadata": {
        "id": "mb0h9XdHe2hR"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지넷 (imageNet)\n",
        "# 100만장이 넘는 이미지를 담고있는 데이터셋\n",
        "# 다양한 종류의 이미지를 포함하며 각 이미지에는 레이블이 붙어있음\n",
        "# 분류 부문 : 1000개의 클래스를 제대로 분류하는지를 겨룸"
      ],
      "metadata": {
        "id": "6TJK4t1mhwkQ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. VGG NET\n",
        "# 합성곱 계층과 풀링게층으로 구성되는 기본적인 CNN\n",
        "# 비중있는 층을 모두 16층 (혹은 19층)으로 심화한게 특징\n",
        "# 3X3작은 필터를 사용합 합성곱 게층을 연속으로 거침\n",
        "# 구성이 간단하며, 응용하기 좋음"
      ],
      "metadata": {
        "id": "ajOSFJQ-iKf3"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. GoogleNet\n",
        "# 세로방향 깊이 뿐만 아니라 가로방향도 깊다는 특징을 가짐\n",
        "# 인셉션구조를 띔 : 가로 방향의 폭이 존재함\n",
        "# 1x1 3x3 5x5 합성곱, 3x3최대풀링 -> 필터결합\n",
        "# 인셉션 구조를 하나의 빌딩블록(구성요소)으로 사용하는것이 googlenet의 특징\n",
        "# 1x1 합성곱 게층을 많은곳에서 사용함 : 채널쪽으로 크기를 줄이는것 , 매개변수 제거와 고속처리에 기여"
      ],
      "metadata": {
        "id": "t4lkWKdhik51"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. ResNet\n",
        "# 마이크로소프트 팀이 개발한 네트워크\n",
        "# 층을 깊게 하는것이 성능향상에 중요하지만, 층이 지나치게 잎으면 학습이 잘되지않고, 오히려 성능이 떨어지는 경우도많음\n",
        "# 그런문제를 해결하기 위해서 skip connection을 도입\n",
        "# 층의 깊이에 비례해 성능을 향상 시킬 수 있게 한 핵심임\n",
        "# skip connection : 입력데이터를 합성공 계층을 건너뛰어 출력에 바로 더하는 구조\n",
        "# 역전파때 스킵연결이 신호 감쇠를 막아줌, 입력데이터를 그대로 흘리는 것으로 역전파때도 상류의 기울기를 그대로 하류로 보냄\n",
        "# 합성곱 계층을 2개 층마다 건너뛰면서 층을 깊게함\n"
      ],
      "metadata": {
        "id": "b-qo3kHnmXlO"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. 딥러닝 고속화 \n",
        "# 대부분 딥러닝 프레임워크는 gpu를 활용\n",
        "# 학습을 복수의 gpu와 여러 기기로 분산수행함\n",
        "# gpu를 활용한 고속화 : 그래픽 처리뿐 아니라 범용 수치연산에도 이용됨\n",
        "# gpu 컴퓨팅 : gpu로 범용수치 연산을 수행하는것\n",
        "# cuda활용 \n",
        "# cuDNN : Cuda위에서 동작하는 라이브러리, 딥러닝에 최적화된 함수등이 구현됨\n"
      ],
      "metadata": {
        "id": "Rg89H9e4nbDq"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. 분산학습\n",
        "# gpu로 딥러닝 연산을 가속할수있지만 수평확장의 필요성 대두됨\n",
        "# gpu와 컴퓨터를 활용한 분산학습을 지원한 딥러닝 프레임워크들이 있음 ex. 텐서플로, cntk\n",
        "# 계산을 어떻게 분산시키느냐는 몹시 어려운문제\n"
      ],
      "metadata": {
        "id": "3yKRR6CWpR1Q"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. 연산 정밀도와 비트 줄이기\n",
        "# 딥러닝 높은 수치 정밀도를 요구하진 않음\n",
        "# 신경망을 흐르는 데이터를 퇴화시켜도 출력에 주는 영향은 적음\n",
        "# 32비트 단정밀도, 64비트 배정밀도, 16비트 반정밀도만 사용해도 학습에 문제가 없음\n",
        "# 딥러닝을 고속화하기 위해 비트를 줄이는 기술은 앞으로 주시해야할 분야\n",
        "# 딥러닝을 임베디드용으로 이용할때 중요한 주제\n"
      ],
      "metadata": {
        "id": "7LsBoW-zqXMX"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########## 딥러닝의 활용 ###########\n"
      ],
      "metadata": {
        "id": "EDyovwdtrQQj"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 사물검출(Object detection)\n",
        "# R-CNN(Regions with CNN) : 후보영역 추출-CNN특징계산-영역분류 / Selective search 기법사용\n",
        "# faster R-CNN : 후보영역 추출까지 CNN으로 처리하는 기법, 모든일을 하나의  CNN에서 처리하기 때문에 매우 빠름"
      ],
      "metadata": {
        "id": "aUEtQFpxrce-"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 분할(Segmentation)\n",
        "# 픽셀단위로 객체마다 채색된 지도 데이터를 사용해 학습함\n",
        "# FCN(Fully Convolutional Network) : 합성곱 계층만으로 구성된 네트워크"
      ],
      "metadata": {
        "id": "taCXuT8PsQjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. 이미지 캡션생성\n",
        "# 컴퓨터 비전과 자연어를 융합한 분야\n",
        "# NIC (Neural image caption): DEEP CNN+ RNN으로 구성됨\n",
        "# CF. RNN: 순환적 관계를 갖는 신경망으로 자연어나 시계열 데이터 등의 연속된 데이터를 다룰때 많이 사용됨"
      ],
      "metadata": {
        "id": "BYmyMef0ncjK"
      },
      "execution_count": 45,
      "outputs": []
    }
  ]
}